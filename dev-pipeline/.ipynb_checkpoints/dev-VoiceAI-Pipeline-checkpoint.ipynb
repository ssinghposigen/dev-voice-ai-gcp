{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "from datetime import datetime, timezone, UTC\n",
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "import json, logging\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    ")\n",
    "def list_download_calls_s3_to_gcs(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import logging, json\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import logging as cloud_logging\n",
    "    from datetime import datetime, timedelta, timezone, UTC\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Definitions\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def fetch_secrets(project_id, secret_id, version_id=\"latest\"):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "    def setup_logger(log_file):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "\n",
    "    def generate_gcs_folders(    \n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    ):\n",
    "        try:\n",
    "             # Setup logger\n",
    "            logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "            gcs_folders = {}\n",
    "            gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "            gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "            gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "            gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "            gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "            gcs_folders['gcs_logs_folder'] = f\"{pipeline_run_name}/Logs\"\n",
    "\n",
    "            # Initialize GCS Client\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "            # Create empty folders directly\n",
    "            for folder in gcs_folders.values():\n",
    "                blob = bucket.blob(f\"{folder}/\")\n",
    "                blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "                logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "            logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "            return gcs_folders\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "\n",
    "\n",
    "    def generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(\"Started: generating S3 folder prefix.\")\n",
    "            # Get current date and time\n",
    "            current_datetime = datetime.now()\n",
    "\n",
    "            # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "            if current_datetime.hour == 0:\n",
    "                adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "            else:\n",
    "                adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "            # Extract year, month, and day from the adjusted date\n",
    "            year = str(adjusted_datetime.year)\n",
    "            month = f\"{adjusted_datetime.month:02d}\"\n",
    "            day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "            # Construct the prefix for S3 listing\n",
    "            prefix = f\"{year}/{month}/{day}/\"\n",
    "            logger.info(\"Completed: generating S3 folder prefix {prefix}.\")\n",
    "\n",
    "            return prefix\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "\n",
    "    def get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "            # Initialize S3 Client\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key\n",
    "            )\n",
    "\n",
    "            all_files = []\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "            # Get current UTC time (timezone-aware)\n",
    "            current_time = datetime.now(timezone.utc)\n",
    "            # Calculate the time threshold (2 hours before the current time)\n",
    "            time_threshold = current_time - timedelta(hours=time_interval)\n",
    "            logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "            all_files = []\n",
    "\n",
    "            for page in pages:\n",
    "                for obj in page.get('Contents', []):\n",
    "                    file_path = obj['Key']\n",
    "                    s3_ts = obj['LastModified']\n",
    "\n",
    "                    # Extract timestamp from filename\n",
    "                    try:\n",
    "                        # Skip non-JSON files\n",
    "                        if file_path.endswith('.json'):\n",
    "                            call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                            call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                            # Compare only the time part\n",
    "                            if call_timestamp.time() <= time_threshold.time():\n",
    "                                all_files.append({\n",
    "                                    'File': file_path,\n",
    "                                    'Call_ID': call_id,\n",
    "                                    'File_Timestamp': call_timestamp,\n",
    "                                    'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                    'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                    'S3_Timestamp': s3_ts,\n",
    "                                    'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                    'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            if all_files:\n",
    "                df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "                df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "                # Subset the DataFrame for only the most recent 2 hours bin\n",
    "                df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "\n",
    "                # Write the DataFrame to GCS\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "                csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "                df_calls_list.to_csv(csv_path, index=False)\n",
    "                logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "                logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "                return df_calls_list\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"0 Files fetched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    def download_transcripts_to_gcs(\n",
    "        file,\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        s3_client,\n",
    "        s3_analysis_bucket\n",
    "    ):\n",
    "        \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "        local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "        gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "        try:\n",
    "            # Download file from S3\n",
    "            s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "            # Upload to GCS\n",
    "            blob = gcs_bucket.blob(gcs_blob_path)\n",
    "            blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "            return file, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "            handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            return None, file\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Variables\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "\n",
    "    log_file = f\"{pipeline_run_name}.logs\"  \n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "    aws_access_key = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "    aws_secret_key = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "    s3_analysis_bucket = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "    s3_transcripts_location = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "    time_interval = 3\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Calling\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    logger = setup_logger(log_file)\n",
    "\n",
    "    gcs_folders = generate_gcs_folders(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    )\n",
    "\n",
    "    gcs_staging_folder = gcs_folders['gcs_staging_folder']\n",
    "    gcs_transcripts_folder = gcs_folders['gcs_transcripts_folder']\n",
    "    gcs_errored_folder = gcs_folders['gcs_errored_folder']\n",
    "    gcs_logs_folder = gcs_folders['gcs_logs_folder']\n",
    "\n",
    "    s3_prefix = generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    )\n",
    "\n",
    "    df_calls_list = get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix\n",
    "    )\n",
    "\n",
    "    if len(df_calls_list)>0:\n",
    "        files_list = df_calls_list.File.to_list()\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\", \n",
    "            aws_access_key_id=aws_access_key, \n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        success_downloads = []\n",
    "        failed_downloads = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            logger.info(f\"Started: bulk download to GCS transcripts#: {len(files_list)}\")\n",
    "\n",
    "            future_to_file = {\n",
    "                executor.submit(\n",
    "                    download_transcripts_to_gcs,\n",
    "                    file,\n",
    "                    pipeline_run_name,\n",
    "                    vai_gcs_bucket,\n",
    "                    gcs_staging_folder,\n",
    "                    gcs_errored_folder,\n",
    "                    gcs_transcripts_folder,\n",
    "                    s3_client,\n",
    "                    s3_analysis_bucket\n",
    "                ): file for file in files_list\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_file):\n",
    "                try:\n",
    "                    success, failed = future.result()  # Get results\n",
    "\n",
    "                    if success:\n",
    "                        success_downloads.append(success)\n",
    "                    if failed:\n",
    "                        failed_downloads.append(failed)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "        # Upload logs to GCS Bucket:\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "        blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "        blob.upload_from_filename(log_file, checksum=None)\n",
    "\n",
    "        logger.info(f\"Completed: bulk download to GCS transcripts, Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\")\n",
    "\n",
    "    else:\n",
    "        logger.info(\"No Calls to Process.\")\n",
    "        # Upload logs to GCS Bucket:\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "        blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "        blob.upload_from_filename(log_file, checksum=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Parallel Process Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/var/tmp/ipykernel_160764/3116979347.py:294: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "2025-03-30 09:21:08 [INFO]: Completed: Fetching Category, Sub-Category Mapping.\n",
      "2025-03-30 09:21:08 [INFO]: Fetching Transcripts from GCS: cx-voiceai-process-calls-2025-03-25-18-26-56/Transcripts\n",
      "2025-03-30 09:21:08 [INFO]: Completed: Fetching from GCS Transcripts List#: 331\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1179\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;66;03m# Merge all threaded transcripts\u001b[39;00m\n\u001b[32m   1178\u001b[39m threads_log_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m threads_log_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.log\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m \u001b[43mmerge_logs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreads_log_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_log_file\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;66;03m# # Step 3: Merge all outputs into master files after processing\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[38;5;66;03m# merge_and_save_transcripts(\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[38;5;66;03m#     vai_gcs_bucket,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1198\u001b[39m \n\u001b[32m   1199\u001b[39m \u001b[38;5;66;03m# Upload the master log file back into GCS Bucket\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mmerge_logs\u001b[39m\u001b[34m(log_files, master_log_file)\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m thread_log:\n\u001b[32m    168\u001b[39m             master_log.write(thread_log.read() + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll thread logs merged into: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaster_log_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# @dsl.component(\n",
    "#     base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    "# )\n",
    "# def process_transcripts(\n",
    "#     pipeline_run_name: str,\n",
    "#     project_id: str,\n",
    "#     secret_id: str,\n",
    "#     version_id: str\n",
    "# ):\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import logging\n",
    "import re, os, json, io\n",
    "from datetime import datetime, timezone, UTC\n",
    "from typing import List, Dict\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "import snowflake.connector as sc\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "from google.cloud import secretmanager\n",
    "from google.cloud import storage\n",
    "from google.cloud import dlp_v2\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "# Sentiments\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Initialize Google Cloud Logging client\n",
    "cloud_logging_client = cloud_logging.Client()\n",
    "cloud_logging_client.setup_logging()\n",
    "\n",
    "\"\"\"\n",
    "========================================================\n",
    "Function: Exception hanlding mechanism\n",
    "========================================================\n",
    "\"\"\"\n",
    "def fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id=\"latest\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "\n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "        \n",
    "        \n",
    "def setup_logger(\n",
    "    log_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Path of the log file.\n",
    "\n",
    "    Returns:\n",
    "        logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "        if not logger.handlers:  # Avoid adding multiple handlers\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "\n",
    "            # File Handler\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            # Console Handler\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize logger: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Function to create thread-specific log files\n",
    "def setup_thread_logger(\n",
    "    contact_id\n",
    "):\n",
    "    \"\"\"Create a separate log file for each transcript.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "    log_filename = f\"{contact_id}_{timestamp}.log\"\n",
    "    log_filepath = os.path.join(temp_log_folder, log_filename)\n",
    "\n",
    "    thread_logger = logging.getLogger(log_filename)\n",
    "    thread_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Remove handlers to prevent duplication\n",
    "    if thread_logger.hasHandlers():\n",
    "        thread_logger.handlers.clear()\n",
    "\n",
    "    file_handler = logging.FileHandler(log_filepath)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    thread_logger.addHandler(file_handler)\n",
    "\n",
    "    return thread_logger, log_filepath\n",
    "\n",
    "\n",
    "def merge_logs(\n",
    "    log_files,\n",
    "    master_log_file,\n",
    "    master_logger\n",
    "):\n",
    "    \"\"\"Merge all thread logs into master log file.\"\"\"\n",
    "    sorted_logs = sorted(log_files)  # Sort logs based on filename timestamps\n",
    "\n",
    "    with open(master_log_file, \"a\") as master_log:\n",
    "        for log_file in sorted_logs:\n",
    "            with open(log_file, \"r\") as thread_log:\n",
    "                master_log.write(thread_log.read() + \"\\n\")\n",
    "\n",
    "    master_logger.info(f\"All thread logs merged into: {master_log_file}\")\n",
    "\n",
    "\n",
    "def handle_exception(\n",
    "    file_id,\n",
    "    vai_gcs_bucket,\n",
    "    run_folder,\n",
    "    error_folder,\n",
    "    error_message,\n",
    "    logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "def fetch_transcripts_from_gcs(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    gcs_transcripts_folder,\n",
    "    master_logger\n",
    "):\n",
    "    \"\"\"\n",
    "    List all files in a GCS bucket, handling pagination.\n",
    "\n",
    "    :param bucket_name: Name of the GCS bucket\n",
    "    :param prefix: (Optional) Folder path to filter files\n",
    "    :return: List of file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        master_logger.info(f\"Fetching Transcripts from GCS: {gcs_transcripts_folder}\")\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "        blobs_iterator = bucket.list_blobs(prefix=gcs_transcripts_folder)  # GCS handles pagination internally\n",
    "\n",
    "        transcripts_list = []\n",
    "        for page in blobs_iterator.pages:  # Handling pagination\n",
    "            for blob in page:\n",
    "                if not blob.name.endswith(\"/\"):\n",
    "                    transcripts_list.append(blob.name)\n",
    "                    # transcripts_list.append(os.path.basename(blob.name))\n",
    "        master_logger.info(f\"Completed: Fetching from GCS Transcripts List#: {len(transcripts_list)}\")\n",
    "        return transcripts_list\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e), master_logger)\n",
    "\n",
    "\n",
    "def fetch_category_mapping_from_snowflake(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    snf_account,\n",
    "    snf_user,\n",
    "    snf_private_key,\n",
    "    snf_private_key_pwd,\n",
    "    snf_warehouse,\n",
    "    snf_catsubcat_databse,\n",
    "    snf_catsubcat_schema,\n",
    "    snf_catsubcat_view,\n",
    "    master_logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "    :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "    :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "    :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "    :return: Pandas DataFrame with category mappings.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load & Decrypt the Private Key\n",
    "        snf_private_key = serialization.load_pem_private_key(\n",
    "            snf_private_key.encode(),\n",
    "            password=snf_private_key_pwd.encode(),\n",
    "            backend=None  # Default backend\n",
    "        )\n",
    "\n",
    "        # Step 2: Convert to Snowflake Compatible Format\n",
    "        pkey_bytes = snf_private_key.private_bytes(\n",
    "            encoding=serialization.Encoding.DER,\n",
    "            format=serialization.PrivateFormat.PKCS8,\n",
    "            encryption_algorithm=serialization.NoEncryption(),\n",
    "        )\n",
    "\n",
    "        # Step 3: Connect to Snowflake\n",
    "        catsubcat_conn_params = {\n",
    "            'account': snf_account,\n",
    "            'user': snf_user,\n",
    "            'private_key': snf_private_key,\n",
    "            'warehouse': snf_warehouse,\n",
    "            'database': snf_catsubcat_databse,\n",
    "            'schema': snf_catsubcat_schema\n",
    "        }\n",
    "\n",
    "        # Connect to Snowflake\n",
    "        conn = sc.connect(**catsubcat_conn_params)\n",
    "\n",
    "        # Fetch data from Snowflake\n",
    "        query = f\"SELECT CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        master_logger.info(\"Completed: Fetching Category, Sub-Category Mapping.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e), master_logger)\n",
    "\n",
    "\"\"\"\n",
    "========================================================\n",
    "Function: Create Dataframe: Intra Call \n",
    "========================================================\n",
    "\"\"\"            \n",
    "def mask_pii_in_captions(\n",
    "    contact_id,\n",
    "    df,\n",
    "    project_id,\n",
    "    thread_logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Masks PII data in the 'caption' column of a pandas DataFrame using Google Cloud DLP API.\n",
    "\n",
    "    Args:\n",
    "        contact_id: Identifier for logging purposes\n",
    "        df (pandas.DataFrame): DataFrame with a 'caption' column to process\n",
    "        project_id (str): Your Google Cloud project ID\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with masked PII in the 'caption' column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        thread_logger.info(f\"{contact_id}: Masking PII Data\")\n",
    "\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        masked_df = df.copy()\n",
    "\n",
    "        # Add unique markers to each caption to identify them after processing\n",
    "        masked_df['original_index'] = masked_df.index\n",
    "        masked_df['marked_caption'] = masked_df.index.astype(str) + \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n",
    "\n",
    "        # Concatenate all captions for bulk processing\n",
    "        all_captions = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n",
    "\n",
    "        # Initialize DLP client\n",
    "        dlp_client = dlp_v2.DlpServiceClient()\n",
    "\n",
    "        # Specify the parent resource name\n",
    "        parent = f\"projects/{project_id}/locations/global\"\n",
    "\n",
    "        # Custom dictionary detector for PosiGen\n",
    "        posigen_dictionary = {\n",
    "            \"info_type\": {\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"},\n",
    "            \"dictionary\": {\n",
    "                \"word_list\": {\n",
    "                    \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Configure inspection config with rule set for exclusions\n",
    "        inspect_config = {\n",
    "            \"info_types\": [\n",
    "                {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                {\"name\": \"STREET_ADDRESS\"},\n",
    "                {\"name\": \"IP_ADDRESS\"},\n",
    "                {\"name\": \"DATE_OF_BIRTH\"}\n",
    "            ],\n",
    "            \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE,\n",
    "            \"custom_info_types\": [posigen_dictionary],\n",
    "            \"rule_set\": [\n",
    "                {\n",
    "                    \"info_types\": [{\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"}],\n",
    "                    \"rules\": [\n",
    "                        {\n",
    "                            \"exclusion_rule\": {\n",
    "                                \"matching_type\": dlp_v2.MatchingType.MATCHING_TYPE_FULL_MATCH,\n",
    "                                \"dictionary\": {\n",
    "                                    \"word_list\": {\n",
    "                                        \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Configure deidentification to use \"[REDACTED]\" instead of asterisks\n",
    "        deidentify_config = {\n",
    "            \"info_type_transformations\": {\n",
    "                \"transformations\": [\n",
    "                    {\n",
    "                        \"info_types\": [\n",
    "                            {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                            {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                            {\"name\": \"STREET_ADDRESS\"},\n",
    "                            {\"name\": \"IP_ADDRESS\"},\n",
    "                            {\"name\": \"DATE_OF_BIRTH\"}\n",
    "                        ],\n",
    "                        \"primitive_transformation\": {\n",
    "                            \"replace_config\": {\n",
    "                                \"new_value\": {\"string_value\": \"[REDACTED]\"}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Create deidentify request\n",
    "        item = {\"value\": all_captions}\n",
    "\n",
    "        # Call the DLP API\n",
    "        try:\n",
    "            response = dlp_client.deidentify_content(\n",
    "                request={\n",
    "                    \"parent\": parent,\n",
    "                    \"deidentify_config\": deidentify_config,\n",
    "                    \"inspect_config\": inspect_config,\n",
    "                    \"item\": item,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            thread_logger.error(f\"{contact_id}: Error in DLP API call: {e}\")\n",
    "            return df  # Return original DataFrame if masking fails\n",
    "\n",
    "        # Get processed content and split by record boundaries\n",
    "        processed_content = response.item.value\n",
    "        processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\n\")\n",
    "\n",
    "        # Create mapping from original indices to processed captions\n",
    "        processed_dict = {}\n",
    "        for record in processed_records:\n",
    "            parts = record.split(\"|||SEPARATOR|||\", 1)\n",
    "            if len(parts) == 2:\n",
    "                idx, content = parts\n",
    "                processed_dict[int(idx)] = content\n",
    "\n",
    "        # Update the DataFrame with redacted content\n",
    "        masked_df['caption'] = masked_df.apply(\n",
    "            lambda row: processed_dict.get(row['original_index'], row['caption']), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Additional processing to mask all digits with asterisks\n",
    "        def mask_digits(text):\n",
    "            \"\"\"Replaces digits with asterisks while preserving '[REDACTED]' markers.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                return text\n",
    "            parts = text.split(\"[REDACTED]\")\n",
    "            for i in range(len(parts)):\n",
    "                parts[i] = re.sub(r'\\d', '*', parts[i])\n",
    "            return \"[REDACTED]\".join(parts)\n",
    "\n",
    "        # Apply the digit masking function to each processed caption\n",
    "        masked_df['caption'] = masked_df['caption'].apply(mask_digits)\n",
    "\n",
    "        # Drop temporary columns\n",
    "        masked_df.drop(['original_index', 'marked_caption'], axis=1, inplace=True)\n",
    "\n",
    "        thread_logger.info(f\"{contact_id}: Completed Masking PII Data\")\n",
    "        return masked_df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"mask_pii_in_captions() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def get_sentiment_label(row):\n",
    "    try:\n",
    "        # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "        if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "            return 'Positive'\n",
    "        elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"get_sentiment_label() failed: {str(e)}\")\n",
    "\n",
    "def get_different_times(\n",
    "    intra_call,\n",
    "    thread_logger\n",
    "):\n",
    "    try:\n",
    "        # Apply formatting to both time columns\n",
    "        intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "        intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "        intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "        intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "        intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "        intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "        intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "        intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "        intra_call['load_date'] = datetime.now()\n",
    "\n",
    "        # Dropping time formatted columns\n",
    "        intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "        return intra_call\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"get_different_times() failed: {str(e)}\")\n",
    "\n",
    "def get_sentiment_scores(\n",
    "    contact_id,\n",
    "    text_list,\n",
    "    thread_logger\n",
    "):\n",
    "    try:\n",
    "        thread_logger.info(f\"{contact_id}: Calculating Caption Sentiments.\")\n",
    "        dict_sentiments = []\n",
    "        for text in text_list:\n",
    "            encoded_input = tokenizer(text, return_tensors='pt')\n",
    "            output = model_sentiment(**encoded_input)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "            merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "            dict_sentiments.append(merged_dict)\n",
    "\n",
    "        df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "        df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "        thread_logger.info(f\"{contact_id}: Completed calculating Caption Sentiments.\")\n",
    "\n",
    "        return df_dict_sentiments\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"get_sentiment_scores() failed: {str(e)}\")\n",
    "\n",
    "def process_transcript(\n",
    "    contact_id,\n",
    "    transcript_data,\n",
    "    tokenizer,\n",
    "    thread_logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Pre-process the transcript loaded from S3 Buckets:\n",
    "    1. Load the transcript as Pandas Dataframe.\n",
    "    2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "    3. Format the time in minutes and seconds.\n",
    "    4. Rename the columns for better understanding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        thread_logger.info(f\"{contact_id}: Loading the Transcript as Pandas Dataframe.\")\n",
    "        transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "        # Select the relevant Columns\n",
    "        columns_to_select = [\n",
    "            'BeginOffsetMillis',\n",
    "            'EndOffsetMillis',\n",
    "            'ParticipantId',\n",
    "            'Content'\n",
    "        ]\n",
    "        formatted_df = transcript_df[columns_to_select].copy()\n",
    "\n",
    "        # Optionally rename columns to reflect their new format\n",
    "        formatted_df = formatted_df.rename(columns={\n",
    "            'BeginOffsetMillis': 'Begin_Offset',\n",
    "            'EndOffsetMillis': 'End_Offset',\n",
    "            'Content': 'caption',\n",
    "            'Sentiment': 'sentiment_label',\n",
    "            'ParticipantId': 'speaker_tag'\n",
    "        })\n",
    "\n",
    "        # Inserting the Call ID:\n",
    "        formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "        formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "        thread_logger.info(f\"{contact_id}: Returning formated DataFrame.\")\n",
    "        return formatted_df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"process_transcript() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_intra_call_df(\n",
    "    contact_id,\n",
    "    gcp_project_id,\n",
    "    vai_gcs_bucket,\n",
    "    pipeline_run_name,\n",
    "    transcript_data,\n",
    "    tokenizer,\n",
    "    thread_logger\n",
    "):\n",
    "    try:\n",
    "        thread_logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "        intra_call = process_transcript(\n",
    "            contact_id,\n",
    "            transcript_data,\n",
    "            tokenizer,\n",
    "            thread_logger\n",
    "        )\n",
    "        \n",
    "        df_sentiment_scores = get_sentiment_scores(\n",
    "            contact_id,\n",
    "            intra_call.caption.to_list(),\n",
    "            thread_logger\n",
    "        )\n",
    "        \n",
    "        intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "        intra_call = get_different_times(\n",
    "            intra_call,\n",
    "            thread_logger\n",
    "        )\n",
    "        \n",
    "        intra_call = mask_pii_in_captions(\n",
    "            contact_id,\n",
    "            intra_call,\n",
    "            gcp_project_id,\n",
    "            thread_logger\n",
    "        )\n",
    "        \n",
    "        thread_logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "\n",
    "        return intra_call\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"create_intra_call_df() failed: {str(e)}\")\n",
    "\n",
    "\"\"\"\n",
    "========================================================\n",
    "Function: Create Dataframe: Inter Call \n",
    "========================================================\n",
    "\"\"\"\n",
    "def dict_to_newline_string(data):\n",
    "    \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "    try:\n",
    "        formatted_str = \"\"\n",
    "        for key, value in data.items():\n",
    "            formatted_str += f\"{key}:\\n\"\n",
    "            for item in value:\n",
    "                formatted_str += f\"  - {item}\\n\"\n",
    "        return formatted_str.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"dict_to_newline_string() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class CategoryValidator:\n",
    "    def __init__(self, df_cat_subcat_mapping):\n",
    "        \"\"\"\n",
    "        Initialize with category mapping from a Pandas DataFrame.\n",
    "        :param df_cat_subcat_mapping: Pandas DataFrame containing 'CATEGORY' and 'SUBCATEGORY' columns.\n",
    "        \"\"\"\n",
    "        self.df_cat_subcat_mapping = df_cat_subcat_mapping  # Ensure only the correct DataFrame is used\n",
    "        self.valid_categories = set(df_cat_subcat_mapping['CATEGORY'].dropna().unique())\n",
    "        self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "    def _create_category_mapping(self):\n",
    "        \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "        try:\n",
    "            mapping = {}\n",
    "            for _, row in self.df_cat_subcat_mapping.dropna().iterrows():\n",
    "                category = row['CATEGORY']\n",
    "                subcategory = row['SUBCATEGORY']\n",
    "\n",
    "                if category not in mapping:\n",
    "                    mapping[category] = set()\n",
    "\n",
    "                if subcategory:  # Only add non-null subcategories\n",
    "                    mapping[category].add(subcategory)\n",
    "\n",
    "            return mapping\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"_create_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "    def validate_category(self, category: str) -> bool:\n",
    "        \"\"\"Check if category is valid.\"\"\"\n",
    "        try:\n",
    "            return category in self.valid_categories\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"validate_category() failed: {str(e)}\")\n",
    "\n",
    "    def validate_subcategory(self, category: str, subcategory: str) -> bool:\n",
    "        \"\"\"Check if subcategory is valid for the given category.\"\"\"\n",
    "        try:\n",
    "            return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"validate_subcategory() failed: {str(e)}\")\n",
    "\n",
    "    def get_valid_subcategories(self, category: str) -> set:\n",
    "        \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "        try:\n",
    "            return self.category_subcategory_map.get(category, set())\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_valid_subcategories() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class CallSummary(BaseModel):\n",
    "    summary: str = Field(..., max_length=500)\n",
    "\n",
    "class CallTopic(BaseModel):\n",
    "    primary_topic: str = Field(..., max_length=100)\n",
    "    category: str = Field(..., max_length=100)\n",
    "    sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "    def validate_category_mapping(\n",
    "        self,\n",
    "        category_validator: CategoryValidator,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"Validate category and subcategory against mapping. Replace with 'Unspecified' if invalid.\"\"\"\n",
    "        try:\n",
    "            if not category_validator.validate_category(self.category):\n",
    "                thread_logger.warning(f\"Invalid category: {self.category}. Replacing with 'Unspecified'.\")\n",
    "                self.category = \"Unspecified\"\n",
    "                self.sub_category = \"Unspecified\"\n",
    "            elif not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "                thread_logger.warning(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'. Replacing subcategory with 'Unspecified'.\")\n",
    "                self.sub_category = \"Unspecified\"\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"validate_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "class AgentCoaching(BaseModel):\n",
    "    strengths: List[str] = Field(..., max_items=3)\n",
    "    improvement_areas: List[str] = Field(..., max_items=3)\n",
    "    specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "    skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class TranscriptAnalysis(BaseModel):\n",
    "    call_summary: CallSummary\n",
    "    call_topic: CallTopic\n",
    "    agent_coaching: AgentCoaching\n",
    "\n",
    "class KPIExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        df_cat_subcat_mapping,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the KPIExtractor with Vertex AI model and category validator.\n",
    "        :param project_id: GCP Project ID\n",
    "        :param location: GCP Region\n",
    "        :param df_cat_subcat_mapping: Pandas DataFrame with 'CATEGORY' and 'SUBCATEGORY'\n",
    "        \"\"\"\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "        self.category_validator = CategoryValidator(df_cat_subcat_mapping)\n",
    "\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 1024,\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "\n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_categories_prompt(self) -> str:\n",
    "        \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "        try:\n",
    "            categories_prompt = []\n",
    "\n",
    "            for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "                if category is None:  # Skip if category is None\n",
    "                    continue\n",
    "\n",
    "                # Ensure subcategories are valid (remove None values)\n",
    "                valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "\n",
    "                if valid_subcategories:\n",
    "                    subcats = ', '.join(sorted(valid_subcategories))\n",
    "                else:\n",
    "                    subcats = \"No defined subcategories\"\n",
    "\n",
    "                categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "\n",
    "            return '\\n'.join(categories_prompt)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_categories_prompt() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def create_prompt(self, transcript):\n",
    "        \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "        categories_guidance = self.get_categories_prompt()\n",
    "\n",
    "        return f\"\"\"\n",
    "        Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "        Keep responses concise, specific, and actionable.\n",
    "\n",
    "        Guidelines:\n",
    "        - Call summary should be factual and highlight key interactions\n",
    "        - Topics and categories MUST match the following valid mappings:\n",
    "        {categories_guidance}\n",
    "        - Coaching points should be specific and actionable\n",
    "        - All responses must follow the exact structure specified\n",
    "        - Ensure all lists have the specified maximum number of items\n",
    "        - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "        Transcript:\n",
    "        {transcript}\n",
    "\n",
    "        Required Output Structure:\n",
    "        {{\n",
    "            \"call_summary\": {{\n",
    "                \"summary\": \"3-4 line overview of the call\"\n",
    "            }},\n",
    "            \"call_topic\": {{\n",
    "                \"primary_topic\": \"Main topic of discussion\",\n",
    "                \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "            }},\n",
    "            \"agent_coaching\": {{\n",
    "                \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        1. Maintain exact JSON structure\n",
    "        2. No additional fields or comments\n",
    "        3. No markdown formatting\n",
    "        4. Ensure all arrays have the exact number of items specified\n",
    "        5. Keep all text concise and professional\n",
    "        6. Do not mention any PII information such as Customer Name etc.\n",
    "        7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "        \"\"\"\n",
    "\n",
    "    def extract_json(self, response):\n",
    "        \"\"\"Extract valid JSON from response\"\"\"\n",
    "        try:\n",
    "            match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "            else:\n",
    "                json_str = response.strip()\n",
    "            return json.loads(json_str)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"extract_json() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def validate_response(\n",
    "        self,\n",
    "        response_json,\n",
    "        thread_logger,\n",
    "        contact_id = None        \n",
    "    ):\n",
    "        \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "        try:\n",
    "            # First validate basic structure with Pydantic\n",
    "            analysis = TranscriptAnalysis(**response_json)\n",
    "\n",
    "            # Then validate category mapping\n",
    "            analysis.call_topic.validate_category_mapping(self.category_validator, thread_logger)\n",
    "\n",
    "            return analysis\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"validate_response() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def extract_genai_kpis(self, transcript, contact_id = None):\n",
    "        \"\"\"Extract KPIs from transcript with validation\"\"\"\n",
    "        try:\n",
    "            # Generate prompt\n",
    "            prompt = self.create_prompt(transcript)\n",
    "\n",
    "            # Get response from Gemini\n",
    "            response = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=self.generation_config,\n",
    "                safety_settings=self.safety_settings\n",
    "            )\n",
    "\n",
    "            # Parse JSON response\n",
    "            response_json = self.extract_json(response.text)\n",
    "\n",
    "            # Validate response structure and categories\n",
    "            validated_response = self.validate_response(response_json, contact_id)\n",
    "\n",
    "            return validated_response.model_dump()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"extract_genai_kpis() failed: {str(e)}\")\n",
    "\n",
    "            \n",
    "\"\"\"\n",
    "========================================================\n",
    "Function: Create Dataframe Inter Call\n",
    "========================================================\n",
    "\"\"\"\n",
    "def create_inter_call_df(\n",
    "    contact_id,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    pipeline_run_name,\n",
    "    transcript_data,\n",
    "    ac_last_modified_date,\n",
    "    df_intra_call,\n",
    "    gcp_project_id,\n",
    "    gcp_project_location,\n",
    "    df_cat_subcat_mapping,\n",
    "    thread_logger\n",
    "):\n",
    "    try:\n",
    "        thread_logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "        thread_logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "        extractor = KPIExtractor(\n",
    "            gcp_project_id,\n",
    "            gcp_project_location,\n",
    "            df_cat_subcat_mapping,\n",
    "            thread_logger\n",
    "        )\n",
    "        transcript = \" \".join(df_intra_call.caption)\n",
    "        call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "        thread_logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "\n",
    "        inter_call_dict = {}\n",
    "        inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "        inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "        inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "        inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "        inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "        inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "        inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "        df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "        # Replace values where Categories are not in allowed list\n",
    "        allowed_categories = df_cat_subcat_mapping['CATEGORY'].drop_duplicates().to_list()\n",
    "        df_inter_call.loc[\n",
    "            ~df_inter_call['category'].isin(allowed_categories) | df_inter_call['category'].isna(),\n",
    "            ['category', 'sub_category']\n",
    "        ] = 'Unspecified'\n",
    "\n",
    "        df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "        df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "        df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "        df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "        # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "        # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "        df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "        df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "        df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "        thread_logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "\n",
    "        return df_inter_call\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"create_inter_call_df() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "========================================================\n",
    "Function: Process Single Transcript\n",
    "========================================================\n",
    "\"\"\"\n",
    "def process_single_transcript(\n",
    "    pipeline_run_name,\n",
    "    gcp_project_id,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    gcs_logs_folder,\n",
    "    gcs_intra_call_dfs_folder,\n",
    "    gcs_inter_call_dfs_folder,\n",
    "    transcript_path,\n",
    "    tokenizer,\n",
    "    gcp_project_location,\n",
    "    df_cat_subcat_mapping\n",
    "):\n",
    "    contact_id = transcript_path.split('/')[-1].split('analysis')[0].strip('_')\n",
    "    ac_last_modified_date = datetime.strptime(\n",
    "            transcript_path.split('/')[-1].split('analysis_')[-1].split('.')[0].replace('_', ':'),\n",
    "            '%Y-%m-%dT%H:%M:%SZ'\n",
    "        )\n",
    "    \n",
    "    thread_logger, log_filepath = setup_thread_logger(contact_id)\n",
    "\n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(transcript_path)\n",
    "        transcript_data = json.loads(blob.download_as_text())\n",
    "\n",
    "        thread_logger.info(f\"{contact_id}: started processing\")\n",
    "\n",
    "        df_intra_call = create_intra_call_df(\n",
    "            contact_id,\n",
    "            gcp_project_id,\n",
    "            vai_gcs_bucket,\n",
    "            pipeline_run_name,\n",
    "            transcript_data,\n",
    "            tokenizer,\n",
    "            thread_logger\n",
    "        )\n",
    "\n",
    "        df_inter_call = create_inter_call_df(\n",
    "            contact_id,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            pipeline_run_name,\n",
    "            transcript_data,\n",
    "            ac_last_modified_date,\n",
    "            df_intra_call,\n",
    "            gcp_project_id,\n",
    "            gcp_project_location,\n",
    "            df_cat_subcat_mapping,\n",
    "            thread_logger\n",
    "        )\n",
    "\n",
    "        if not df_intra_call.empty and not df_inter_call.empty:\n",
    "            csv_path_df_intra_call = f\"gs://{vai_gcs_bucket}/{gcs_intra_call_dfs_folder}/{contact_id}_df_intra_call.csv\"\n",
    "            df_intra_call.to_csv(csv_path_df_intra_call, index=False)\n",
    "            thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_intra_call.csv\")\n",
    "\n",
    "            csv_path_df_inter_call = f\"gs://{vai_gcs_bucket}/{gcs_inter_call_dfs_folder}/{contact_id}_df_inter_call.csv\"\n",
    "            df_inter_call.to_csv(csv_path_df_inter_call, index=False)\n",
    "            thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_inter_call.csv\")\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Processing Complete\")\n",
    "            thread_logger.info(\"\")\n",
    "            thread_logger.info(\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e), thread_logger)\n",
    "        return None # Continue processing other files\n",
    "\n",
    "    return log_filepath\n",
    "\n",
    "def merge_and_save_transcripts(\n",
    "    bucket_name,\n",
    "    input_folder,\n",
    "    output_folder,\n",
    "    output_file\n",
    "):\n",
    "    try:\n",
    "        \"\"\"Reads, merges all files in a GCS folder, and saves the master DataFrame as CSV.\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "\n",
    "        dfs = [\n",
    "            pd.read_parquet(bucket.blob(blob.name).open(\"rb\")) if blob.name.endswith(\".parquet\") \n",
    "            else pd.read_csv(bucket.blob(blob.name).open(\"r\")) \n",
    "            for blob in bucket.list_blobs(prefix=input_folder) \n",
    "            if blob.name.endswith(('.csv', '.parquet'))\n",
    "        ]\n",
    "\n",
    "        if dfs:\n",
    "            master_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "            # Convert DataFrame to CSV in-memory\n",
    "            csv_buffer = io.StringIO()\n",
    "            master_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            # Upload CSV to GCS\n",
    "            bucket.blob(f\"{output_folder}/{output_file}\").upload_from_string(\n",
    "                csv_buffer.getvalue(), content_type=\"text/csv\"\n",
    "            )\n",
    "            logger.info(f\"Completed: merging and writing {output_file} to {output_folder}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {input_folder}: {str(e)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "========================================================\n",
    "Variables\n",
    "========================================================\n",
    "\"\"\"\n",
    "project_id = \"dev-posigen\"\n",
    "secret_id = \"dev-cx-voiceai\"\n",
    "version_id= \"latest\"\n",
    "pipeline_run_name = \"cx-voiceai-process-calls-2025-03-25-18-26-56\"\n",
    "\n",
    "configs = fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id\n",
    ")\n",
    "\n",
    "# GCP Configuration\n",
    "gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "# Pipeline Configuration\n",
    "gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "# Snowflake Configuration\n",
    "snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "snf_private_key = configs.get(\"private_key\")\n",
    "snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "snf_catsubcat_databse = configs.get(\"VAI_SNF_CATSUBCAT_DATABASE\")\n",
    "snf_catsubcat_schema = configs.get(\"VAI_SNF_CATSUBCAT_SCHEMA\")\n",
    "snf_catsubcat_view = configs.get(\"VAI_SNF_CATSUBCAT_VIEW\")\n",
    "\n",
    "# Max parallelism for multi-threading\n",
    "max_parallelism = 10\n",
    "\n",
    "# Step 2: Download Master Log File from GCS\n",
    "master_log_file = f\"{pipeline_run_name}.logs\"\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(vai_gcs_bucket)\n",
    "blob = bucket.blob(f\"{gcs_logs_folder}/{master_log_file}\")\n",
    "# Download master log file\n",
    "blob.download_to_filename(master_log_file)\n",
    "\n",
    "master_logger = setup_logger(master_log_file)\n",
    "\n",
    "temp_log_folder = \"temp_logs\"\n",
    "os.makedirs(temp_log_folder, exist_ok=True)\n",
    "\n",
    "df_cat_subcat_mapping = fetch_category_mapping_from_snowflake(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    snf_account,\n",
    "    snf_user,\n",
    "    snf_private_key,\n",
    "    snf_private_key_pwd,\n",
    "    snf_warehouse,\n",
    "    snf_catsubcat_databse,\n",
    "    snf_catsubcat_schema,\n",
    "    snf_catsubcat_view,\n",
    "    master_logger\n",
    ")\n",
    "\n",
    "transcripts_list = fetch_transcripts_from_gcs(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    gcs_transcripts_folder,\n",
    "    master_logger\n",
    ")\n",
    "\n",
    "# Multi-threaded execution\n",
    "threads_log_files = []  # Store generated log files\n",
    "\n",
    "# Multi-threaded execution\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallelism) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_single_transcript,\n",
    "            pipeline_run_name,\n",
    "            gcp_project_id, \n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            gcs_logs_folder,\n",
    "            gcs_intra_call_dfs_folder,\n",
    "            gcs_inter_call_dfs_folder,\n",
    "            transcript_path,\n",
    "            tokenizer,\n",
    "            gcp_project_location,\n",
    "            df_cat_subcat_mapping\n",
    "        ) for transcript_path in transcripts_list[38:50]\n",
    "    ]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        threads_log_files.append(future.result())\n",
    "\n",
    "# Merge all threaded transcripts\n",
    "threads_log_files = [file for file in threads_log_files if isinstance(file, str) and file.endswith(\".log\")]\n",
    "merge_logs(\n",
    "    threads_log_files,\n",
    "    master_log_file,\n",
    "    master_logger\n",
    ")\n",
    "\n",
    "# # Step 3: Merge all outputs into master files after processing\n",
    "# merge_and_save_transcripts(\n",
    "#     vai_gcs_bucket,\n",
    "#     gcs_intra_call_dfs_folder,\n",
    "#     gcs_stagging_folder,\n",
    "#     \"master_intra_call_df.csv\"\n",
    "# )\n",
    "\n",
    "# merge_and_save_transcripts(\n",
    "#     vai_gcs_bucket,\n",
    "#     gcs_inter_call_dfs_folder,\n",
    "#     gcs_stagging_folder,\n",
    "#     \"master_inter_call_df.csv\"\n",
    "# )\n",
    "\n",
    "# Upload the master log file back into GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c8c693d-1ed6-4ae3-bff5-b731be759cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 09:07:52 [INFO]: testing this log entry\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"testing this log entry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d365f-c880-4ace-970d-a5618d503ad6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Write data to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce6cc8-40f2-42d0-b6be-8cd43cd52357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_run_name = VAI_GCP_PIPELINE_RUN_NAME\n",
    "# print(pipeline_run_name)\n",
    "# gcp_project_id = VAI_GCP_PROJECT_ID\n",
    "# print(gcp_project_id)\n",
    "# gcp_project_location = VAI_GCP_PROJECT_LOCATION\n",
    "# print(gcp_project_location)\n",
    "# vai_gcs_bucket = VAI_GCP_PIPELINE_BUCKET\n",
    "# print(vai_gcs_bucket)\n",
    "# gcs_stagging_folder =f\"{pipeline_run_name}/Stagging\"\n",
    "# print(gcs_stagging_folder)\n",
    "# gcs_errored_folder =f\"{pipeline_run_name}/Errored\"\n",
    "# print(gcs_errored_folder)\n",
    "# snf_account = VAI_SNF_ACCOUNT\n",
    "# print(snf_account)\n",
    "# snf_user = VAI_SNF_USER\n",
    "# print(snf_user)\n",
    "# snf_private_key = VAI_SNF_PRIVATE_KEY\n",
    "# # print(snf_private_key)\n",
    "# snf_private_key_pwd = VAI_SNF_PRIVATE_KEY_PWD\n",
    "# print(snf_private_key_pwd)\n",
    "# snf_warehouse = VAI_SNF_WAREHOUSE\n",
    "# print(snf_warehouse)\n",
    "# snf_schema = VAI_SNF_SCHEMA\n",
    "# print(snf_schema)\n",
    "# snf_database = VAI_SNF_DATABASE\n",
    "# print(snf_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da9211-4634-4f47-9226-0bf6e6d7a165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    # base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    ")\n",
    "def write_data_to_snowflake(\n",
    "    pipeline_run_name: str,\n",
    "    gcp_project_id: str,\n",
    "    gcp_project_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    snf_account: str,\n",
    "    snf_user: str,\n",
    "    snf_private_key: str,\n",
    "    snf_private_key_pwd: str,\n",
    "    snf_warehouse: str,\n",
    "    snf_database: str,\n",
    "    snf_schema: str\n",
    "):\n",
    "    import io, logging\n",
    "    import pytz\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from google.cloud import storage\n",
    "    import snowflake.connector as sc\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "    \n",
    "    def insert_new_records(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_databse,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        df\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "\n",
    "        Steps:\n",
    "        1. Fetches existing IDs from table.\n",
    "        2. Filters out rows with existing IDs from DataFrame.\n",
    "        3. Adds 'LOAD_DATE_UTC' column with current UTC timestamp.\n",
    "        4. Inserts only new records.\n",
    "\n",
    "        Args:\n",
    "            conn: Snowflake connection object.\n",
    "            table_name (str): Name of the target table.\n",
    "            df (pd.DataFrame): DataFrame containing the data (must have 'CONTACT_ID' column).\n",
    "\n",
    "        Returns:\n",
    "            int: Number of inserted records.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_databse,\n",
    "                'schema': snf_schema\n",
    "            }\n",
    "\n",
    "            conn = sc.connect(**conn_params)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Step 1: Get existing IDs from Snowflake table\n",
    "            cursor.execute(f\"SELECT DISTINCT(CONTACT_ID) FROM {table_name}\")\n",
    "            existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "            # Step 2: Filter DataFrame to keep only new records\n",
    "            new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "\n",
    "            if new_records_df.empty:\n",
    "                logger.info(\"No new records to insert\")\n",
    "                return 0\n",
    "\n",
    "            # Step 3: Add UTC timestamp column\n",
    "            utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_records_df = new_records_df.copy()  # Avoid modifying original df\n",
    "            new_records_df[\"LOAD_DATE\"] = utc_now  # Add new column\n",
    "\n",
    "            # Step 4: Insert new records into Snowflake\n",
    "            success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "\n",
    "            logger.info(f\"Inserted {nrows} new records with UTC load date\")\n",
    "            logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return nrows\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "\n",
    "    # Function to read CSV from GCS\n",
    "    def read_gcs_csv(file_path):\n",
    "        blob = bucket.blob(file_path)\n",
    "        csv_data = blob.download_as_text()\n",
    "        return pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "    try:\n",
    "        gcs_stagging_folder=f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_errored_folder=f\"{pipeline_run_name}/Errored\"\n",
    "        \n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "\n",
    "        # Read Inter & Intra Call DataFrames\n",
    "        inter_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_inter_call_df.csv\")\n",
    "        inter_call_df.columns = inter_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "        intra_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_intra_call_df.csv\")\n",
    "        intra_call_df.columns = intra_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "\n",
    "        logger.info(f\"Started: writing data to snowflake.\")\n",
    "        table_name ='SRC_GCP_INTER_CALLS'    \n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            inter_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTER_CALLS: Inserted records #{len(inter_call_df)}\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        table_name ='SRC_GCP_INTRA_CALLS'\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            intra_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTRA_CALLS: Inserted records #{len(intra_call_df)}\")\n",
    "        logger.info(f\"Completed: writing data to snowflake.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"VAI Audio to KPI Pipeline\",\n",
    "    description=\"Process Amazon Audio Transcripts into KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline to:\n",
    "    1. List calls from S3 and download them to GCS.\n",
    "    2. Process each transcript in parallel using Kubeflow Pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: List and Download Calls from S3 to GCS\n",
    "    get_calls_to_process = list_download_calls_s3_to_gcs(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        project_id=project_id,\n",
    "        secret_id=secret_id,\n",
    "        version_id=version_id\n",
    "    )\n",
    "\n",
    "    # # Step 2: Parallel Process Transcripts (linked to Step 1)\n",
    "    # process_calls = process_transcripts(\n",
    "    #     log_file=log_file,\n",
    "    #     pipeline_run_name=pipeline_run_name,\n",
    "    #     gcp_project_id=gcp_project_id,\n",
    "    #     gcp_project_location=gcp_project_location,\n",
    "    #     vai_gcs_bucket=vai_gcs_bucket,\n",
    "    #     snf_account=snf_account,\n",
    "    #     snf_user=snf_user,\n",
    "    #     snf_private_key=snf_private_key,\n",
    "    #     snf_private_key_pwd=snf_private_key_pwd,\n",
    "    #     snf_warehouse=snf_warehouse,\n",
    "    #     snf_catsubcat_databse=snf_catsubcat_databse,\n",
    "    #     snf_catsubcat_schema=snf_catsubcat_schema,\n",
    "    #     snf_catsubcat_view=snf_catsubcat_view,\n",
    "    #     max_parallelism=max_parallelism\n",
    "    # )\n",
    "    \n",
    "    # Enforce sequential execution\n",
    "    # process_calls.after(get_calls_to_process)\n",
    "    \n",
    "# Step 3: Write the Data to Snowflake\n",
    "#     persist_to_snowflake = write_data_to_snowflake(\n",
    "#         pipeline_run_name=pipeline_run_name,\n",
    "#         gcp_project_id=gcp_project_id,\n",
    "#         gcp_project_location=gcp_project_location,\n",
    "#         vai_gcs_bucket=vai_gcs_bucket,\n",
    "#         snf_account=snf_account,\n",
    "#         snf_user=snf_user,\n",
    "#         snf_private_key=snf_private_key,\n",
    "#         snf_private_key_pwd=snf_private_key_pwd,\n",
    "#         snf_warehouse=snf_warehouse,\n",
    "#         snf_database=snf_database,\n",
    "#         snf_schema=snf_schema\n",
    "#     )\n",
    "    \n",
    "#     # Enforce sequential execution\n",
    "#     persist_to_snowflake.after(process_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, 'cx-voiceai-process-calls.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38487e72-e52a-43c7-917f-0d305ad627a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8625e81-29a4-44f6-90bd-c2889745f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = timestamp = datetime.now(UTC).strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"dev-posigen\", location=\"us-central1\")\n",
    "\n",
    "# Create pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    job_id = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    template_path = f\"cx-voiceai-process-calls.yaml\",\n",
    "    pipeline_root = f\"gs://dev-aws-connect-audio\",\n",
    "    project = \"dev-posigen\",\n",
    "    location = \"us-central1\",\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        \"pipeline_run_name\": f\"cx-voiceai-process-calls-{TIMESTAMP}\",\n",
    "        \"project_id\": \"dev-posigen\",\n",
    "        \"secret_id\": \"dev-cx-voiceai\",\n",
    "        \"version_id\": \"1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ac4-60c0-4607-aabf-23f5f31f9ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e56a67-28e5-4d7f-82a2-263e6d06e901",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b3a463-dbc3-465c-a7c3-9459f26b2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vai-pipeline-run-2025-03-26-14-16-45?project=275963620760\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-26-14-16-45\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362431c-209a-4f36-b98a-b1894eb429d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f08ee6e-68a9-4c66-be23-c2db71c0df49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b18bf-ff1c-4550-a5c4-8b7f798aed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = kfp.Client()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
