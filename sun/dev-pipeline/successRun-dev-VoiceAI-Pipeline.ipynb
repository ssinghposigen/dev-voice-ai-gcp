{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "from kfp.dsl import component\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import secretmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16409781-492e-4507-a1d3-50d11a3e6bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02cdc275-ab65-4e18-8755-d600371372d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "    \n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "    \n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    \n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    \n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    \n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a6d605d-01cd-445b-9530-b810b4c7bffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"dev-posigen\"\n",
    "secret_id = \"dev-cx-voiceai\"\n",
    "version_id=\"1\"\n",
    "configs = access_secret(project_id, secret_id)\n",
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23fa675f-1adb-426d-9bb0-ec81b5c49174",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_objects = 1\n",
    "\n",
    "# Generate timestamp in a Vertex-compatible format\n",
    "TIMESTAMP = datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "VAI_GCP_PROJECT_ID = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "VAI_GCP_PROJECT_LOCATION = configs.get(\"VAI_GCP_PROJECT_LOCATION\")\n",
    "VAI_GCP_PIPELINE_BUCKET = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "VAI_GCP_PIPELINE_NAME = \"fetch_file_from_s3\"\n",
    "VAI_GCP_PIPELINE_RUN_NAME = f\"{configs.get(\"VAI_GCP_PIPELINE_NAME\")}-{TIMESTAMP}\"\n",
    "GCP_PIPELINE_ROOT = f\"gs://{VAI_GCP_PIPELINE_BUCKET}/{VAI_GCP_PIPELINE_RUN_NAME}\"\n",
    "\n",
    "VAI_AWS_ACCESS_KEY = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "VAI_AWS_SECRET_KEY = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "\n",
    "VAI_S3_ANALYSIS_BUCKET = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "VAI_S3_TRANSCRIPTS_LOCATION = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f11fb1-562e-4d6b-9470-c6d80b21445c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7a7884-05bf-4ae3-8755-0bcc5982c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "def setup_logger():\n",
    "    \"\"\"Set up a logger for the pipeline run.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbac45-d2c4-4163-a463-9b768a0ff1e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1e6715d-d5d0-4d5a-9c54-6b1d13a02a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exception(\n",
    "    file_id: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    run_folder: str,\n",
    "    error_folder: str,\n",
    "    error_message: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    ")\n",
    "def list_s3_files_to_gcs(\n",
    "    pipeline_run_name: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    s3_analysis_bucket: str,\n",
    "    s3_transcript_location: str,\n",
    "    vai_gcs_bucket: str,  \n",
    "    max_objects: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch audio file from S3 and return it as a BytesIO object\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Generate pipeline folder paths\n",
    "        staging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "        errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "        \n",
    "        # Initialize GCS Client\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        \n",
    "        # ðŸ”¹ Corrected: Create empty folders directly\n",
    "        for folder in [staging_folder, errored_folder]:\n",
    "            blob = bucket.blob(f\"{folder}/\")\n",
    "            blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "        \n",
    "        logging.info(f\"Created folders: {staging_folder} and {errored_folder} in GCS.\")\n",
    "        \n",
    "        # Initialize S3 Client\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Fetching New Transcripts to process\")\n",
    "        response = s3_client.list_objects_v2(Bucket=s3_analysis_bucket, Prefix=s3_transcript_location)\n",
    "        \n",
    "        all_files = []\n",
    "        for obj in response.get('Contents', []):\n",
    "            if obj['Key'].endswith('.json'):\n",
    "                file_path = obj['Key']\n",
    "                s3_ts = obj['LastModified']\n",
    "                \n",
    "                call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                TIMESTAMP = datetime.strptime(file_path.split('analysis_')[-1].split('.')[0].replace('_', ':'), '%Y-%m-%dT%H:%M:%SZ')    \n",
    "        \n",
    "                all_files.append({\n",
    "                    'File': file_path,\n",
    "                    'ID': call_id,\n",
    "                    'File_TIMESTAMP': TIMESTAMP,\n",
    "                    'File_Date': TIMESTAMP.strftime('%Y-%m-%d'),\n",
    "                    'File_Time': TIMESTAMP.strftime('%H:%M:%S'),\n",
    "                    'S3_TIMESTAMP': s3_ts,\n",
    "                    'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                    'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                })\n",
    "        \n",
    "        files_sorted = pd.DataFrame(all_files).sort_values(['File_TIMESTAMP'], ascending=False, ignore_index=True)\n",
    "        \n",
    "        # Write DataFrame to the correct GCS path\n",
    "        csv_path = f\"gs://{vai_gcs_bucket}/{staging_folder}/{pipeline_run_name}_S3_Transcripts_fetched.csv\"\n",
    "        files_sorted.to_csv(csv_path, index=False)\n",
    "        \n",
    "        logger.info(f\"Written Transcripts to GCS Bucket: {csv_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, errored_folder, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Process Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdde1c-b340-427c-9092-0b1e96dd7c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d5ca7-03cd-4629-ac8e-50df45336bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=VAI_GCP_PIPELINE_RUN_NAME,\n",
    "    description=\"Proces Amazon Audio Transcripts to KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    s3_analysis_bucket: str,\n",
    "    s3_transcript_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    max_objects: int\n",
    "):\n",
    "    fetch_transcripts = list_s3_files_to_gcs(\n",
    "        pipeline_run_name=VAI_GCP_PIPELINE_RUN_NAME,\n",
    "        aws_access_key=VAI_AWS_ACCESS_KEY,\n",
    "        aws_secret_key=VAI_AWS_SECRET_KEY,\n",
    "        s3_analysis_bucket=VAI_S3_ANALYSIS_BUCKET,\n",
    "        s3_transcript_location=VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "        vai_gcs_bucket=VAI_GCP_PIPELINE_BUCKET,\n",
    "        max_objects=max_objects\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, f'{VAI_GCP_PIPELINE_NAME}.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ac4-60c0-4607-aabf-23f5f31f9ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117d409-5818-4656-ac27-64b7ea9ab747",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run in Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f82dae3d-a209-41df-af85-97d715d94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=VAI_GCP_PROJECT_ID, location=VAI_GCP_PROJECT_LOCATION)\n",
    "\n",
    "max_objects = 1\n",
    "\n",
    "# Create pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name = f\"{VAI_GCP_PIPELINE_RUN_NAME}\".lower(),\n",
    "    job_id = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    template_path = f\"{VAI_GCP_PIPELINE_NAME}.yaml\",\n",
    "    pipeline_root = f\"gs://{VAI_GCP_PIPELINE_BUCKET}\",\n",
    "    project = VAI_GCP_PROJECT_ID,\n",
    "    location = VAI_GCP_PROJECT_LOCATION,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        \"pipeline_run_name\":VAI_GCP_PIPELINE_RUN_NAME,\n",
    "        \"aws_access_key\":VAI_AWS_ACCESS_KEY,\n",
    "        \"aws_secret_key\":VAI_AWS_SECRET_KEY,\n",
    "        \"s3_analysis_bucket\":VAI_S3_ANALYSIS_BUCKET,\n",
    "        \"s3_transcript_location\":VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "        \"vai_gcs_bucket\":VAI_GCP_PIPELINE_BUCKET,\n",
    "        \"max_objects\":max_objects\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0b3a463-dbc3-465c-a7c3-9459f26b2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:28:16,332 - INFO - Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vai-pipeline-run-2025-03-05-11-28-16?project=275963620760\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-05-11-28-16\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362431c-209a-4f36-b98a-b1894eb429d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
