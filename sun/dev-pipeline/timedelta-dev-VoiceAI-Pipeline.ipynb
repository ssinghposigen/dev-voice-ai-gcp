{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "from kfp.dsl import component\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import secretmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16409781-492e-4507-a1d3-50d11a3e6bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ad2c9-6548-4d9b-8828-f271bc72f0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "    \n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "    \n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    \n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    \n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    \n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7d28d8-4037-4cd2-adda-70267502033c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"dev-posigen\"\n",
    "secret_id = \"dev-cx-voiceai\"\n",
    "version_id=\"1\"\n",
    "configs = access_secret(project_id, secret_id)\n",
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fa675f-1adb-426d-9bb0-ec81b5c49174",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_objects = 1\n",
    "\n",
    "# Generate timestamp in a Vertex-compatible format\n",
    "TIMESTAMP = datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "VAI_GCP_PROJECT_ID = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "VAI_GCP_PROJECT_LOCATION = configs.get(\"VAI_GCP_PROJECT_LOCATION\")\n",
    "VAI_GCP_PIPELINE_BUCKET = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "VAI_GCP_PIPELINE_NAME = \"fetch_file_from_s3\"\n",
    "VAI_GCP_PIPELINE_RUN_NAME = f\"{configs.get(\"VAI_GCP_PIPELINE_NAME\")}-{TIMESTAMP}\"\n",
    "GCP_PIPELINE_ROOT = f\"gs://{VAI_GCP_PIPELINE_BUCKET}/{VAI_GCP_PIPELINE_RUN_NAME}\"\n",
    "\n",
    "VAI_AWS_ACCESS_KEY = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "VAI_AWS_SECRET_KEY = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "\n",
    "VAI_S3_ANALYSIS_BUCKET = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "VAI_S3_TRANSCRIPTS_LOCATION = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f11fb1-562e-4d6b-9470-c6d80b21445c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7a7884-05bf-4ae3-8755-0bcc5982c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "def setup_logger():\n",
    "    \"\"\"Set up a logger for the pipeline run.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbac45-d2c4-4163-a463-9b768a0ff1e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e6715d-d5d0-4d5a-9c54-6b1d13a02a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exception(\n",
    "    file_id,\n",
    "    vai_gcs_bucket,\n",
    "    run_folder,\n",
    "    error_folder,\n",
    "    error_message\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.component(\n",
    "#     base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    "# )\n",
    "# def list_s3_files_to_gcs(\n",
    "#     pipeline_run_name: str,\n",
    "#     aws_access_key: str,\n",
    "#     aws_secret_key: str,\n",
    "#     s3_analysis_bucket: str,\n",
    "#     s3_transcript_location: str,\n",
    "#     vai_gcs_bucket: str,  \n",
    "#     max_objects: int\n",
    "# ):\n",
    "\"\"\"\n",
    "Fetch audio file from S3 and return it as a BytesIO object\n",
    "\"\"\"\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import logging\n",
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    # Generate pipeline folder paths\n",
    "    staging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "    transcripts_folder = f\"{staging_folder}/Transcripts\"\n",
    "    max_objects = f\"{pipeline_run_name}/Errored\"\n",
    "\n",
    "    # Initialize GCS Client\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "    # Create empty folders directly\n",
    "    for folder in [staging_folder, transcripts_folder, max_objects]:\n",
    "        blob = bucket.blob(f\"{folder}/\")\n",
    "        blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "\n",
    "    logging.info(f\"Created folders: {staging_folder}, {transcripts_folder}, and {max_objects} in GCS.\")\n",
    "\n",
    "    # Initialize S3 Client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    all_files = []\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=s3_transcript_location)\n",
    "\n",
    "    # Get current timestamp and calculate 2-hour window\n",
    "    current_time = datetime.utcnow()\n",
    "    time_threshold = current_time - timedelta(hours=2)\n",
    "\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            file_path = obj['Key']\n",
    "\n",
    "            # Skip non-JSON files\n",
    "            if not file_path.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "\n",
    "            # Extract timestamp from filename\n",
    "            try:\n",
    "                call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the file falls within the last 2 hours\n",
    "            if call_timestamp >= time_threshold:\n",
    "                all_files.append({\n",
    "                    'File': file_path,\n",
    "                    'ID': call_id,\n",
    "                    'File_Timestamp': call_timestamp,\n",
    "                    'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                    'File_Time': call_timestamp.time().strftime('%H:%M:%S')\n",
    "                })\n",
    "\n",
    "    if all_files:\n",
    "        df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "    else:\n",
    "        df_calls_list = pd.DataFrame()\n",
    "\n",
    "    # Write the DataFrame to GCS\n",
    "    csv_path = f\"gs://{vai_gcs_bucket}/{staging_folder}/{pipeline_run_name}_s3_Transcripts_fetched.csv\"\n",
    "    df_calls_list.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "\n",
    "    # Bulk Download filtered files\n",
    "    if not df_calls_list.empty:\n",
    "        for _, row in df_calls_list.iterrows():\n",
    "            file_key = row['File']\n",
    "            local_file_path = f\"/tmp/{file_key.split('/')[-1]}\"  # Temporary local storage\n",
    "            gcs_blob_path = f\"{transcripts_folder}/{file_key.split('/')[-1]}\"\n",
    "\n",
    "            try:\n",
    "                # Download file from S3\n",
    "                s3_client.download_file(s3_analysis_bucket, file_key, local_file_path)\n",
    "\n",
    "                # Upload to GCS\n",
    "                blob = bucket.blob(gcs_blob_path)\n",
    "                blob.upload_from_filename(local_file_path)\n",
    "                logger.info(f\"Downloaded and uploaded: {file_key} -> {gcs_blob_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download {file_key}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, max_objects, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Process Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934345a-030e-4959-9d23-9cb6ad4cffe6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initiate Master Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a191893-c40d-4495-b37f-f0736faacac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_master_dataframes(\n",
    "    stagging_folder,\n",
    "    bucket_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Checks for 'df_intra_calls_data.csv' and 'df_inter_calls_data.csv' in GCS.\n",
    "    If files exist, loads them into Pandas DataFrames; otherwise, creates empty DataFrames.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Define file paths in GCS\n",
    "    intra_calls_path = f\"{stagging_folder}/df_intra_calls_data.csv\"\n",
    "    inter_calls_path = f\"{stagging_folder}/df_inter_calls_data.csv\"\n",
    "\n",
    "    # Load or create intra_calls DataFrame\n",
    "    if storage.Blob(bucket=bucket, name=stagging_folder).exists(client):\n",
    "        logger.info(f\"df_intra_calls_data.csv exists in GCS.\")\n",
    "        blob = bucket.blob(intra_calls_path)\n",
    "        df_intra_calls_data = pd.read_csv(blob.open(\"r\"))\n",
    "        df_intra_calls_data[\"contact_id\"] = df_intra_calls_data[\"contact_id\"].astype('string')\n",
    "    else:\n",
    "        logger.info(f\"df_intra_calls_data.csv does not exist in GCS. Creating an empty DataFrame.\")\n",
    "        df_intra_calls_data = pd.DataFrame()\n",
    "\n",
    "    # Load or create inter_calls DataFrame\n",
    "    if storage.Blob(bucket=bucket, name=stagging_folder).exists(client):\n",
    "        logger.info(f\"df_inter_calls_data.csv exists in GCS.\")\n",
    "        blob = bucket.blob(inter_calls_path)\n",
    "        df_inter_calls_data = pd.read_csv(blob.open(\"r\"))\n",
    "        df_inter_calls_data[\"contact_id\"] = df_inter_calls_data[\"contact_id\"].astype('string')\n",
    "    else:\n",
    "        logger.info(f\"df_inter_calls_data.csv does not exist in GCS. Creating an empty DataFrame.\")\n",
    "        df_inter_calls_data = pd.DataFrame()\n",
    "\n",
    "    return df_intra_calls_data, df_inter_calls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d0546-c656-4dda-b1c4-6696c326c75a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fetch Transcript from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb48cb85-dbeb-4931-91ad-fd57d2d60911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript_from_s3(\n",
    "    contact_id,\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    s3_analysis_bucket,\n",
    "    file_key\n",
    "):\n",
    "    \"\"\"\n",
    "    Read Transcript JSON content from a specific file in S3.\n",
    "    \n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_key: Full path/key of the JSON file\n",
    "    :return: Parsed JSON content\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = s3_client.get_object(Bucket=s3_analysis_bucket, Key=file_key)    \n",
    "        # Read the content\n",
    "        json_content = response['Body'].read().decode('utf-8')    \n",
    "        # Parse JSON\n",
    "        return json.loads(json_content)\n",
    "    \n",
    "    except Exception as e:\n",
    "        handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, max_objects, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fa583-76af-4d7a-ad28-a62edf2fe2f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Intra Call DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba596c89-f564-4801-bec2-272ada3d6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pii_in_captions(\n",
    "    contact_id,\n",
    "    df,\n",
    "    project_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Masks PII data in the 'caption' column of a pandas DataFrame using Google Cloud DLP API.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with a 'caption' column to process\n",
    "        project_id (str): Your Google Cloud project ID\n",
    "        location_id (str, optional): GCP location ID. Defaults to \"global\".\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with masked PII in the 'caption' column\n",
    "    \"\"\"\n",
    "    logger.info(f\"{contact_id}: Masking PII Data\")\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    masked_df = df.copy()\n",
    "    \n",
    "    # Concatenate all captions for bulk processing\n",
    "    all_captions = \"\\n\".join(df['caption'].astype(str).tolist())\n",
    "    \n",
    "    # Initialize DLP client\n",
    "    dlp_client = dlp_v2.DlpServiceClient()\n",
    "    \n",
    "    # Specify the parent resource name\n",
    "    parent = f\"projects/{project_id}\"\n",
    "    \n",
    "    # Configure inspection config\n",
    "    inspect_config = {\n",
    "        \"info_types\": [\n",
    "            {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "            {\"name\": \"STREET_ADDRESS\"},\n",
    "            {\"name\": \"IP_ADDRESS\"},\n",
    "            {\"name\": \"ORGANIZATION_NAME\"}\n",
    "        ],\n",
    "        # Custom exclusion rules\n",
    "        \"rule_set\": [\n",
    "            {\n",
    "                \"info_types\": [{\"name\": \"ORGANIZATION_NAME\"}],\n",
    "                \"rules\": [\n",
    "                    {\n",
    "                        \"exclusion_rule\": {\n",
    "                            \"dictionary\": {\n",
    "                                \"word_list\": {\n",
    "                                    \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                                }\n",
    "                            },\n",
    "                            \"matching_type\": dlp_v2.MatchingType.MATCHING_TYPE_FULL_MATCH,\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Configure deidentification\n",
    "    # Important: Use different transformations for different info types\n",
    "    deidentify_config = {\n",
    "        \"info_type_transformations\": {\n",
    "            \"transformations\": [\n",
    "                # Mask credit cards, addresses, IPs, organizations (except posigen)\n",
    "                {\n",
    "                    \"info_types\": [\n",
    "                        {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                        {\"name\": \"STREET_ADDRESS\"},\n",
    "                        {\"name\": \"IP_ADDRESS\"},\n",
    "                        {\"name\": \"ORGANIZATION_NAME\"}\n",
    "                    ],\n",
    "                    \"primitive_transformation\": {\n",
    "                        \"character_mask_config\": {\n",
    "                            \"masking_character\": \"*\",\n",
    "                            \"number_to_mask\": 100  # Mask all characters\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create deidentify request\n",
    "    item = {\"value\": all_captions}\n",
    "    \n",
    "    # Call the DLP API\n",
    "    response = dlp_client.deidentify_content(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"deidentify_config\": deidentify_config,\n",
    "            \"inspect_config\": inspect_config,\n",
    "            \"item\": item,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Split the processed content back into separate captions\n",
    "    processed_captions = response.item.value.split(\"\\n\")\n",
    "    \n",
    "    # Additional processing to mask all digits with asterisks\n",
    "    # (while preserving the DLP API masking for specific PII types)\n",
    "    def mask_digits(text):\n",
    "        # Function to replace digits with asterisks, preserving DLP masked content\n",
    "        return re.sub(r'(?<!\\*)\\d(?!\\*)', '*', text)\n",
    "    \n",
    "    # Apply the digit masking function to each processed caption\n",
    "    masked_captions = [mask_digits(caption) for caption in processed_captions]\n",
    "    \n",
    "    # Update the DataFrame with masked captions\n",
    "    masked_df['caption'] = masked_captions[:len(masked_df)]\n",
    "    \n",
    "    logger.info(f\"{contact_id}: Completed Masking PII Data\")\n",
    "    return masked_df\n",
    "\n",
    "    \n",
    "def get_sentiment_label(row):\n",
    "    # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "    if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "        return 'Positive'\n",
    "    elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def get_different_times(intra_call):\n",
    "    # Apply formatting to both time columns\n",
    "    intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "    intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "    intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "    intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "    intra_call['load_date'] = datetime.now()\n",
    "\n",
    "    # Dropping time formatted columns\n",
    "    intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "    return intra_call\n",
    "\n",
    "def get_sentiment_scores(\n",
    "    contact_id,\n",
    "    text_list\n",
    "):\n",
    "    logger.info(f\"{contact_id}: Calculating Caption Sentiments.\")\n",
    "    dict_sentiments = []\n",
    "    for text in text_list:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model_sentiment(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "        merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "        dict_sentiments.append(merged_dict)\n",
    "\n",
    "    df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "    df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "    logger.info(f\"{contact_id}: Completed calculating Caption Sentiments.\")\n",
    "    \n",
    "    return df_dict_sentiments\n",
    "\n",
    "def process_transcript(\n",
    "    contact_id,\n",
    "    transcript_data,\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Pre-process the transcript loaded from S3 Buckets:\n",
    "    1. Load the transcript as Pandas Dataframe.\n",
    "    2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "    3. Format the time in minutes and seconds.\n",
    "    4. Rename the columns for better understanding.\n",
    "    \"\"\"\n",
    "    logger.info(f\"{contact_id}: Loading the Transcript as Pandas Dataframe.\")\n",
    "    transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "    # Select the relevant Columns\n",
    "    columns_to_select = [\n",
    "        'BeginOffsetMillis',\n",
    "        'EndOffsetMillis',\n",
    "        'ParticipantId',\n",
    "        'Content'\n",
    "    ]\n",
    "    formatted_df = transcript_df[columns_to_select].copy()\n",
    "    \n",
    "    # Optionally rename columns to reflect their new format\n",
    "    formatted_df = formatted_df.rename(columns={\n",
    "        'BeginOffsetMillis': 'Begin_Offset',\n",
    "        'EndOffsetMillis': 'End_Offset',\n",
    "        'Content': 'caption',\n",
    "        'Sentiment': 'sentiment_label',\n",
    "        'ParticipantId': 'speaker_tag'\n",
    "    })\n",
    "\n",
    "    # Inserting the Call ID:\n",
    "    formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "    formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "    logger.info(f\"{contact_id}: Returning formated DataFrame.\")\n",
    "    return formatted_df\n",
    "    \n",
    "def create_intra_call_df(\n",
    "    contact_id,\n",
    "    vai_gcs_bucket,\n",
    "    pipeline_run_name,\n",
    "    max_objects,\n",
    "    transcript_data,\n",
    "    tokenizer\n",
    "):\n",
    "    intra_call = process_transcript(contact_id, transcript_data, tokenizer)\n",
    "    df_sentiment_scores = get_sentiment_scores(contact_id, intra_call.caption.to_list())\n",
    "    intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "    intra_call = get_different_times(intra_call)\n",
    "    intra_call = mask_pii_in_captions(contact_id, intra_call, gcp_project_id)\n",
    "    \n",
    "    return intra_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1f2d1-af5e-4e84-8298-93d520823d00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Inter Call Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de67f361-bf2b-4152-a8bd-505af05e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_newline_string(data):\n",
    "    \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for key, value in data.items():\n",
    "        formatted_str += f\"{key}:\\n\"\n",
    "        for item in value:\n",
    "            formatted_str += f\"  - {item}\\n\"\n",
    "    return formatted_str.strip()\n",
    "    \n",
    "\n",
    "class CategoryValidator:\n",
    "    def __init__(self, catsubcat_conn_params, snf_catsubcat_view):\n",
    "        \"\"\"\n",
    "        Initialize with category mapping from a Snowflake View.\n",
    "        :param snowflake_conn_paramsionary containing Snowflake connection details.\n",
    "        :param view_name: Name of the Snowflake View containing category mappings.\n",
    "        \"\"\"\n",
    "        self.catsubcat_conn_params = catsubcat_conn_params\n",
    "        self.snf_catsubcat_view = snf_catsubcat_view\n",
    "        self.category_mapping = self._fetch_category_mapping_from_snowflake()\n",
    "        self.valid_categories = set(self.category_mapping['CATEGORY'].unique())\n",
    "        self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "    def _fetch_category_mapping_from_snowflake(self):\n",
    "        \"\"\"Fetch Category-Subcategory mapping from a Snowflake View and return as DataFrame.\"\"\"\n",
    "        try:\n",
    "            conn = sc.connect(**self.catsubcat_conn_params)\n",
    "            query = f\"SELECT CATEGORY, SUBCATEGORY FROM {self.snf_catsubcat_view}\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            conn.close()\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error fetching category mapping from Snowflake: {e}\")\n",
    "\n",
    "    def _create_category_mapping(self):\n",
    "        \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "        mapping = {}\n",
    "        for _, row in self.category_mapping.iterrows():\n",
    "            if row['CATEGORY'] not in mapping:\n",
    "                mapping[row['CATEGORY']] = set()\n",
    "            mapping[row['CATEGORY']].add(row['SUBCATEGORY'])\n",
    "        return mapping\n",
    "\n",
    "    def validate_category(self, category):\n",
    "        \"\"\"Check if category is valid.\"\"\"\n",
    "        return category in self.valid_categories\n",
    "\n",
    "    def validate_subcategory(self, category, subcategory):\n",
    "        \"\"\"Check if subcategory is valid for given category.\"\"\"\n",
    "        return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "\n",
    "    def get_valid_subcategories(self, category):\n",
    "        \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "        return self.category_subcategory_map.get(category, set())\n",
    "\n",
    "class CallSummary(BaseModel):\n",
    "    summary: str = Field(..., max_length=500)\n",
    "    # key_points: List[str] = Field(..., max_items=5)\n",
    "    # outcome = Field(..., max_length=200)\n",
    "    # follow_up_recommendations: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class CallTopic(BaseModel):\n",
    "    primary_topic: str = Field(..., max_length=100)\n",
    "    category: str = Field(..., max_length=100)\n",
    "    sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "    def validate_category_mapping(self, category_validator: CategoryValidator):\n",
    "        \"\"\"Validate category and subcategory against mapping\"\"\"\n",
    "        if not category_validator.validate_category(self.category):\n",
    "            logger.error(f\"Invalid category: {self.category}\")\n",
    "        if not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "            logger.error(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'\")\n",
    "\n",
    "class AgentCoaching(BaseModel):\n",
    "    strengths: List[str] = Field(..., max_items=3)\n",
    "    improvement_areas: List[str] = Field(..., max_items=3)\n",
    "    specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "    skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class TranscriptAnalysis(BaseModel):\n",
    "    call_summary: CallSummary\n",
    "    call_topic: CallTopic\n",
    "    agent_coaching: AgentCoaching\n",
    "\n",
    "class KPIExtractor:\n",
    "    def __init__(self, project_id, location):\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "        self.category_validator = CategoryValidator(catsubcat_conn_params, snf_catsubcat_view)\n",
    "        \n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 1024,\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40,\n",
    "            \"response_format\": \"json\"\n",
    "        }\n",
    "        \n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        }\n",
    "\n",
    "    def get_categories_prompt(self) -> str:\n",
    "        \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "        categories_prompt = []\n",
    "        \n",
    "        for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "            if category is None:  # Skip if category is None\n",
    "                continue\n",
    "            \n",
    "            # Ensure subcategories are valid (remove None values)\n",
    "            valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "            \n",
    "            if valid_subcategories:\n",
    "                subcats = ', '.join(sorted(valid_subcategories))\n",
    "            else:\n",
    "                subcats = \"No defined subcategories\"\n",
    "            \n",
    "            categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "        \n",
    "        return '\\n'.join(categories_prompt)\n",
    "\n",
    "    def create_prompt(self, transcript):\n",
    "        \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "        categories_guidance = self.get_categories_prompt()\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "        Keep responses concise, specific, and actionable.\n",
    "\n",
    "        Guidelines:\n",
    "        - Call summary should be factual and highlight key interactions\n",
    "        - Topics and categories MUST match the following valid mappings:\n",
    "        {categories_guidance}\n",
    "        - Coaching points should be specific and actionable\n",
    "        - All responses must follow the exact structure specified\n",
    "        - Ensure all lists have the specified maximum number of items\n",
    "        - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "        Transcript:\n",
    "        {transcript}\n",
    "\n",
    "        Required Output Structure:\n",
    "        {{\n",
    "            \"call_summary\": {{\n",
    "                \"summary\": \"3-4 line overview of the call\"\n",
    "            }},\n",
    "            \"call_topic\": {{\n",
    "                \"primary_topic\": \"Main topic of discussion\",\n",
    "                \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "            }},\n",
    "            \"agent_coaching\": {{\n",
    "                \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        1. Maintain exact JSON structure\n",
    "        2. No additional fields or comments\n",
    "        3. No markdown formatting\n",
    "        4. Ensure all arrays have the exact number of items specified\n",
    "        5. Keep all text concise and professional\n",
    "        6. Do not mention any PII information such as Customer Name etc.\n",
    "        7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "        \"\"\"\n",
    "\n",
    "    def extract_json(self, response):\n",
    "        \"\"\"Extract valid JSON from response\"\"\"\n",
    "        match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "        else:\n",
    "            json_str = response.strip()\n",
    "        \n",
    "        try:\n",
    "           return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "           logger.error(\"Invalid JSON response\")\n",
    "           pass\n",
    "\n",
    "    def validate_response(self, response_json, contact_id = None):\n",
    "        \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "        try:\n",
    "            # First validate basic structure with Pydantic\n",
    "            analysis = TranscriptAnalysis(**response_json)\n",
    "            \n",
    "            # Then validate category mapping\n",
    "            analysis.call_topic.validate_category_mapping(self.category_validator)\n",
    "            \n",
    "            return analysis\n",
    "        except ValidationError as e:\n",
    "            logger.error(f\"{contact_id if contact_id else ''}: Pydantic validation error - {e}\")\n",
    "            pass\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"{contact_id if contact_id else ''}: Category validation error - {e}\")\n",
    "            pass\n",
    "\n",
    "    def extract_genai_kpis(self, transcript, contact_id = None):\n",
    "        \"\"\"Extract KPIs from transcript with validation\"\"\"\n",
    "        try:\n",
    "            # Generate prompt\n",
    "            prompt = self.create_prompt(transcript)\n",
    "            \n",
    "            # Get response from Gemini\n",
    "            response = self.model.generate_content(\n",
    "                prompt\n",
    "                # generation_config=self.generation_config,\n",
    "                # safety_settings=self.safety_settings\n",
    "            )\n",
    "\n",
    "            \n",
    "            logger.debug(f\"Gemini API Response: {response}\")\n",
    "            \n",
    "            # Parse JSON response\n",
    "            response_json = self.extract_json(response.text)\n",
    "            \n",
    "            # Validate response structure and categories\n",
    "            validated_response = self.validate_response(response_json, contact_id)\n",
    "            \n",
    "            return validated_response.model_dump()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"{contact_id if contact_id else ''}: Error extracting KPIs: {str(e)}\")\n",
    "            pass\n",
    "\n",
    "    \n",
    "def create_inter_call_df(\n",
    "    contact_id,\n",
    "    vai_gcs_bucket,\n",
    "    vai_gcs_folder_stagging,\n",
    "    max_objects,\n",
    "    pipeline_run_name,\n",
    "    transcript_data,\n",
    "    ac_last_modified_date,\n",
    "    df_intra_call,\n",
    "    gcp_project_id,\n",
    "    gcp_project_location,\n",
    "    snf_account,\n",
    "    snf_user,\n",
    "    snf_private_key_file,\n",
    "    snf_private_key_pwd,\n",
    "    snf_warehouse,\n",
    "    snf_catsubcat_databse,\n",
    "    snf_catsubcat_schema,\n",
    "    snf_catsubcat_view\n",
    "):\n",
    "    try:\n",
    "        logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "        extractor = KPIExtractor(gcp_project_id, gcp_project_location)\n",
    "        transcript = \" \".join(df_intra_call.caption)\n",
    "        call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "        logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "        \n",
    "        logger.info(f\"{contact_id}: Creating Inter Call df\")\n",
    "        inter_call_dict = {}\n",
    "        inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "        inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "        inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "        inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "        inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "        inter_call_dict['category_generated'] = call_gen_kpis['call_topic']['category']\n",
    "        inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "        inter_call_dict['sub_category_generated'] = call_gen_kpis['call_topic']['sub_category']\n",
    "        inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "        df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "        \n",
    "        logger.info(f\"{contact_id}:  Add metadata from AWS\")\n",
    "        df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "        df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "        df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "        df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "        # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "        # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "        df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "        df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "        df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "        logger.info(f\"{contact_id}: Completed creating Inter Call df\")\n",
    "        \n",
    "        return df_inter_call\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, max_objects, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b38d2-09af-4cfe-9a32-14380eee0b1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Run Process Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    ")\n",
    "def process_files(\n",
    "    pipeline_run_name: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    vai_gcs_folder_stagging: str,\n",
    "    vai_gcs_folder_errored: str,\n",
    "    gcp_project_id: str,\n",
    "    gcp_project_location: str,\n",
    "    snf_account: str,\n",
    "    snf_user: str,\n",
    "    snf_private_key_file: str,\n",
    "    snf_private_key_pwd: str,\n",
    "    snf_warehouse: str,\n",
    "    snf_catsubcat_databse: str,\n",
    "    snf_catsubcat_schema: str,\n",
    "    snf_catsubcat_view: str,\n",
    "    max_objects: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads CSV from staging, processes files, and writes processed data back.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.special import softmax\n",
    "    import logging\n",
    "    import re, os\n",
    "    from datetime import datetime\n",
    "    from typing import List, Dict\n",
    "    \n",
    "    from pydantic import BaseModel, Field, ValidationError\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud import dlp_v2\n",
    "    import vertexai\n",
    "    import vertexai.preview.generative_models as generative_models\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "    \n",
    "    # Sentiments\n",
    "    from transformers import pipeline\n",
    "    from transformers import AutoTokenizer, AutoConfig\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # Initialize GCS Client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Download the private key from GCS\n",
    "    bucket_name, blob_name = snf_private_key_file.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    private_key_path = \"/tmp/snowflake_key.p8\"  # Local temp storage\n",
    "    blob.download_to_filename(private_key_path)\n",
    "    \n",
    "    catsubcat_conn_params = {\n",
    "        'account': snf_account,\n",
    "        'user': snf_user,\n",
    "        'private_key_file': private_key_path,\n",
    "        'private_key_file_pwd': snf_private_key_pwd,\n",
    "        'warehouse': snf_warehouse,\n",
    "        'database': snf_catsubcat_databse,\n",
    "        'schema': snf_catsubcat_schema\n",
    "    }\n",
    "    \n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    stagging_folder = f\"gs://{vai_gcs_bucket}/{pipeline_run_name}/{pipeline_run_name}/{vai_gcs_folder_stagging}\"\n",
    "    logger.info(f\"Setting Stagging Folder: {stagging_folder}\")\n",
    "    \n",
    "    logger.info(\"Initiating Master DataFrames: df_intra_calls_data, df_inter_calls_data\")\n",
    "    df_intra_calls_data, df_inter_calls_data = initiate_master_dataframes(stagging_folder, vai_gcs_bucket)\n",
    "    \n",
    "    s3_Transcripts_fetched = f\"gs://{vai_gcs_bucket}/{pipeline_run_name}/{vai_gcs_folder_stagging}/{pipeline_run_name}_s3_Transcripts_fetched.csv\"\n",
    "    transcripts = pd.read_csv(s3_Transcripts_fetched)\n",
    "    \n",
    "    logger.info(\"--------------\")\n",
    "    logger.info(f\"Transcripts to process: {transcripts.shape[0]}\")\n",
    "    logger.info(\"--------------\")\n",
    "    \n",
    "    transcript = transcripts.File[0]\n",
    "    for transcript in transcripts.File[:max_objects]:\n",
    "        try:\n",
    "            transcript = transcripts.File[0]\n",
    "            contact_id = transcript.split('/')[-1].split('.')[0].split('analysis')[0].strip('_')\n",
    "            ac_last_modified_date = datetime.strptime(transcript.split('analysis_')[-1].split('.')[0].replace('_', ':'), '%Y-%m-%dT%H:%M:%SZ')\n",
    "            \n",
    "            if (len(df_intra_calls_data) > 0 and contact_id in df_intra_calls_data.CONTACT_ID.unique()) and (len(df_inter_calls_data) > 0 and contact_id in df_inter_calls_data.CONTACT_ID.unique()):\n",
    "                logger.info(f\"{contact_id}: Call already Processed.\")\n",
    "                logger.info(\"\")\n",
    "                logger.info(\"\")\n",
    "            else:\n",
    "                logger.info(f\"{contact_id}: Processing\")\n",
    "            \n",
    "                logger.info(f\"{contact_id}: Fetching Transcript from S3\")\n",
    "                transcript_data = fetch_transcript_from_s3(contact_id, aws_access_key, aws_secret_key, s3_analysis_bucket, transcript)\n",
    "                logger.info(f\"{contact_id}: Completed fetching Transcript from S3\")\n",
    "            \n",
    "                logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "                df_intra_call = create_intra_call_df(\n",
    "                    contact_id,\n",
    "                    vai_gcs_bucket,\n",
    "                    pipeline_run_name,\n",
    "                    max_objects,\n",
    "                    transcript_data,\n",
    "                    tokenizer\n",
    "                )\n",
    "                logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "            \n",
    "                logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "                df_inter_call = create_inter_call_df(\n",
    "                    contact_id,\n",
    "                    vai_gcs_bucket,\n",
    "                    vai_gcs_folder_stagging,\n",
    "                    max_objects,\n",
    "                    pipeline_run_name,\n",
    "                    transcript_data,\n",
    "                    ac_last_modified_date,\n",
    "                    df_intra_call,\n",
    "                    gcp_project_id,\n",
    "                    gcp_project_location,\n",
    "                    snf_account,\n",
    "                    snf_user,\n",
    "                    snf_private_key_file,\n",
    "                    snf_private_key_pwd,\n",
    "                    snf_warehouse,\n",
    "                    snf_catsubcat_databse,\n",
    "                    snf_catsubcat_schema,\n",
    "                    snf_catsubcat_view\n",
    "                )\n",
    "                logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "    \n",
    "    \n",
    "                ###============================================================###\n",
    "                if not df_intra_call.empty and not df_inter_call.empty:\n",
    "                    df_intra_call.columns = df_intra_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "                    df_intra_calls_data = pd.concat([df_intra_calls_data, df_intra_call], ignore_index=True)\n",
    "                    df_intra_calls_data.to_csv(f\"{stagging_folder}/df_intra_calls_data.csv\", index=False)\n",
    "                    logger.info(f\"{contact_id}: Persisted df_intra_calls_data to CSV.\")\n",
    "                    \n",
    "                    # Appending to Inter-calls Master DataFrame\n",
    "                    df_inter_call.columns = df_inter_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "                    df_inter_calls_data = pd.concat([df_inter_calls_data, df_inter_call], ignore_index=True)\n",
    "                    df_inter_calls_data.to_csv(f\"{stagging_folder}/df_inter_calls_data.csv\", index=False)\n",
    "                    logger.info(f\"{contact_id}: Persisted df_intra_calls_data to CSV.\")\n",
    "                    logger.info(f\"{contact_id}: Processing Complete\")\n",
    "                    logger.info(\"\")\n",
    "                    logger.info(\"\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"--------------------------\")\n",
    "            handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, max_objects, str(e))\n",
    "            logger.error(\"--------------------------\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078e654-4f1c-4e41-9034-2f5a74990baf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Write Data to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98285d4-080b-451f-89a4-1f3c9832936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1de5c3-2317-4937-b909-2f5a4ece7d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Constant argument inputs must be one of type ['String', 'Integer', 'Float', 'Boolean', 'List', 'Dict'] Got: None of type <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m      3\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess Amazon Audio Transcripts to KPIs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvai_audio_to_kpi_pipeline\u001b[39m(\n\u001b[1;32m      6\u001b[0m     pipeline_run_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      7\u001b[0m     vai_gcs_bucket: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     vai_gcs_folder_stagging: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      9\u001b[0m     vai_gcs_folder_errored: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     10\u001b[0m     gcp_project_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     11\u001b[0m     gcp_project_location: \u001b[38;5;28mstr\u001b[39m,    \n\u001b[1;32m     12\u001b[0m     aws_access_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     13\u001b[0m     aws_secret_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     14\u001b[0m     s3_analysis_bucket: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     15\u001b[0m     s3_transcript_location: \u001b[38;5;28mstr\u001b[39m,    \n\u001b[1;32m     16\u001b[0m     snf_account: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     17\u001b[0m     snf_user: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     18\u001b[0m     snf_private_key_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     19\u001b[0m     snf_private_key_pwd: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     20\u001b[0m     snf_warehouse: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     21\u001b[0m     snf_catsubcat_databse: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     22\u001b[0m     snf_catsubcat_schema: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     23\u001b[0m     snf_catsubcat_view: \u001b[38;5;28mstr\u001b[39m,    \n\u001b[1;32m     24\u001b[0m     max_objects: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     fetch_transcripts \u001b[38;5;241m=\u001b[39m list_s3_files_to_gcs(\n\u001b[1;32m     27\u001b[0m         pipeline_run_name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m     28\u001b[0m         aws_access_key\u001b[38;5;241m=\u001b[39mVAI_AWS_ACCESS_KEY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         max_objects\u001b[38;5;241m=\u001b[39mmax_objects\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     process_transcripts \u001b[38;5;241m=\u001b[39m process_files(        \n\u001b[1;32m     37\u001b[0m         pipeline_run_name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m     38\u001b[0m         vai_gcs_bucket\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_BUCKET,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         max_objects\u001b[38;5;241m=\u001b[39mmax_objects\n\u001b[1;32m     52\u001b[0m     )\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/pipeline_context.py:71\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     69\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m component_factory\u001b[38;5;241m.\u001b[39mcreate_graph_component_from_func(\n\u001b[1;32m     72\u001b[0m     func,\n\u001b[1;32m     73\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m     74\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m     75\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mdisplay_name,\n\u001b[1;32m     76\u001b[0m     pipeline_config\u001b[38;5;241m=\u001b[39mpipeline_config,\n\u001b[1;32m     77\u001b[0m )\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/component_factory.py:708\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    704\u001b[0m     func,\n\u001b[1;32m    705\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    706\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    707\u001b[0m )\n\u001b[0;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_component\u001b[38;5;241m.\u001b[39mGraphComponent(\n\u001b[1;32m    709\u001b[0m     component_spec\u001b[38;5;241m=\u001b[39mcomponent_spec,\n\u001b[1;32m    710\u001b[0m     pipeline_func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    711\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mdisplay_name,\n\u001b[1;32m    712\u001b[0m     pipeline_config\u001b[38;5;241m=\u001b[39mpipeline_config,\n\u001b[1;32m    713\u001b[0m )\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/graph_component.py:61\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     53\u001b[0m         pipeline_channel\u001b[38;5;241m.\u001b[39mcreate_pipeline_channel(\n\u001b[1;32m     54\u001b[0m             name\u001b[38;5;241m=\u001b[39marg_name,\n\u001b[1;32m     55\u001b[0m             channel_type\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     56\u001b[0m             is_artifact_list\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mis_artifact_list,\n\u001b[1;32m     57\u001b[0m         ))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context\u001b[38;5;241m.\u001b[39mPipeline(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m---> 61\u001b[0m     pipeline_outputs \u001b[38;5;241m=\u001b[39m pipeline_func(\u001b[38;5;241m*\u001b[39margs_list)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[79], line 36\u001b[0m, in \u001b[0;36mvai_audio_to_kpi_pipeline\u001b[0;34m(pipeline_run_name, vai_gcs_bucket, vai_gcs_folder_stagging, vai_gcs_folder_errored, gcp_project_id, gcp_project_location, aws_access_key, aws_secret_key, s3_analysis_bucket, s3_transcript_location, snf_account, snf_user, snf_private_key_file, snf_private_key_pwd, snf_warehouse, snf_catsubcat_databse, snf_catsubcat_schema, snf_catsubcat_view, max_objects)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m      3\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess Amazon Audio Transcripts to KPIs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     max_objects: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     25\u001b[0m ):\n\u001b[1;32m     26\u001b[0m     fetch_transcripts \u001b[38;5;241m=\u001b[39m list_s3_files_to_gcs(\n\u001b[1;32m     27\u001b[0m         pipeline_run_name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m     28\u001b[0m         aws_access_key\u001b[38;5;241m=\u001b[39mVAI_AWS_ACCESS_KEY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         max_objects\u001b[38;5;241m=\u001b[39mmax_objects\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0m     process_transcripts \u001b[38;5;241m=\u001b[39m process_files(        \n\u001b[1;32m     37\u001b[0m         pipeline_run_name\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_RUN_NAME,\n\u001b[1;32m     38\u001b[0m         vai_gcs_bucket\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_BUCKET,\n\u001b[1;32m     39\u001b[0m         vai_gcs_folder_stagging\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_FOLDER_STAGGING,\n\u001b[1;32m     40\u001b[0m         vai_gcs_folder_errored\u001b[38;5;241m=\u001b[39mVAI_GCP_PIPELINE_FOLDER_ERRORED,\n\u001b[1;32m     41\u001b[0m         gcp_project_id\u001b[38;5;241m=\u001b[39mVAI_GCP_PROJECT_ID,\n\u001b[1;32m     42\u001b[0m         gcp_project_location\u001b[38;5;241m=\u001b[39mVAI_GCP_PROJECT_LOCATION,\n\u001b[1;32m     43\u001b[0m         snf_account\u001b[38;5;241m=\u001b[39mVAI_SNF_ACCOUNT,\n\u001b[1;32m     44\u001b[0m         snf_user\u001b[38;5;241m=\u001b[39mVAI_SNF_USER,\n\u001b[1;32m     45\u001b[0m         snf_private_key_file\u001b[38;5;241m=\u001b[39mVAI_SNF_PRIVATE_KEY_FILE,\n\u001b[1;32m     46\u001b[0m         snf_private_key_pwd\u001b[38;5;241m=\u001b[39mVAI_SNF_PRIVATE_KEY_PWD,\n\u001b[1;32m     47\u001b[0m         snf_warehouse\u001b[38;5;241m=\u001b[39mVAI_SNF_WAREHOUS,\n\u001b[1;32m     48\u001b[0m         snf_catsubcat_databse\u001b[38;5;241m=\u001b[39mVAI_SNF_CATSUBCAT_DATABASE,\n\u001b[1;32m     49\u001b[0m         snf_catsubcat_schema\u001b[38;5;241m=\u001b[39mVAI_SNF_CATSUBCAT_SCHEMA,\n\u001b[1;32m     50\u001b[0m         snf_catsubcat_view\u001b[38;5;241m=\u001b[39mVAI_SNF_CATSUBCAT_VIEW,\n\u001b[1;32m     51\u001b[0m         max_objects\u001b[38;5;241m=\u001b[39mmax_objects\n\u001b[1;32m     52\u001b[0m     )\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/base_component.py:101\u001b[0m, in \u001b[0;36mBaseComponent.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     95\u001b[0m         arg_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m missing_arguments)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() missing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_arguments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m required \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_or_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_task\u001b[38;5;241m.\u001b[39mPipelineTask(\n\u001b[1;32m    102\u001b[0m     component_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec,\n\u001b[1;32m    103\u001b[0m     args\u001b[38;5;241m=\u001b[39mtask_inputs,\n\u001b[1;32m    104\u001b[0m     execute_locally\u001b[38;5;241m=\u001b[39mpipeline_context\u001b[38;5;241m.\u001b[39mPipeline\u001b[38;5;241m.\u001b[39mget_default_pipeline() \u001b[38;5;129;01mis\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m     execution_caching_default\u001b[38;5;241m=\u001b[39mpipeline_context\u001b[38;5;241m.\u001b[39mPipeline\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m.\u001b[39mget_execution_caching_default(),\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/pipeline_task.py:119\u001b[0m, in \u001b[0;36mPipelineTask.__init__\u001b[0;34m(self, component_spec, args, execute_locally, execution_caching_default)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent_spec\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m got an unexpected input:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m     input_spec \u001b[38;5;241m=\u001b[39m component_spec\u001b[38;5;241m.\u001b[39minputs[input_name]\n\u001b[0;32m--> 119\u001b[0m     type_utils\u001b[38;5;241m.\u001b[39mverify_type_compatibility(\n\u001b[1;32m    120\u001b[0m         given_value\u001b[38;5;241m=\u001b[39margument_value,\n\u001b[1;32m    121\u001b[0m         expected_spec\u001b[38;5;241m=\u001b[39minput_spec,\n\u001b[1;32m    122\u001b[0m         error_message_prefix\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncompatible argument passed to the input \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m of component \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent_spec\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec \u001b[38;5;241m=\u001b[39m component_spec\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_spec \u001b[38;5;241m=\u001b[39m structures\u001b[38;5;241m.\u001b[39mTaskSpec(\n\u001b[1;32m    130\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_task_handler(),\n\u001b[1;32m    131\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(args\u001b[38;5;241m.\u001b[39mitems()),\n\u001b[1;32m    132\u001b[0m     dependent_tasks\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    133\u001b[0m     component_ref\u001b[38;5;241m=\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    134\u001b[0m     enable_caching\u001b[38;5;241m=\u001b[39mexecution_caching_default)\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/types/type_utils.py:281\u001b[0m, in \u001b[0;36mverify_type_compatibility\u001b[0;34m(given_value, expected_spec, error_message_prefix, checks_input, raise_on_error)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# extract and normalize types\u001b[39;00m\n\u001b[1;32m    280\u001b[0m expected_type \u001b[38;5;241m=\u001b[39m expected_spec\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 281\u001b[0m given_type \u001b[38;5;241m=\u001b[39m _get_type_string_from_component_argument(given_value)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# avoid circular imports\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m for_loop\n",
      "File \u001b[0;32m~/Installs/anaconda3/envs/posigen/lib/python3.12/site-packages/kfp/dsl/types/type_utils.py:251\u001b[0m, in \u001b[0;36m_get_type_string_from_component_argument\u001b[0;34m(argument_value)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argument_value, artifact_types\u001b[38;5;241m.\u001b[39mArtifact):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput artifacts are not supported. Got input artifact of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    250\u001b[0m     )\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConstant argument inputs must be one of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_TYPE_TO_TYPE_NAME\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_value\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(argument_value)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Constant argument inputs must be one of type ['String', 'Integer', 'Float', 'Boolean', 'List', 'Dict'] Got: None of type <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=VAI_GCP_PIPELINE_RUN_NAME,\n",
    "    description=\"Proces Amazon Audio Transcripts to KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    s3_analysis_bucket: str,\n",
    "    s3_transcript_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    max_objects: int\n",
    "):\n",
    "    fetch_transcripts = list_s3_files_to_gcs(\n",
    "        pipeline_run_name=VAI_GCP_PIPELINE_RUN_NAME,\n",
    "        aws_access_key=VAI_AWS_ACCESS_KEY,\n",
    "        aws_secret_key=VAI_AWS_SECRET_KEY,\n",
    "        s3_analysis_bucket=VAI_S3_ANALYSIS_BUCKET,\n",
    "        s3_transcript_location=VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "        vai_gcs_bucket=VAI_GCP_PIPELINE_BUCKET,\n",
    "        max_objects=max_objects\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, f'{VAI_GCP_PIPELINE_NAME}.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ac4-60c0-4607-aabf-23f5f31f9ca7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run the Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dae3d-a209-41df-af85-97d715d94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=VAI_GCP_PROJECT_ID, location=VAI_GCP_PROJECT_LOCATION)\n",
    "\n",
    "max_objects = 1\n",
    "\n",
    "# # Create pipeline job\n",
    "# job = aiplatform.PipelineJob(\n",
    "#     display_name = f\"{VAI_GCP_PIPELINE_RUN_NAME}\".lower(),\n",
    "#     job_id = f\"vai-pipeline-run--{TIMESTAMP}\".lower(),\n",
    "#     template_path = f\"{VAI_GCP_PIPELINE_NAME}.yaml\",\n",
    "#     pipeline_root = f\"gs://{VAI_GCP_PIPELINE_BUCKET}\",\n",
    "#     project = VAI_GCP_PROJECT_ID,\n",
    "#     location = VAI_GCP_PROJECT_LOCATION,\n",
    "#     enable_caching = False,\n",
    "#     parameter_values={\n",
    "#         \"pipeline_run_name\": VAI_GCP_PIPELINE_RUN_NAME,\n",
    "#         \"vai_gcs_bucket\": VAI_GCP_PIPELINE_BUCKET,\n",
    "#         \"vai_gcs_folder_stagging\": VAI_GCP_PIPELINE_FOLDER_STAGGING,\n",
    "#         \"vai_gcs_folder_errored\": VAI_GCP_PIPELINE_FOLDER_ERRORED,\n",
    "#         \"gcp_project_id\": VAI_GCP_PROJECT_ID,\n",
    "#         \"gcp_project_location\": VAI_GCP_PROJECT_LOCATION,\n",
    "        \n",
    "#         \"aws_access_key\":VAI_AWS_ACCESS_KEY,\n",
    "#         \"aws_secret_key\":VAI_AWS_SECRET_KEY,\n",
    "#         \"s3_analysis_bucket\":VAI_S3_ANALYSIS_BUCKET,\n",
    "#         \"s3_transcript_location\":VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "\n",
    "#         \"snf_account\": VAI_SNF_ACCOUNT,\n",
    "#         \"snf_user\": VAI_SNF_USER,\n",
    "#         \"snf_private_key_file\": VAI_SNF_PRIVATE_KEY_FILE,\n",
    "#         \"snf_private_key_pwd\": VAI_SNF_PRIVATE_KEY_PWD,\n",
    "#         \"snf_warehouse\": VAI_SNF_WAREHOUS,\n",
    "#         \"snf_catsubcat_databse\": VAI_SNF_CATSUBCAT_DATABASE,\n",
    "#         \"snf_catsubcat_schema\": VAI_SNF_CATSUBCAT_SCHEMA,\n",
    "#         \"snf_catsubcat_view\": VAI_SNF_CATSUBCAT_VIEW,\n",
    "        \n",
    "#         \"max_objects\":max_objects\n",
    "#     }\n",
    "# )\n",
    "# Create pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name = f\"{VAI_GCP_PIPELINE_RUN_NAME}\".lower(),\n",
    "    job_id = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    template_path = f\"{VAI_GCP_PIPELINE_NAME}.yaml\",\n",
    "    pipeline_root = f\"gs://{VAI_GCP_PIPELINE_BUCKET}\",\n",
    "    project = VAI_GCP_PROJECT_ID,\n",
    "    location = VAI_GCP_PROJECT_LOCATION,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        \"pipeline_run_name\":VAI_GCP_PIPELINE_RUN_NAME,\n",
    "        \"aws_access_key\":VAI_AWS_ACCESS_KEY,\n",
    "        \"aws_secret_key\":VAI_AWS_SECRET_KEY,\n",
    "        \"s3_analysis_bucket\":VAI_S3_ANALYSIS_BUCKET,\n",
    "        \"s3_transcript_location\":VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "        \"vai_gcs_bucket\":VAI_GCP_PIPELINE_BUCKET,\n",
    "        \"max_objects\":max_objects\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3a463-dbc3-465c-a7c3-9459f26b2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362431c-209a-4f36-b98a-b1894eb429d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
