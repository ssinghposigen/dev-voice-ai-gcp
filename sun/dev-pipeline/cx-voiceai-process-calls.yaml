# PIPELINE DEFINITION
# Name: vai-audio-to-kpi-pipeline
# Description: Process Amazon Audio Transcripts into KPIs
# Inputs:
#    aws_access_key: str
#    aws_secret_key: str
#    gcp_project_id: str
#    gcp_project_location: str
#    max_parallelism: int
#    pipeline_run_name: str
#    s3_analysis_bucket: str
#    s3_transcripts_location: str
#    snf_account: str
#    snf_catsubcat_databse: str
#    snf_catsubcat_schema: str
#    snf_catsubcat_view: str
#    snf_database: str
#    snf_private_key: str
#    snf_private_key_pwd: str
#    snf_schema: str
#    snf_user: str
#    snf_warehouse: str
#    time_interval: int
#    vai_gcs_bucket: str
components:
  comp-process-transcripts:
    executorLabel: exec-process-transcripts
    inputDefinitions:
      parameters:
        gcp_project_id:
          parameterType: STRING
        gcp_project_location:
          parameterType: STRING
        max_parallelism:
          parameterType: NUMBER_INTEGER
        pipeline_run_name:
          parameterType: STRING
        snf_account:
          parameterType: STRING
        snf_catsubcat_databse:
          parameterType: STRING
        snf_catsubcat_schema:
          parameterType: STRING
        snf_catsubcat_view:
          parameterType: STRING
        snf_private_key:
          parameterType: STRING
        snf_private_key_pwd:
          parameterType: STRING
        snf_user:
          parameterType: STRING
        snf_warehouse:
          parameterType: STRING
        vai_gcs_bucket:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-process-transcripts:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - process_transcripts
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef process_transcripts(\n    pipeline_run_name: str,\n    gcp_project_id:\
          \ str,\n    gcp_project_location: str,\n    vai_gcs_bucket: str,\n    snf_account:\
          \ str,\n    snf_user: str,\n    snf_private_key: str,\n    snf_private_key_pwd:\
          \ str,\n    snf_warehouse: str,\n    snf_catsubcat_databse: str,\n    snf_catsubcat_schema:\
          \ str,\n    snf_catsubcat_view: str,\n    max_parallelism: int\n):\n   \
          \ from concurrent.futures import ThreadPoolExecutor\n    import threading\n\
          \    import concurrent.futures\n\n    import pandas as pd\n    import numpy\
          \ as np\n    from scipy.special import softmax\n    import logging\n   \
          \ import re, os, json, io\n    from datetime import datetime, timezone\n\
          \    from typing import List, Dict\n\n    from pydantic import BaseModel,\
          \ Field, ValidationError\n\n    import snowflake.connector as sc\n    from\
          \ cryptography.hazmat.primitives import serialization\n\n    from google.cloud\
          \ import storage\n    from google.cloud import dlp_v2\n    from google.cloud\
          \ import logging as cloud_logging\n\n    import vertexai\n    import vertexai.preview.generative_models\
          \ as generative_models\n    from vertexai.generative_models import GenerativeModel,\
          \ GenerationConfig, Part\n\n    # Sentiments\n    from transformers import\
          \ pipeline\n    from transformers import AutoTokenizer, AutoConfig\n   \
          \ from transformers import AutoModelForSequenceClassification\n\n    os.environ[\"\
          TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\
          \n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    config = AutoConfig.from_pretrained(MODEL)\n\
          \    model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\
          \n    # Initialize Google Cloud Logging client\n    cloud_logging_client\
          \ = cloud_logging.Client()\n    cloud_logging_client.setup_logging()\n\n\
          \    \"\"\"\n    ========================================================\n\
          \    Function: Logging mechanism\n    ========================================================\n\
          \    \"\"\"\n    # Set up logging\n    def setup_logger(log_filename, bucket_name,\
          \ log_folder, cloud_log_name=None):\n        \"\"\"\n        Sets up a logger\
          \ that writes logs to both a file and Google Cloud Logging.\n        \"\"\
          \"\n        # Create a logger\n        logger = logging.getLogger(log_filename)\n\
          \        logger.setLevel(logging.INFO)\n\n        # Prevent duplicate logs\n\
          \        if not logger.hasHandlers():\n            # File Handler (for local\
          \ + GCS upload)\n            log_path = f\"/tmp/{log_filename}\"  # Temp\
          \ file for logs\n            file_handler = logging.FileHandler(log_path)\n\
          \            file_handler.setFormatter(logging.Formatter('%(asctime)s -\
          \ %(levelname)s - %(message)s'))\n            logger.addHandler(file_handler)\n\
          \n            # Google Cloud Logging\n            client = cloud_logging.Client()\n\
          \            cloud_handler = cloud_logging.handlers.CloudLoggingHandler(client,\
          \ name=cloud_log_name or log_filename)\n            cloud_handler.setFormatter(logging.Formatter('%(asctime)s\
          \ - %(levelname)s - %(message)s'))\n            logger.addHandler(cloud_handler)\n\
          \n        return logger\n\n    def upload_log_to_gcs(logger, log_filename,\
          \ bucket_name, log_folder):\n        \"\"\"\n        Uploads the local log\
          \ file to GCS.\n        \"\"\"\n        client = storage.Client()\n    \
          \    bucket = client.bucket(bucket_name)\n        blob = bucket.blob(f\"\
          {log_folder}/{log_filename}\")\n\n        log_path = f\"/tmp/{log_filename}\"\
          \n        if os.path.exists(log_path):\n            blob.upload_from_filename(log_path)\n\
          \            logger.info(f\"Uploaded {log_filename} to GCS: {log_folder}\"\
          )\n\n    def merge_logs_to_master(bucket_name, logs_folder, master_log_filename,\
          \ master_logger):\n        \"\"\"\n        Merges all thread log files into\
          \ the master log.\n        \"\"\"\n        client = storage.Client()\n \
          \       bucket = client.bucket(bucket_name)\n        blobs = list(bucket.list_blobs(prefix=logs_folder))\n\
          \n        with open(f\"/tmp/{master_log_filename}\", \"w\") as master_log:\n\
          \            for blob in blobs:\n                if blob.name.endswith(\"\
          .log\"):\n                    master_log.write(f\"\\n--- Merging: {blob.name}\
          \ ---\\n\")\n                    master_log.write(blob.download_as_text())\n\
          \                    master_log.write(\"\\n\")\n\n        # Upload merged\
          \ log to GCS\n        blob = bucket.blob(f\"{logs_folder}/{master_log_filename}\"\
          )\n        blob.upload_from_filename(f\"/tmp/{master_log_filename}\")\n\n\
          \        master_logger.info(f\"Merged logs uploaded to: {logs_folder}/{master_log_filename}\"\
          )\n\n\n    \"\"\"\n    ========================================================\n\
          \    Function: Exception hanlding mechanism\n    ========================================================\n\
          \    \"\"\"\n    def handle_exception(\n        file_id,\n        vai_gcs_bucket,\n\
          \        run_folder,\n        error_folder,\n        error_message\n   \
          \ ):\n        \"\"\"\n        Logs the error, appends the file_id to error\
          \ tracking CSV, and triggers a notification.\n        \"\"\"\n        try:\n\
          \            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\
          \n\n            logger.error(f\"Error processing file {file_id}: {error_message}\"\
          )\n\n            gcs_client = storage.Client()\n            bucket = gcs_client.bucket(vai_gcs_bucket)\n\
          \            blob = bucket.blob(error_df_path)\n\n            if blob.exists():\n\
          \                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\"\
          )\n            else:\n                error_df = pd.DataFrame(columns=[\"\
          File_ID\", \"Error_Message\"])\n\n            error_df = pd.concat([error_df,\
          \ pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])],\
          \ ignore_index=True)\n            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\"\
          , index=False)\n            logger.info(f\"Logged error for file {file_id}\
          \ in {error_df_path}\")\n\n        except Exception as e:\n            logger.error(f\"\
          Failed to write to error tracking file: {e}\")\n\n    def fetch_transcripts_from_gcs(\n\
          \        pipeline_run_name,\n        vai_gcs_bucket,\n        gcs_stagging_folder,\n\
          \        gcs_errored_folder,\n        gcs_transcripts_folder,\n        master_logger\n\
          \    ):\n        \"\"\"\n        List all files in a GCS bucket, handling\
          \ pagination.\n\n        :param bucket_name: Name of the GCS bucket\n  \
          \      :param prefix: (Optional) Folder path to filter files\n        :return:\
          \ List of file paths\n        \"\"\"\n        try:\n            master_logger.info(f\"\
          Fetching Transcripts from GCS: {gcs_transcripts_folder}\")\n           \
          \ client = storage.Client()\n            bucket = client.bucket(vai_gcs_bucket)\n\
          \            blobs_iterator = bucket.list_blobs(prefix=gcs_transcripts_folder)\
          \  # GCS handles pagination internally\n\n            transcripts_list =\
          \ []\n            for page in blobs_iterator.pages:  # Handling pagination\n\
          \                for blob in page:\n                    if not blob.name.endswith(\"\
          /\"):\n                        transcripts_list.append(blob.name)\n    \
          \        master_logger.info(f\"Completed: Fetching from GCS Transcripts\
          \ List#: {len(transcripts_list)}\")\n            return transcripts_list\n\
          \n        except Exception as e:\n            handle_exception(\"N/A\",\
          \ vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n\n\n \
          \   def fetch_category_mapping_from_snowflake(\n        pipeline_run_name,\n\
          \        vai_gcs_bucket,\n        gcs_stagging_folder,\n        gcs_errored_folder,\n\
          \        snf_account,\n        snf_user,\n        snf_private_key,\n   \
          \     snf_private_key_pwd,\n        snf_warehouse,\n        snf_catsubcat_databse,\n\
          \        snf_catsubcat_schema,\n        snf_catsubcat_view,\n        master_logger\n\
          \    ):\n        \"\"\"\n        Fetch Category-Subcategory mapping from\
          \ Snowflake using a private key stored in GCP Secret Manager.\n\n      \
          \  :param snf_secret_project_id: GCP project where the secret is stored.\n\
          \        :param secret_name: Name of the secret containing the Snowflake\
          \ private key.\n        :param snowflake_params: Dictionary containing Snowflake\
          \ connection parameters.\n\n        :return: Pandas DataFrame with category\
          \ mappings.\n        \"\"\"\n\n        try:\n            # Step 1: Load\
          \ & Decrypt the Private Key\n            snf_private_key = serialization.load_pem_private_key(\n\
          \                snf_private_key.encode(),\n                password=snf_private_key_pwd.encode(),\n\
          \                backend=None  # Default backend\n            )\n\n    \
          \        # Step 2: Convert to Snowflake Compatible Format\n            pkey_bytes\
          \ = snf_private_key.private_bytes(\n                encoding=serialization.Encoding.DER,\n\
          \                format=serialization.PrivateFormat.PKCS8,\n           \
          \     encryption_algorithm=serialization.NoEncryption(),\n            )\n\
          \n            # Step 3: Connect to Snowflake\n            catsubcat_conn_params\
          \ = {\n                'account': snf_account,\n                'user':\
          \ snf_user,\n                'private_key': snf_private_key,\n         \
          \       'warehouse': snf_warehouse,\n                'database': snf_catsubcat_databse,\n\
          \                'schema': snf_catsubcat_schema\n            }\n\n     \
          \       # Connect to Snowflake\n            conn = sc.connect(**catsubcat_conn_params)\n\
          \n            # Fetch data from Snowflake\n            query = f\"SELECT\
          \ CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n            df = pd.read_sql(query,\
          \ conn)\n            conn.close()\n            master_logger.info(\"Completed:\
          \ Fetching Category, Sub-Category Mapping.\")\n\n            return df\n\
          \n        except Exception as e:\n            handle_exception(\"N/A\",\
          \ vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n\n   \
          \ \"\"\"\n    ========================================================\n\
          \    Function: Create Dataframe: Intra Call \n    ========================================================\n\
          \    \"\"\"            \n    def mask_pii_in_captions(\n        contact_id,\n\
          \        df,\n        project_id\n    ):\n        \"\"\"\n        Masks\
          \ PII data in the 'caption' column of a pandas DataFrame using Google Cloud\
          \ DLP API.\n\n        Args:\n            contact_id: Identifier for logging\
          \ purposes\n            df (pandas.DataFrame): DataFrame with a 'caption'\
          \ column to process\n            project_id (str): Your Google Cloud project\
          \ ID\n\n        Returns:\n            pandas.DataFrame: DataFrame with masked\
          \ PII in the 'caption' column\n        \"\"\"\n        try:\n          \
          \  logger.info(f\"{contact_id}: Masking PII Data\")\n\n            # Create\
          \ a copy of the DataFrame to avoid modifying the original\n            masked_df\
          \ = df.copy()\n\n            # Add unique markers to each caption to identify\
          \ them after processing\n            masked_df['original_index'] = masked_df.index\n\
          \            masked_df['marked_caption'] = masked_df.index.astype(str) +\
          \ \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n\n           \
          \ # Concatenate all captions for bulk processing\n            all_captions\
          \ = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n\n\
          \            # Initialize DLP client\n            dlp_client = dlp_v2.DlpServiceClient()\n\
          \n            # Specify the parent resource name\n            parent = f\"\
          projects/{project_id}/locations/global\"\n\n            # Custom dictionary\
          \ detector for PosiGen\n            posigen_dictionary = {\n           \
          \     \"info_type\": {\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"},\n      \
          \          \"dictionary\": {\n                    \"word_list\": {\n   \
          \                     \"words\": [\"posigen\", \"Posigen\", \"PosiGen\"\
          , \"POSIGEN\"]\n                    }\n                }\n            }\n\
          \n            # Configure inspection config with rule set for exclusions\n\
          \            inspect_config = {\n                \"info_types\": [\n   \
          \                 {\"name\": \"CREDIT_CARD_NUMBER\"},\n                \
          \    {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n                    {\"\
          name\": \"STREET_ADDRESS\"},\n                    {\"name\": \"IP_ADDRESS\"\
          },\n                    {\"name\": \"DATE_OF_BIRTH\"}\n                ],\n\
          \                \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE,\n     \
          \           \"custom_info_types\": [posigen_dictionary],\n             \
          \   \"rule_set\": [\n                    {\n                        \"info_types\"\
          : [{\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"}],\n                       \
          \ \"rules\": [\n                            {\n                        \
          \        \"exclusion_rule\": {\n                                    \"matching_type\"\
          : dlp_v2.MatchingType.MATCHING_TYPE_FULL_MATCH,\n                      \
          \              \"dictionary\": {\n                                     \
          \   \"word_list\": {\n                                            \"words\"\
          : [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n               \
          \                         }\n                                    }\n   \
          \                             }\n                            }\n       \
          \                 ]\n                    }\n                ]\n        \
          \    }\n\n            # Configure deidentification to use \"[REDACTED]\"\
          \ instead of asterisks\n            deidentify_config = {\n            \
          \    \"info_type_transformations\": {\n                    \"transformations\"\
          : [\n                        {\n                            \"info_types\"\
          : [\n                                {\"name\": \"CREDIT_CARD_NUMBER\"},\n\
          \                                {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"\
          },\n                                {\"name\": \"STREET_ADDRESS\"},\n  \
          \                              {\"name\": \"IP_ADDRESS\"},\n           \
          \                     {\"name\": \"DATE_OF_BIRTH\"}\n                  \
          \          ],\n                            \"primitive_transformation\"\
          : {\n                                \"replace_config\": {\n           \
          \                         \"new_value\": {\"string_value\": \"[REDACTED]\"\
          }\n                                }\n                            }\n  \
          \                      }\n                    ]\n                }\n   \
          \         }\n\n            # Create deidentify request\n            item\
          \ = {\"value\": all_captions}\n\n            # Call the DLP API\n      \
          \      try:\n                response = dlp_client.deidentify_content(\n\
          \                    request={\n                        \"parent\": parent,\n\
          \                        \"deidentify_config\": deidentify_config,\n   \
          \                     \"inspect_config\": inspect_config,\n            \
          \            \"item\": item,\n                    }\n                )\n\
          \            except Exception as e:\n                logger.error(f\"{contact_id}:\
          \ Error in DLP API call: {e}\")\n                return df  # Return original\
          \ DataFrame if masking fails\n\n            # Get processed content and\
          \ split by record boundaries\n            processed_content = response.item.value\n\
          \            processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\\
          n\")\n\n            # Create mapping from original indices to processed\
          \ captions\n            processed_dict = {}\n            for record in processed_records:\n\
          \                parts = record.split(\"|||SEPARATOR|||\", 1)\n        \
          \        if len(parts) == 2:\n                    idx, content = parts\n\
          \                    processed_dict[int(idx)] = content\n\n            #\
          \ Update the DataFrame with redacted content\n            masked_df['caption']\
          \ = masked_df.apply(\n                lambda row: processed_dict.get(row['original_index'],\
          \ row['caption']), \n                axis=1\n            )\n\n         \
          \   # Additional processing to mask all digits with asterisks\n        \
          \    def mask_digits(text):\n                \"\"\"Replaces digits with\
          \ asterisks while preserving '[REDACTED]' markers.\"\"\"\n             \
          \   if not isinstance(text, str):\n                    return text\n   \
          \             parts = text.split(\"[REDACTED]\")\n                for i\
          \ in range(len(parts)):\n                    parts[i] = re.sub(r'\\d', '*',\
          \ parts[i])\n                return \"[REDACTED]\".join(parts)\n\n     \
          \       # Apply the digit masking function to each processed caption\n \
          \           masked_df['caption'] = masked_df['caption'].apply(mask_digits)\n\
          \n            # Drop temporary columns\n            masked_df.drop(['original_index',\
          \ 'marked_caption'], axis=1, inplace=True)\n\n            logger.info(f\"\
          {contact_id}: Completed Masking PII Data\")\n            return masked_df\n\
          \n        except Exception as e:\n            raise RuntimeError(f\"mask_pii_in_captions()\
          \ failed: {str(e)}\")\n\n\n    def get_sentiment_label(row):\n        try:\n\
          \            # Check conditions in order of priority (Positive > Negative\
          \ > Neutral)\n            if row['positive'] > row['negative'] and row['positive']\
          \ > row['neutral']:\n                return 'Positive'\n            elif\
          \ row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n\
          \                return 'Negative'\n            else:\n                return\
          \ 'Neutral'\n\n        except Exception as e:\n            raise RuntimeError(f\"\
          get_sentiment_label() failed: {str(e)}\")\n\n    def get_different_times(intra_call):\n\
          \        try:\n            # Apply formatting to both time columns\n   \
          \         intra_call['start_time_second'] = (intra_call['Begin_Offset']\
          \ / 1000).astype(int)\n            intra_call['end_time_second'] = (intra_call['End_Offset']\
          \ / 1000).astype(int)\n            intra_call['time_spoken_second'] = intra_call['end_time_second']\
          \ - intra_call['start_time_second']\n            intra_call['time_spoken_second']\
          \ = intra_call['time_spoken_second'].where(intra_call['time_spoken_second']\
          \ >= 0, 0)\n            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n\
          \            intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1)\
          \ - intra_call['end_time_second']\n            intra_call['time_silence_second']\
          \ = intra_call['time_silence_second'].where(intra_call['time_silence_second']\
          \ >= 0, 0)\n            intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n\
          \            intra_call['load_date'] = datetime.now()\n\n            # Dropping\
          \ time formatted columns\n            intra_call = intra_call.drop(['Begin_Offset',\
          \ 'End_Offset'], axis=1)\n\n            return intra_call\n\n        except\
          \ Exception as e:\n            raise RuntimeError(f\"get_different_times()\
          \ failed: {str(e)}\")\n\n    def get_sentiment_scores(\n        contact_id,\n\
          \        text_list\n    ):\n        try:\n            logger.info(f\"{contact_id}:\
          \ Calculating Caption Sentiments.\")\n            dict_sentiments = []\n\
          \            for text in text_list:\n                encoded_input = tokenizer(text,\
          \ return_tensors='pt')\n                output = model_sentiment(**encoded_input)\n\
          \                scores = output[0][0].detach().numpy()\n              \
          \  scores = np.round(np.multiply(softmax(scores), 100), 2)\n           \
          \     merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n\
          \                dict_sentiments.append(merged_dict)\n\n            df_dict_sentiments\
          \ = pd.DataFrame(dict_sentiments)\n            df_dict_sentiments['sentiment_lable']\
          \ = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label,\
          \ axis=1)\n            logger.info(f\"{contact_id}: Completed calculating\
          \ Caption Sentiments.\")\n\n            return df_dict_sentiments\n\n  \
          \      except Exception as e:\n            raise RuntimeError(f\"get_sentiment_scores()\
          \ failed: {str(e)}\")\n\n    def process_transcript(\n        contact_id,\n\
          \        transcript_data,\n        tokenizer\n    ):\n        \"\"\"\n \
          \       Pre-process the transcript loaded from S3 Buckets:\n        1. Load\
          \ the transcript as Pandas Dataframe.\n        2. Select only the necessary\
          \ columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content',\
          \ 'Sentiment', 'LoudnessScore'].\n        3. Format the time in minutes\
          \ and seconds.\n        4. Rename the columns for better understanding.\n\
          \        \"\"\"\n        try:\n            logger.info(f\"{contact_id}:\
          \ Loading the Transcript as Pandas Dataframe.\")\n            transcript_df\
          \ = pd.json_normalize(transcript_data['Transcript'])\n\n            # Select\
          \ the relevant Columns\n            columns_to_select = [\n            \
          \    'BeginOffsetMillis',\n                'EndOffsetMillis',\n        \
          \        'ParticipantId',\n                'Content'\n            ]\n  \
          \          formatted_df = transcript_df[columns_to_select].copy()\n\n  \
          \          # Optionally rename columns to reflect their new format\n   \
          \         formatted_df = formatted_df.rename(columns={\n               \
          \ 'BeginOffsetMillis': 'Begin_Offset',\n                'EndOffsetMillis':\
          \ 'End_Offset',\n                'Content': 'caption',\n               \
          \ 'Sentiment': 'sentiment_label',\n                'ParticipantId': 'speaker_tag'\n\
          \            })\n\n            # Inserting the Call ID:\n            formatted_df.insert(loc=0,\
          \ column='contact_id', value=contact_id)\n            formatted_df['call_language']\
          \ = transcript_data['LanguageCode']\n\n            logger.info(f\"{contact_id}:\
          \ Returning formated DataFrame.\")\n            return formatted_df\n\n\
          \        except Exception as e:\n            raise RuntimeError(f\"process_transcript()\
          \ failed: {str(e)}\")\n\n\n    def create_intra_call_df(\n        contact_id,\n\
          \        gcp_project_id,\n        vai_gcs_bucket,\n        pipeline_run_name,\n\
          \        transcript_data,\n        tokenizer,\n        logger\n    ):\n\
          \        try:\n            logger.info(f\"{contact_id}: Creating df_intra_call\
          \ \")\n            intra_call = process_transcript(contact_id, transcript_data,\
          \ tokenizer)\n            df_sentiment_scores = get_sentiment_scores(contact_id,\
          \ intra_call.caption.to_list())\n            intra_call = pd.concat([intra_call,\
          \ df_sentiment_scores], axis=1)    \n            intra_call = get_different_times(intra_call)\n\
          \            intra_call = mask_pii_in_captions(contact_id, intra_call, gcp_project_id)\n\
          \            logger.info(f\"{contact_id}: Successfully created df_intra_call\
          \ \")\n\n            return intra_call\n\n        except Exception as e:\n\
          \            raise RuntimeError(f\"create_intra_call_df() failed: {str(e)}\"\
          )\n\n    \"\"\"\n    ========================================================\n\
          \    Function: Create Dataframe: Inter Call \n    ========================================================\n\
          \    \"\"\"\n    def dict_to_newline_string(data):\n        \"\"\"Converts\
          \ a dictionary into a new-line formatted string.\"\"\"\n        try:\n \
          \           formatted_str = \"\"\n            for key, value in data.items():\n\
          \                formatted_str += f\"{key}:\\n\"\n                for item\
          \ in value:\n                    formatted_str += f\"  - {item}\\n\"\n \
          \           return formatted_str.strip()\n\n        except Exception as\
          \ e:\n            raise RuntimeError(f\"dict_to_newline_string() failed:\
          \ {str(e)}\")\n\n\n    class CategoryValidator:\n        def __init__(self,\
          \ df_cat_subcat_mapping):\n            \"\"\"\n            Initialize with\
          \ category mapping from a Pandas DataFrame.\n            :param df_cat_subcat_mapping:\
          \ Pandas DataFrame containing 'CATEGORY' and 'SUBCATEGORY' columns.\n  \
          \          \"\"\"\n            self.df_cat_subcat_mapping = df_cat_subcat_mapping\
          \  # Ensure only the correct DataFrame is used\n            self.valid_categories\
          \ = set(df_cat_subcat_mapping['CATEGORY'].dropna().unique())\n         \
          \   self.category_subcategory_map = self._create_category_mapping()\n\n\
          \        def _create_category_mapping(self):\n            \"\"\"Create category\
          \ to subcategory mapping.\"\"\"\n            try:\n                mapping\
          \ = {}\n                for _, row in self.df_cat_subcat_mapping.dropna().iterrows():\n\
          \                    category = row['CATEGORY']\n                    subcategory\
          \ = row['SUBCATEGORY']\n\n                    if category not in mapping:\n\
          \                        mapping[category] = set()\n\n                 \
          \   if subcategory:  # Only add non-null subcategories\n               \
          \         mapping[category].add(subcategory)\n\n                return mapping\n\
          \n            except Exception as e:\n                raise RuntimeError(f\"\
          _create_category_mapping() failed: {str(e)}\")\n\n        def validate_category(self,\
          \ category: str) -> bool:\n            \"\"\"Check if category is valid.\"\
          \"\"\n            try:\n                return category in self.valid_categories\n\
          \n            except Exception as e:\n                raise RuntimeError(f\"\
          validate_category() failed: {str(e)}\")\n\n        def validate_subcategory(self,\
          \ category: str, subcategory: str) -> bool:\n            \"\"\"Check if\
          \ subcategory is valid for the given category.\"\"\"\n            try:\n\
          \                return category in self.category_subcategory_map and subcategory\
          \ in self.category_subcategory_map[category]\n\n            except Exception\
          \ as e:\n                raise RuntimeError(f\"validate_subcategory() failed:\
          \ {str(e)}\")\n\n        def get_valid_subcategories(self, category: str)\
          \ -> set:\n            \"\"\"Get valid subcategories for a category.\"\"\
          \"\n            try:\n                return self.category_subcategory_map.get(category,\
          \ set())\n\n            except Exception as e:\n                raise RuntimeError(f\"\
          get_valid_subcategories() failed: {str(e)}\")\n\n\n    class CallSummary(BaseModel):\n\
          \        summary: str = Field(..., max_length=500)\n\n    class CallTopic(BaseModel):\n\
          \        primary_topic: str = Field(..., max_length=100)\n        category:\
          \ str = Field(..., max_length=100)\n        sub_category: str = Field(...,\
          \ max_length=100)\n\n        def validate_category_mapping(self, category_validator:\
          \ CategoryValidator):\n            \"\"\"Validate category and subcategory\
          \ against mapping\"\"\"\n            try:\n                if not category_validator.validate_category(self.category):\n\
          \                    logger.error(f\"Invalid category: {self.category}\"\
          )\n                if not category_validator.validate_subcategory(self.category,\
          \ self.sub_category):\n                    logger.error(f\"Invalid subcategory\
          \ '{self.sub_category}' for category '{self.category}'\")\n\n          \
          \  except Exception as e:\n                raise RuntimeError(f\"validate_category_mapping()\
          \ failed: {str(e)}\")\n\n    class AgentCoaching(BaseModel):\n        strengths:\
          \ List[str] = Field(..., max_items=3)\n        improvement_areas: List[str]\
          \ = Field(..., max_items=3)\n        specific_recommendations: List[str]\
          \ = Field(..., max_items=4)\n        skill_development_focus: List[str]\
          \ = Field(..., max_items=3)\n\n    class TranscriptAnalysis(BaseModel):\n\
          \        call_summary: CallSummary\n        call_topic: CallTopic\n    \
          \    agent_coaching: AgentCoaching\n\n    class KPIExtractor:\n        def\
          \ __init__(self, project_id: str, location: str, df_cat_subcat_mapping):\n\
          \            \"\"\"\n            Initialize the KPIExtractor with Vertex\
          \ AI model and category validator.\n            :param project_id: GCP Project\
          \ ID\n            :param location: GCP Region\n            :param df_cat_subcat_mapping:\
          \ Pandas DataFrame with 'CATEGORY' and 'SUBCATEGORY'\n            \"\"\"\
          \n            vertexai.init(project=project_id, location=location)\n   \
          \         self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n     \
          \       self.category_validator = CategoryValidator(df_cat_subcat_mapping)\n\
          \n            self.generation_config = {\n                \"temperature\"\
          : 0.3,\n                \"max_output_tokens\": 1024,\n                \"\
          top_p\": 0.8,\n                \"top_k\": 40,\n                \"response_format\"\
          : \"json\"\n            }\n\n            self.safety_settings = {\n    \
          \            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n\
          \                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:\
          \ generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n       \
          \         generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:\
          \ generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n       \
          \         generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n\
          \            }\n\n\n        def get_categories_prompt(self) -> str:\n  \
          \          \"\"\"Create prompt section for valid categories and subcategories,\
          \ handling null values\"\"\"\n            try:\n                categories_prompt\
          \ = []\n\n                for category, subcategories in self.category_validator.category_subcategory_map.items():\n\
          \                    if category is None:  # Skip if category is None\n\
          \                        continue\n\n                    # Ensure subcategories\
          \ are valid (remove None values)\n                    valid_subcategories\
          \ = [subcat for subcat in subcategories if subcat is not None]\n\n     \
          \               if valid_subcategories:\n                        subcats\
          \ = ', '.join(sorted(valid_subcategories))\n                    else:\n\
          \                        subcats = \"No defined subcategories\"\n\n    \
          \                categories_prompt.append(f\"Category '{category}' can have\
          \ subcategories: {subcats}\")\n\n                return '\\n'.join(categories_prompt)\n\
          \n            except Exception as e:\n                raise RuntimeError(f\"\
          get_categories_prompt() failed: {str(e)}\")\n\n\n        def create_prompt(self,\
          \ transcript):\n            \"\"\"Create structured prompt with category\
          \ guidance\"\"\"\n            categories_guidance = self.get_categories_prompt()\n\
          \n            return f\"\"\"\n            Analyze this call transcript and\
          \ provide a structured analysis in the exact JSON format specified below.\n\
          \            Keep responses concise, specific, and actionable.\n\n     \
          \       Guidelines:\n            - Call summary should be factual and highlight\
          \ key interactions\n            - Topics and categories MUST match the following\
          \ valid mappings:\n            {categories_guidance}\n            - Coaching\
          \ points should be specific and actionable\n            - All responses\
          \ must follow the exact structure specified\n            - Ensure all lists\
          \ have the specified maximum number of items\n            - All text fields\
          \ must be clear, professional, and free of fluff\n\n            Transcript:\n\
          \            {transcript}\n\n            Required Output Structure:\n  \
          \          {{\n                \"call_summary\": {{\n                  \
          \  \"summary\": \"3-4 line overview of the call\"\n                }},\n\
          \                \"call_topic\": {{\n                    \"primary_topic\"\
          : \"Main topic of discussion\",\n                    \"category\": \"MUST\
          \ BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n                    \"\
          sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\
          \n                }},\n                \"agent_coaching\": {{\n        \
          \            \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength\
          \ 3\"],\n                    \"improvement_areas\": [\"Area 1\", \"Area\
          \ 2\", \"Area 3\"],\n                    \"specific_recommendations\": [\"\
          Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n                    \"skill_development_focus\"\
          : [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n                }}\n        \
          \    }}\n\n            Rules:\n            1. Maintain exact JSON structure\n\
          \            2. No additional fields or comments\n            3. No markdown\
          \ formatting\n            4. Ensure all arrays have the exact number of\
          \ items specified\n            5. Keep all text concise and professional\n\
          \            6. Do not mention any PII information such as Customer Name\
          \ etc.\n            7. STRICTLY use only the categories and subcategories\
          \ from the provided mapping\n            \"\"\"\n\n        def extract_json(self,\
          \ response):\n            \"\"\"Extract valid JSON from response\"\"\"\n\
          \            try:\n                match = re.search(r'```json\\s*([\\s\\\
          S]*?)\\s*```', response)\n                if match:\n                  \
          \  json_str = match.group(1)\n                else:\n                  \
          \  json_str = response.strip()\n                return json.loads(json_str)\n\
          \n            except Exception as e:\n                raise RuntimeError(f\"\
          extract_json() failed: {str(e)}\")\n\n\n        def validate_response(self,\
          \ response_json, contact_id = None):\n            \"\"\"Validate response\
          \ using Pydantic models and category mapping\"\"\"\n            try:\n \
          \               # First validate basic structure with Pydantic\n       \
          \         analysis = TranscriptAnalysis(**response_json)\n\n           \
          \     # Then validate category mapping\n                analysis.call_topic.validate_category_mapping(self.category_validator)\n\
          \n                return analysis\n\n            except Exception as e:\n\
          \                raise RuntimeError(f\"validate_response() failed: {str(e)}\"\
          )\n\n\n        def extract_genai_kpis(self, transcript, contact_id = None):\n\
          \            \"\"\"Extract KPIs from transcript with validation\"\"\"\n\
          \            try:\n                # Generate prompt\n                prompt\
          \ = self.create_prompt(transcript)\n\n                # Get response from\
          \ Gemini\n                response = self.model.generate_content(\n    \
          \                prompt\n                    # generation_config=self.generation_config,\n\
          \                    # safety_settings=self.safety_settings\n          \
          \      )\n\n\n                logger.debug(f\"Gemini API Response: {response}\"\
          )\n\n                # Parse JSON response\n                response_json\
          \ = self.extract_json(response.text)\n\n                # Validate response\
          \ structure and categories\n                validated_response = self.validate_response(response_json,\
          \ contact_id)\n\n                return validated_response.model_dump()\n\
          \n            except Exception as e:\n                raise RuntimeError(f\"\
          extract_genai_kpis() failed: {str(e)}\")\n\n\n\n    \"\"\"\n    ========================================================\n\
          \    Function: Create Dataframe Inter Call\n    ========================================================\n\
          \    \"\"\"\n    def create_inter_call_df(\n        contact_id,\n      \
          \  vai_gcs_bucket,\n        gcs_stagging_folder,\n        pipeline_run_name,\n\
          \        transcript_data,\n        ac_last_modified_date,\n        df_intra_call,\n\
          \        gcp_project_id,\n        gcp_project_location,\n        df_cat_subcat_mapping,\n\
          \        logger\n    ):\n        try:\n            logger.info(f\"{contact_id}:\
          \ Creating df_inter_call \")\n            logger.info(f\"{contact_id}: Extracting\
          \ KPIs from Gemini\")      \n            extractor = KPIExtractor(gcp_project_id,\
          \ gcp_project_location, df_cat_subcat_mapping)\n            transcript =\
          \ \" \".join(df_intra_call.caption)\n            call_gen_kpis = extractor.extract_genai_kpis(transcript)\n\
          \            logger.info(f\"{contact_id}: Completed Extracting KPIs from\
          \ Gemini\") \n\n            inter_call_dict = {}\n            inter_call_dict['contact_id']\
          \ = str(df_intra_call['contact_id'][0])\n            inter_call_dict['call_text']\
          \ = \" \".join(df_intra_call.caption)\n            inter_call_dict['call_summary']\
          \ = call_gen_kpis['call_summary']['summary']\n            inter_call_dict['topic']\
          \ = call_gen_kpis['call_topic']['primary_topic']\n            inter_call_dict['category']\
          \ = call_gen_kpis['call_topic']['category']\n            # inter_call_dict['category_generated']\
          \ = call_gen_kpis['call_topic']['category']\n            inter_call_dict['sub_category']\
          \ = call_gen_kpis['call_topic']['sub_category']\n            # inter_call_dict['sub_category_generated']\
          \ = call_gen_kpis['call_topic']['sub_category']\n            inter_call_dict['agent_coaching']\
          \ = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n          \
          \  df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n\n      \
          \      df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n\
          \            df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n\
          \            df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n\
          \            df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n\
          \            df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n\
          \            df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n\
          \            df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second']\
          \ - df_inter_call['total_talktime_call_second']\n            # df_inter_call['customer_instance_id']\
          \ = transcript_data['CustomerMetadata']['InstanceId']\n            # df_inter_call['call_job_status']\
          \ = transcript_data['JobStatus']\n            df_inter_call['call_language']\
          \ = transcript_data['LanguageCode']\n            df_inter_call['call_s3_uri']\
          \ = transcript_data['CustomerMetadata']['InputS3Uri']\n            df_inter_call['ac_last_modified_date']\
          \ = ac_last_modified_date\n            logger.info(f\"{contact_id}: Successfully\
          \ created df_inter_call \")\n\n            return df_inter_call\n\n    \
          \    except Exception as e:\n            raise RuntimeError(f\"create_inter_call_df()\
          \ failed: {str(e)}\")\n\n\n    \"\"\"\n    ========================================================\n\
          \    Function: Process Single Transcript\n    ========================================================\n\
          \    \"\"\"\n    def process_single_transcript(\n        pipeline_run_name,\n\
          \        gcp_project_id,\n        vai_gcs_bucket,\n        gcs_stagging_folder,\n\
          \        gcs_errored_folder,\n        gcs_logs_folder,\n        gcs_intra_call_dfs_folder,\n\
          \        gcs_inter_call_dfs_folder,\n        transcript_path,\n        tokenizer,\n\
          \        gcp_project_location,\n        df_cat_subcat_mapping\n    ):\n\n\
          \        contact_id = transcript_path.split('/')[2].split('analysis')[0].strip('_')\n\
          \        ac_last_modified_date = datetime.strptime(\n            transcript_path.split('/')[2].split('analysis_')[-1].split('.')[0].replace('_',\
          \ ':'),\n            '%Y-%m-%dT%H:%M:%SZ'\n        )\n\n        thread_log_filename\
          \ = f\"{contact_id}_{datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\
          \n        logger = setup_logger(thread_log_filename, vai_gcs_bucket, gcs_logs_folder)\n\
          \n        try:\n            client = storage.Client()\n            bucket\
          \ = client.bucket(vai_gcs_bucket)\n            blob = bucket.blob(transcript_path)\n\
          \            transcript_data = json.loads(blob. download_as_text())\n\n\
          \            logger.info(f\"{contact_id}: started processing\")\n\n    \
          \        df_intra_call = create_intra_call_df(\n                contact_id,\n\
          \                gcp_project_id,\n                vai_gcs_bucket,\n    \
          \            pipeline_run_name,\n                transcript_data,\n    \
          \            tokenizer,\n                logger\n            )\n\n     \
          \       df_inter_call = create_inter_call_df(\n                contact_id,\n\
          \                vai_gcs_bucket,\n                gcs_stagging_folder,\n\
          \                pipeline_run_name,\n                transcript_data,\n\
          \                ac_last_modified_date,\n                df_intra_call,\n\
          \                gcp_project_id,\n                gcp_project_location,\n\
          \                df_cat_subcat_mapping,\n                logger\n      \
          \      )\n\n            if not df_intra_call.empty and not df_inter_call.empty:\n\
          \                csv_path_df_intra_call = f\"gs://{vai_gcs_bucket}/{gcs_intra_call_dfs_folder}/{contact_id}_df_intra_call.csv\"\
          \n                df_intra_call.to_csv(csv_path_df_intra_call, index=False)\n\
          \                logger.info(f\"{contact_id}: Persisted: {contact_id}_df_intra_call.csv\"\
          )\n\n                csv_path_df_inter_call = f\"gs://{vai_gcs_bucket}/{gcs_inter_call_dfs_folder}/{contact_id}_df_inter_call.csv\"\
          \n                df_inter_call.to_csv(csv_path_df_inter_call, index=False)\n\
          \                logger.info(f\"{contact_id}: Persisted: {contact_id}_df_inter_call.csv\"\
          )\n\n                logger.info(f\"{contact_id}: Processing Complete\"\
          )\n                logger.info(\"\")\n                logger.info(\"\")\n\
          \n        except Exception as e:\n            handle_exception(contact_id,\
          \ vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n     \
          \       return None # Continue processing other files\n\n        finally:\n\
          \            upload_log_to_gcs(logger, thread_log_filename, vai_gcs_bucket,\
          \ gcs_logs_folder)\n\n            for handler in logger.handlers[:]:  #\
          \ Remove handlers to close logger\n                logger.removeHandler(handler)\n\
          \n    def merge_and_save_transcripts(\n        bucket_name,\n        input_folder,\n\
          \        output_folder,\n        output_file,\n        master_logger\n \
          \   ):\n        try:\n            \"\"\"Reads, merges all files in a GCS\
          \ folder, and saves the master DataFrame as CSV.\"\"\"\n            client\
          \ = storage.Client()\n            bucket = client.bucket(bucket_name)\n\n\
          \            dfs = [\n                pd.read_parquet(bucket.blob(blob.name).open(\"\
          rb\")) if blob.name.endswith(\".parquet\") \n                else pd.read_csv(bucket.blob(blob.name).open(\"\
          r\")) \n                for blob in bucket.list_blobs(prefix=input_folder)\
          \ \n                if blob.name.endswith(('.csv', '.parquet'))\n      \
          \      ]\n\n            if dfs:\n                master_df = pd.concat(dfs,\
          \ ignore_index=True)\n\n                # Convert DataFrame to CSV in-memory\n\
          \                csv_buffer = io.StringIO()\n                master_df.to_csv(csv_buffer,\
          \ index=False)\n\n                # Upload CSV to GCS\n                bucket.blob(f\"\
          {output_folder}/{output_file}\").upload_from_string(\n                 \
          \   csv_buffer.getvalue(), content_type=\"text/csv\"\n                )\n\
          \                master_logger.info(f\"Completed: merging and writing {output_file}\
          \ to {output_folder}\")\n\n        except Exception as e:\n            logger.error(f\"\
          Error processing {input_folder}: {str(e)}\")\n\n\n    \"\"\"\n    ========================================================\n\
          \    Function Calling\n    ========================================================\n\
          \    \"\"\"\n    gcs_stagging_folder=f\"{pipeline_run_name}/Stagging\"\n\
          \    gcs_errored_folder=f\"{pipeline_run_name}/Errored\"\n    gcs_logs_folder\
          \ = f\"{pipeline_run_name}/Logs\"\n    gcs_transcripts_folder=f\"{pipeline_run_name}/Transcripts\"\
          \n    gcs_intra_call_dfs_folder=f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\
          \n    gcs_inter_call_dfs_folder=f\"{pipeline_run_name}/Stagging/InterCallDFs\"\
          \n\n    # Set up master logger\n    master_log_filename = f\"process_transcripts_{datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\
          \n    master_logger = setup_logger(master_log_filename, vai_gcs_bucket,\
          \ gcs_logs_folder)\n\n    df_cat_subcat_mapping = fetch_category_mapping_from_snowflake(\n\
          \        pipeline_run_name,\n        vai_gcs_bucket,\n        gcs_stagging_folder,\n\
          \        gcs_errored_folder,\n        snf_account,\n        snf_user,\n\
          \        snf_private_key,\n        snf_private_key_pwd,\n        snf_warehouse,\n\
          \        snf_catsubcat_databse,\n        snf_catsubcat_schema,\n       \
          \ snf_catsubcat_view,\n        master_logger\n    )\n\n    transcripts_list\
          \ = fetch_transcripts_from_gcs(\n        pipeline_run_name,\n        vai_gcs_bucket,\n\
          \        gcs_stagging_folder,\n        gcs_errored_folder,\n        gcs_transcripts_folder,\n\
          \        master_logger\n    )\n\n    # Thread-safe error storage\n    error_list\
          \ = []\n    error_lock = threading.Lock()\n\n    \"\"\"Runs file processing\
          \ in multiple threads while handling errors.\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallelism)\
          \ as executor:\n        futures = [\n            executor.submit(\n    \
          \            process_single_transcript,\n                pipeline_run_name,\n\
          \                gcp_project_id, \n                vai_gcs_bucket,\n   \
          \             gcs_stagging_folder,\n                gcs_errored_folder,\n\
          \                gcs_logs_folder,\n                gcs_intra_call_dfs_folder,\n\
          \                gcs_inter_call_dfs_folder,\n                transcript_path,\n\
          \                tokenizer,\n                gcp_project_location,\n   \
          \             df_cat_subcat_mapping\n            ) for transcript_path in\
          \ transcripts_list[:2]\n        ]\n\n        # Wait for all threads to complete\n\
          \        concurrent.futures.wait(futures)\n        # results = [future.result()\
          \ for future in concurrent.futures.as_completed(futures)]\n        # print(\"\
          Processing Completed:\", results)\n\n    master_logger.info(f\"Completed:\
          \ bulk processing calls#: {len(transcripts_list)}\")\n\n    # Step 3: Merge\
          \ all outputs into master files after processing\n    merge_and_save_transcripts(\n\
          \        vai_gcs_bucket,\n        gcs_intra_call_dfs_folder,\n        gcs_stagging_folder,\n\
          \        \"master_intra_call_df.csv\",\n        master_logger\n    )\n\n\
          \    merge_and_save_transcripts(\n        vai_gcs_bucket,\n        gcs_inter_call_dfs_folder,\n\
          \        gcs_stagging_folder,\n        \"master_inter_call_df.csv\",\n \
          \       master_logger\n    )\n\n    # Merge all logs into master log\n \
          \   merge_logs_to_master(vai_gcs_bucket, gcs_logs_folder, master_log_filename,\
          \ master_logger)\n\n    master_logger.info(\"Completed transcript processing\
          \ pipeline\")\n    upload_log_to_gcs(master_logger, master_log_filename,\
          \ vai_gcs_bucket, gcs_logs_folder)\n\n    # Cleanup\n    for handler in\
          \ master_logger.handlers[:]:\n        master_logger.removeHandler(handler)\n\
          \n"
        image: us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4
pipelineInfo:
  description: Process Amazon Audio Transcripts into KPIs
  name: vai-audio-to-kpi-pipeline
root:
  dag:
    tasks:
      process-transcripts:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-process-transcripts
        inputs:
          parameters:
            gcp_project_id:
              componentInputParameter: gcp_project_id
            gcp_project_location:
              componentInputParameter: gcp_project_location
            max_parallelism:
              componentInputParameter: max_parallelism
            pipeline_run_name:
              componentInputParameter: pipeline_run_name
            snf_account:
              componentInputParameter: snf_account
            snf_catsubcat_databse:
              componentInputParameter: snf_catsubcat_databse
            snf_catsubcat_schema:
              componentInputParameter: snf_catsubcat_schema
            snf_catsubcat_view:
              componentInputParameter: snf_catsubcat_view
            snf_private_key:
              componentInputParameter: snf_private_key
            snf_private_key_pwd:
              componentInputParameter: snf_private_key_pwd
            snf_user:
              componentInputParameter: snf_user
            snf_warehouse:
              componentInputParameter: snf_warehouse
            vai_gcs_bucket:
              componentInputParameter: vai_gcs_bucket
        taskInfo:
          name: process-transcripts
  inputDefinitions:
    parameters:
      aws_access_key:
        parameterType: STRING
      aws_secret_key:
        parameterType: STRING
      gcp_project_id:
        parameterType: STRING
      gcp_project_location:
        parameterType: STRING
      max_parallelism:
        parameterType: NUMBER_INTEGER
      pipeline_run_name:
        parameterType: STRING
      s3_analysis_bucket:
        parameterType: STRING
      s3_transcripts_location:
        parameterType: STRING
      snf_account:
        parameterType: STRING
      snf_catsubcat_databse:
        parameterType: STRING
      snf_catsubcat_schema:
        parameterType: STRING
      snf_catsubcat_view:
        parameterType: STRING
      snf_database:
        parameterType: STRING
      snf_private_key:
        parameterType: STRING
      snf_private_key_pwd:
        parameterType: STRING
      snf_schema:
        parameterType: STRING
      snf_user:
        parameterType: STRING
      snf_warehouse:
        parameterType: STRING
      time_interval:
        parameterType: NUMBER_INTEGER
      vai_gcs_bucket:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.10.1
