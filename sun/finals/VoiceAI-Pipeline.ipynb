{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "from datetime import datetime, timezone\n",
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16409781-492e-4507-a1d3-50d11a3e6bea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873ad2c9-6548-4d9b-8828-f271bc72f0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "    \n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "    \n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    \n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    \n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    \n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d7d28d8-4037-4cd2-adda-70267502033c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"dev-posigen\"\n",
    "secret_id = \"dev-cx-voiceai\"\n",
    "version_id=\"1\"\n",
    "configs = access_secret(project_id, secret_id)\n",
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23fa675f-1adb-426d-9bb0-ec81b5c49174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timestamp in a Vertex-compatible format\n",
    "TIMESTAMP = datetime.now(timezone.utc).strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "VAI_GCP_PROJECT_ID = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "VAI_GCP_PROJECT_LOCATION = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "VAI_GCP_PIPELINE_BUCKET = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "VAI_GCP_PIPELINE_NAME = \"cx-voiceai-process-calls\"\n",
    "VAI_GCP_PIPELINE_RUN_NAME = f\"{configs.get(\"VAI_GCP_PIPELINE_NAME\")}-{TIMESTAMP}\"\n",
    "# VAI_GCP_PIPELINE_RUN_NAME = \"VAI_PIPELINE_RUN-2025-03-05-11-28-16\"\n",
    "GCP_PIPELINE_ROOT = f\"gs://{VAI_GCP_PIPELINE_BUCKET}/{VAI_GCP_PIPELINE_RUN_NAME}\"\n",
    "\n",
    "VAI_AWS_ACCESS_KEY = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "VAI_AWS_SECRET_KEY = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "\n",
    "VAI_S3_ANALYSIS_BUCKET = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "VAI_S3_TRANSCRIPTS_LOCATION = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "\n",
    "VAI_SNF_ACCOUNT = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "VAI_SNF_USER = configs.get(\"VAI_SNF_USER\")\n",
    "VAI_SNF_PRIVATE_KEY = configs.get(\"private_key\")\n",
    "VAI_SNF_PRIVATE_KEY_PWD = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "VAI_SNF_WAREHOUSE = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "VAI_SNF_DATABASE = configs.get(\"VAI_SNF_DATABASE\")\n",
    "VAI_SNF_SCHEMA = configs.get(\"VAI_SNF_SCHEMA\")\n",
    "VAI_SNF_CATSUBCAT_DATABASE = configs.get(\"VAI_SNF_CATSUBCAT_DATABASE\")\n",
    "VAI_SNF_CATSUBCAT_SCHEMA = configs.get(\"VAI_SNF_CATSUBCAT_SCHEMA\")\n",
    "VAI_SNF_CATSUBCAT_VIEW = configs.get(\"VAI_SNF_CATSUBCAT_VIEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5c851a8-9358-4e9a-804b-aafa20bba2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_run_name = VAI_GCP_PIPELINE_RUN_NAME\n",
    "# vai_gcs_bucket = VAI_GCP_PIPELINE_BUCKET\n",
    "# aws_access_key = VAI_AWS_ACCESS_KEY\n",
    "# aws_secret_key = VAI_AWS_SECRET_KEY\n",
    "# s3_analysis_bucket= VAI_S3_ANALYSIS_BUCKET\n",
    "# s3_transcripts_location = VAI_S3_TRANSCRIPTS_LOCATION\n",
    "# time_interval = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    # base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-2\"\n",
    ")\n",
    "def list_calls_s3_download_calls_gcs(\n",
    "    pipeline_run_name: str,    \n",
    "    vai_gcs_bucket: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    s3_analysis_bucket: str,\n",
    "    s3_transcripts_location: str,\n",
    "    time_interval: int\n",
    "):\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Definitions\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def setup_logger():\n",
    "        \"\"\"Set up a logger for the pipeline run.\"\"\"\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logging.getLogger(\"google.cloud.storage\").setLevel(logging.ERROR)\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "    logger = setup_logger()\n",
    "    \n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "        \n",
    "        \n",
    "    def generate_gcs_folders(    \n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    ):\n",
    "        try:\n",
    "            logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "            gcs_folders = {}\n",
    "            gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "            gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "            gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "            gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "            gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "\n",
    "            # Initialize GCS Client\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "            # Create empty folders directly\n",
    "            for folder in gcs_folders.values():\n",
    "                blob = bucket.blob(f\"{folder}/\")\n",
    "                blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "                logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "            logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "            return gcs_folders\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "            \n",
    "    \n",
    "    def generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(\"Started: generating S3 folder prefix.\")\n",
    "            # Get current date and time\n",
    "            current_datetime = datetime.now()\n",
    "\n",
    "            # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "            if current_datetime.hour == 0:\n",
    "                adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "            else:\n",
    "                adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "            # Extract year, month, and day from the adjusted date\n",
    "            year = str(adjusted_datetime.year)\n",
    "            month = f\"{adjusted_datetime.month:02d}\"\n",
    "            day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "            # Construct the prefix for S3 listing\n",
    "            prefix = f\"{year}/{month}/{day}/\"\n",
    "            logger.info(\"Completed: generating S3 folder prefix {prefix}.\")\n",
    "\n",
    "            return prefix\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            \n",
    "\n",
    "    def get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "            # Initialize S3 Client\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key\n",
    "            )\n",
    "\n",
    "            all_files = []\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "            # Get current UTC time (timezone-aware)\n",
    "            current_time = datetime.now(timezone.utc)\n",
    "            # Calculate the time threshold (2 hours before the current time)\n",
    "            time_threshold = current_time - timedelta(hours=time_interval)\n",
    "            logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "            all_files = []\n",
    "\n",
    "            for page in pages:\n",
    "                for obj in page.get('Contents', []):\n",
    "                    file_path = obj['Key']\n",
    "                    s3_ts = obj['LastModified']\n",
    "\n",
    "                    # Extract timestamp from filename\n",
    "                    try:\n",
    "                        # Skip non-JSON files\n",
    "                        if file_path.endswith('.json'):\n",
    "                            call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                            call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                            # Compare only the time part\n",
    "                            if call_timestamp.time() <= time_threshold.time():\n",
    "                                all_files.append({\n",
    "                                    'File': file_path,\n",
    "                                    'Call_ID': call_id,\n",
    "                                    'File_Timestamp': call_timestamp,\n",
    "                                    'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                    'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                    'S3_Timestamp': s3_ts,\n",
    "                                    'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                    'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            if all_files:\n",
    "                df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "                df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "                # Subset the DataFrame for only the most recent 2 hours bin\n",
    "                df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "            else:\n",
    "                df_calls_list = pd.DataFrame()\n",
    "\n",
    "            # Write the DataFrame to GCS\n",
    "            logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "            csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "            df_calls_list.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "            logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "            return df_calls_list\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            \n",
    "    def download_transcripts_to_gcs(\n",
    "        file,\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        s3_client,\n",
    "        s3_analysis_bucket\n",
    "    ):\n",
    "        \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "        local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "        gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "        try:\n",
    "            # Download file from S3\n",
    "            s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "            # Upload to GCS\n",
    "            blob = gcs_bucket.blob(gcs_blob_path)\n",
    "            blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "            return file, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "            handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            return None, file\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Calling\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    gcs_folders = generate_gcs_folders(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    )\n",
    "\n",
    "    gcs_staging_folder = gcs_folders['gcs_staging_folder']\n",
    "    gcs_transcripts_folder = gcs_folders['gcs_transcripts_folder']\n",
    "    gcs_errored_folder = gcs_folders['gcs_errored_folder']\n",
    "\n",
    "    s3_prefix = generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    )\n",
    "\n",
    "    df_calls_list = get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix\n",
    "    )\n",
    "\n",
    "    files_list = df_calls_list.File.to_list()\n",
    "\n",
    "    if files_list:\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\", \n",
    "            aws_access_key_id=aws_access_key, \n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        success_downloads = []\n",
    "        failed_downloads = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            logger.info(f\"Started: bulk download to GCS transcripts#: {len(files_list)}\")\n",
    "\n",
    "            future_to_file = {\n",
    "                executor.submit(\n",
    "                    download_transcripts_to_gcs,\n",
    "                    file,\n",
    "                    pipeline_run_name,\n",
    "                    vai_gcs_bucket,\n",
    "                    gcs_staging_folder,\n",
    "                    gcs_errored_folder,\n",
    "                    gcs_transcripts_folder,\n",
    "                    s3_client,\n",
    "                    s3_analysis_bucket\n",
    "                ): file for file in files_list\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_file):\n",
    "                try:\n",
    "                    success, failed = future.result()  # Get results\n",
    "\n",
    "                    if success:\n",
    "                        success_downloads.append(success)\n",
    "                    if failed:\n",
    "                        failed_downloads.append(failed)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    logger.info(f\"Completed: bulk download to GCS transcripts, Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Parallel Process Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c2288f-c868-4fcf-a173-773ba5f65706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_run_name = VAI_GCP_PIPELINE_RUN_NAME\n",
    "# print(pipeline_run_name)\n",
    "# gcp_project_id = VAI_GCP_PROJECT_ID\n",
    "# print(gcp_project_id)\n",
    "# gcp_project_location = VAI_GCP_PROJECT_LOCATION\n",
    "# print(gcp_project_location)\n",
    "# vai_gcs_bucket = VAI_GCP_PIPELINE_BUCKET\n",
    "# print(vai_gcs_bucket)\n",
    "# gcs_stagging_folder =f\"{pipeline_run_name}/Stagging\"\n",
    "# print(gcs_stagging_folder)\n",
    "# gcs_errored_folder =f\"{pipeline_run_name}/Errored\"\n",
    "# print(gcs_errored_folder)\n",
    "# gcs_transcripts_folder =f\"{pipeline_run_name}/Transcripts\"\n",
    "# print(gcs_transcripts_folder)\n",
    "# gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "# print(gcs_intra_call_dfs_folder)\n",
    "# gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "# print(gcs_inter_call_dfs_folder)\n",
    "# # transcript_path = transcripts_list[0]\n",
    "# print(transcript_path)\n",
    "# snf_account = VAI_SNF_ACCOUNT\n",
    "# print(snf_account)\n",
    "# snf_user = VAI_SNF_USER\n",
    "# print(snf_user)\n",
    "# snf_private_key = VAI_SNF_PRIVATE_KEY\n",
    "# # print(snf_private_key)\n",
    "# snf_private_key_pwd = VAI_SNF_PRIVATE_KEY_PWD\n",
    "# print(snf_private_key_pwd)\n",
    "# snf_warehouse = VAI_SNF_WAREHOUSE\n",
    "# print(snf_warehouse)\n",
    "# snf_catsubcat_databse = VAI_SNF_CATSUBCAT_DATABASE\n",
    "# print(snf_catsubcat_databse)\n",
    "# snf_catsubcat_schema = VAI_SNF_CATSUBCAT_SCHEMA\n",
    "# print(snf_catsubcat_schema)\n",
    "# snf_catsubcat_view = VAI_SNF_CATSUBCAT_VIEW\n",
    "# print(snf_catsubcat_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    # base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-2\"\n",
    ")\n",
    "def process_transcripts(\n",
    "    pipeline_run_name: str,\n",
    "    gcp_project_id: str,\n",
    "    gcp_project_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    snf_account: str,\n",
    "    snf_user: str,\n",
    "    snf_private_key: str,\n",
    "    snf_private_key_pwd: str,\n",
    "    snf_warehouse: str,\n",
    "    snf_catsubcat_databse: str,\n",
    "    snf_catsubcat_schema: str,\n",
    "    snf_catsubcat_view: str,\n",
    "    max_parallelism: int\n",
    "):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import threading\n",
    "    import concurrent.futures\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.special import softmax\n",
    "    import logging\n",
    "    import re, os, json, io\n",
    "    from datetime import datetime\n",
    "    from typing import List, Dict\n",
    "\n",
    "    from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "    import snowflake.connector as sc\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import dlp_v2\n",
    "    import vertexai\n",
    "    import vertexai.preview.generative_models as generative_models\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "    # Sentiments\n",
    "    from transformers import pipeline\n",
    "    from transformers import AutoTokenizer, AutoConfig\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Definitions\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    def setup_logger():\n",
    "        \"\"\"Set up a logger for the pipeline run.\"\"\"\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "    logger = setup_logger()\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "    def fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        List all files in a GCS bucket, handling pagination.\n",
    "\n",
    "        :param bucket_name: Name of the GCS bucket\n",
    "        :param prefix: (Optional) Folder path to filter files\n",
    "        :return: List of file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Fetching Transcripts from GCS: {gcs_transcripts_folder}\")\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blobs_iterator = bucket.list_blobs(prefix=gcs_transcripts_folder)  # GCS handles pagination internally\n",
    "\n",
    "            transcripts_list = []\n",
    "            for page in blobs_iterator.pages:  # Handling pagination\n",
    "                for blob in page:\n",
    "                    if not blob.name.endswith(\"/\"):\n",
    "                        transcripts_list.append(blob.name)\n",
    "            logger.info(f\"Completed: Fetching from GCS Transcripts List#: {len(transcripts_list)}\")\n",
    "            return transcripts_list\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "\n",
    "    def fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            # Step 3: Connect to Snowflake\n",
    "            catsubcat_conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_catsubcat_databse,\n",
    "                'schema': snf_catsubcat_schema\n",
    "            }\n",
    "\n",
    "            # Connect to Snowflake\n",
    "            conn = sc.connect(**catsubcat_conn_params)\n",
    "\n",
    "            # Fetch data from Snowflake\n",
    "            query = f\"SELECT CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            conn.close()\n",
    "            logger.info(\"Completed: Fetching Category, Sub-Category Mapping.\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Intra Call \n",
    "    ========================================================\n",
    "    \"\"\"            \n",
    "    def mask_pii_in_captions(\n",
    "        contact_id,\n",
    "        df,\n",
    "        project_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Masks PII data in the 'caption' column of a pandas DataFrame using Google Cloud DLP API.\n",
    "\n",
    "        Args:\n",
    "            contact_id: Identifier for logging purposes\n",
    "            df (pandas.DataFrame): DataFrame with a 'caption' column to process\n",
    "            project_id (str): Your Google Cloud project ID\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with masked PII in the 'caption' column\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{contact_id}: Started masking PII Data\")\n",
    "\n",
    "            # Create a copy of the DataFrame to avoid modifying the original\n",
    "            masked_df = df.copy()\n",
    "\n",
    "            # Add unique markers to each caption to identify them after processing\n",
    "            masked_df['original_index'] = masked_df.index\n",
    "            masked_df['marked_caption'] = masked_df.index.astype(str) + \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n",
    "\n",
    "            # Concatenate all captions for bulk processing\n",
    "            all_captions = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n",
    "\n",
    "            # Initialize DLP client\n",
    "            dlp_client = dlp_v2.DlpServiceClient()\n",
    "\n",
    "            # Specify the parent resource name\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "\n",
    "            # Custom dictionary detector for PosiGen\n",
    "            posigen_dictionary = {\n",
    "                \"info_type\": {\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"},\n",
    "                \"dictionary\": {\n",
    "                    \"word_list\": {\n",
    "                        \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Configure inspection config with rule set for exclusions\n",
    "            inspect_config = {\n",
    "                \"info_types\": [\n",
    "                    {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                    {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                    {\"name\": \"STREET_ADDRESS\"},\n",
    "                    {\"name\": \"IP_ADDRESS\"},\n",
    "                    {\"name\": \"DATE_OF_BIRTH\"}\n",
    "                ],\n",
    "                \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE,\n",
    "                \"custom_info_types\": [posigen_dictionary],\n",
    "                \"rule_set\": [\n",
    "                    {\n",
    "                        \"info_types\": [{\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"}],\n",
    "                        \"rules\": [\n",
    "                            {\n",
    "                                \"exclusion_rule\": {\n",
    "                                    \"matching_type\": dlp_v2.MatchingType.MATCHING_TYPE_FULL_MATCH,\n",
    "                                    \"dictionary\": {\n",
    "                                        \"word_list\": {\n",
    "                                            \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Configure deidentification to use \"[REDACTED]\" instead of asterisks\n",
    "            deidentify_config = {\n",
    "                \"info_type_transformations\": {\n",
    "                    \"transformations\": [\n",
    "                        {\n",
    "                            \"info_types\": [\n",
    "                                {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                                {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                                {\"name\": \"STREET_ADDRESS\"},\n",
    "                                {\"name\": \"IP_ADDRESS\"},\n",
    "                                {\"name\": \"DATE_OF_BIRTH\"}\n",
    "                            ],\n",
    "                            \"primitive_transformation\": {\n",
    "                                \"replace_config\": {\n",
    "                                    \"new_value\": {\"string_value\": \"[REDACTED]\"}\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Create deidentify request\n",
    "            item = {\"value\": all_captions}\n",
    "\n",
    "            # Call the DLP API\n",
    "            try:\n",
    "                response = dlp_client.deidentify_content(\n",
    "                    request={\n",
    "                        \"parent\": parent,\n",
    "                        \"deidentify_config\": deidentify_config,\n",
    "                        \"inspect_config\": inspect_config,\n",
    "                        \"item\": item,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{contact_id}: Error in DLP API call: {e}\")\n",
    "                return df  # Return original DataFrame if masking fails\n",
    "\n",
    "            # Get processed content and split by record boundaries\n",
    "            processed_content = response.item.value\n",
    "            processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\n\")\n",
    "\n",
    "            # Create mapping from original indices to processed captions\n",
    "            processed_dict = {}\n",
    "            for record in processed_records:\n",
    "                parts = record.split(\"|||SEPARATOR|||\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    idx, content = parts\n",
    "                    processed_dict[int(idx)] = content\n",
    "\n",
    "            # Update the DataFrame with redacted content\n",
    "            masked_df['caption'] = masked_df.apply(\n",
    "                lambda row: processed_dict.get(row['original_index'], row['caption']), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Additional processing to mask all digits with asterisks\n",
    "            def mask_digits(text):\n",
    "                \"\"\"Replaces digits with asterisks while preserving '[REDACTED]' markers.\"\"\"\n",
    "                if not isinstance(text, str):\n",
    "                    return text\n",
    "                parts = text.split(\"[REDACTED]\")\n",
    "                for i in range(len(parts)):\n",
    "                    parts[i] = re.sub(r'\\d', '*', parts[i])\n",
    "                return \"[REDACTED]\".join(parts)\n",
    "\n",
    "            # Apply the digit masking function to each processed caption\n",
    "            masked_df['caption'] = masked_df['caption'].apply(mask_digits)\n",
    "\n",
    "            # Drop temporary columns\n",
    "            masked_df.drop(['original_index', 'marked_caption'], axis=1, inplace=True)\n",
    "\n",
    "            logger.info(f\"{contact_id}: Completed masking PII Data\")\n",
    "            return masked_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"mask_pii_in_captions() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def get_sentiment_label(row):\n",
    "        try:\n",
    "            # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "            if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "                return 'Positive'\n",
    "            elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "                return 'Negative'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_label() failed: {str(e)}\")\n",
    "\n",
    "    def get_different_times(intra_call):\n",
    "        try:\n",
    "            # Apply formatting to both time columns\n",
    "            intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "            intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "            intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "            intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "            intra_call['load_date'] = datetime.now()\n",
    "\n",
    "            # Dropping time formatted columns\n",
    "            intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "            return intra_call\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_different_times() failed: {str(e)}\")\n",
    "\n",
    "    def get_sentiment_scores(\n",
    "        contact_id,\n",
    "        text_list\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"{contact_id}: Calculating Caption Sentiments.\")\n",
    "            dict_sentiments = []\n",
    "            for text in text_list:\n",
    "                encoded_input = tokenizer(text, return_tensors='pt')\n",
    "                output = model_sentiment(**encoded_input)\n",
    "                scores = output[0][0].detach().numpy()\n",
    "                scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "                merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "                dict_sentiments.append(merged_dict)\n",
    "\n",
    "            df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "            df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "            logger.info(f\"{contact_id}: Completed calculating Caption Sentiments.\")\n",
    "\n",
    "            return df_dict_sentiments\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_scores() failed: {str(e)}\")\n",
    "\n",
    "    def process_transcript(\n",
    "        contact_id,\n",
    "        transcript_data,\n",
    "        tokenizer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Pre-process the transcript loaded from S3 Buckets:\n",
    "        1. Load the transcript as Pandas Dataframe.\n",
    "        2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "        3. Format the time in minutes and seconds.\n",
    "        4. Rename the columns for better understanding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{contact_id}: Loading the Transcript as Pandas Dataframe.\")\n",
    "            transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "            # Select the relevant Columns\n",
    "            columns_to_select = [\n",
    "                'BeginOffsetMillis',\n",
    "                'EndOffsetMillis',\n",
    "                'ParticipantId',\n",
    "                'Content'\n",
    "            ]\n",
    "            formatted_df = transcript_df[columns_to_select].copy()\n",
    "\n",
    "            # Optionally rename columns to reflect their new format\n",
    "            formatted_df = formatted_df.rename(columns={\n",
    "                'BeginOffsetMillis': 'Begin_Offset',\n",
    "                'EndOffsetMillis': 'End_Offset',\n",
    "                'Content': 'caption',\n",
    "                'Sentiment': 'sentiment_label',\n",
    "                'ParticipantId': 'speaker_tag'\n",
    "            })\n",
    "\n",
    "            # Inserting the Call ID:\n",
    "            formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "            formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "            logger.info(f\"{contact_id}: Returning formated DataFrame.\")\n",
    "            return formatted_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"process_transcript() failed: {str(e)}\")\n",
    "                \n",
    "\n",
    "    def create_intra_call_df(\n",
    "        contact_id,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        tokenizer\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "            intra_call = process_transcript(contact_id, transcript_data, tokenizer)\n",
    "            df_sentiment_scores = get_sentiment_scores(contact_id, intra_call.caption.to_list())\n",
    "            intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "            intra_call = get_different_times(intra_call)\n",
    "            intra_call = mask_pii_in_captions(contact_id, intra_call, gcp_project_id)\n",
    "            logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "\n",
    "            return intra_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_intra_call_df() failed: {str(e)}\")\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Inter Call \n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def dict_to_newline_string(data):\n",
    "        \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "        try:\n",
    "            formatted_str = \"\"\n",
    "            for key, value in data.items():\n",
    "                formatted_str += f\"{key}:\\n\"\n",
    "                for item in value:\n",
    "                    formatted_str += f\"  - {item}\\n\"\n",
    "            return formatted_str.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"dict_to_newline_string() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    class CategoryValidator:\n",
    "        def __init__(self, df_cat_subcat_mapping):\n",
    "            \"\"\"\n",
    "            Initialize with category mapping from a Pandas DataFrame.\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame containing 'CATEGORY' and 'SUBCATEGORY' columns.\n",
    "            \"\"\"\n",
    "            self.df_cat_subcat_mapping = df_cat_subcat_mapping  # Ensure only the correct DataFrame is used\n",
    "            self.valid_categories = set(df_cat_subcat_mapping['CATEGORY'].dropna().unique())\n",
    "            self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "        def _create_category_mapping(self):\n",
    "            \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "            try:\n",
    "                mapping = {}\n",
    "                for _, row in self.df_cat_subcat_mapping.dropna().iterrows():\n",
    "                    category = row['CATEGORY']\n",
    "                    subcategory = row['SUBCATEGORY']\n",
    "\n",
    "                    if category not in mapping:\n",
    "                        mapping[category] = set()\n",
    "\n",
    "                    if subcategory:  # Only add non-null subcategories\n",
    "                        mapping[category].add(subcategory)\n",
    "\n",
    "                return mapping\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"_create_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "        def validate_category(self, category: str) -> bool:\n",
    "            \"\"\"Check if category is valid.\"\"\"\n",
    "            try:\n",
    "                return category in self.valid_categories\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category() failed: {str(e)}\")\n",
    "\n",
    "        def validate_subcategory(self, category: str, subcategory: str) -> bool:\n",
    "            \"\"\"Check if subcategory is valid for the given category.\"\"\"\n",
    "            try:\n",
    "                return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_subcategory() failed: {str(e)}\")\n",
    "\n",
    "        def get_valid_subcategories(self, category: str) -> set:\n",
    "            \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "            try:\n",
    "                return self.category_subcategory_map.get(category, set())\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_valid_subcategories() failed: {str(e)}\")\n",
    "                \n",
    "\n",
    "    class CallSummary(BaseModel):\n",
    "        summary: str = Field(..., max_length=500)\n",
    "\n",
    "    class CallTopic(BaseModel):\n",
    "        primary_topic: str = Field(..., max_length=100)\n",
    "        category: str = Field(..., max_length=100)\n",
    "        sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "        def validate_category_mapping(self, category_validator: CategoryValidator):\n",
    "            \"\"\"Validate category and subcategory against mapping\"\"\"\n",
    "            try:\n",
    "                if not category_validator.validate_category(self.category):\n",
    "                    logger.error(f\"Invalid category: {self.category}\")\n",
    "                if not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "                    logger.error(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "    class AgentCoaching(BaseModel):\n",
    "        strengths: List[str] = Field(..., max_items=3)\n",
    "        improvement_areas: List[str] = Field(..., max_items=3)\n",
    "        specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "        skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "    class TranscriptAnalysis(BaseModel):\n",
    "        call_summary: CallSummary\n",
    "        call_topic: CallTopic\n",
    "        agent_coaching: AgentCoaching\n",
    "\n",
    "    class KPIExtractor:\n",
    "        def __init__(self, project_id: str, location: str, df_cat_subcat_mapping):\n",
    "            \"\"\"\n",
    "            Initialize the KPIExtractor with Vertex AI model and category validator.\n",
    "            :param project_id: GCP Project ID\n",
    "            :param location: GCP Region\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame with 'CATEGORY' and 'SUBCATEGORY'\n",
    "            \"\"\"\n",
    "            vertexai.init(project=project_id, location=location)\n",
    "            self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "            self.category_validator = CategoryValidator(df_cat_subcat_mapping)\n",
    "\n",
    "            self.generation_config = {\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 1024,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40,\n",
    "                \"response_format\": \"json\"\n",
    "            }\n",
    "\n",
    "            self.safety_settings = {\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            }\n",
    "\n",
    "            \n",
    "        def get_categories_prompt(self) -> str:\n",
    "            \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "            try:\n",
    "                categories_prompt = []\n",
    "\n",
    "                for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "                    if category is None:  # Skip if category is None\n",
    "                        continue\n",
    "\n",
    "                    # Ensure subcategories are valid (remove None values)\n",
    "                    valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "\n",
    "                    if valid_subcategories:\n",
    "                        subcats = ', '.join(sorted(valid_subcategories))\n",
    "                    else:\n",
    "                        subcats = \"No defined subcategories\"\n",
    "\n",
    "                    categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "\n",
    "                return '\\n'.join(categories_prompt)\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_categories_prompt() failed: {str(e)}\")\n",
    "                \n",
    "\n",
    "        def create_prompt(self, transcript):\n",
    "            \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "            categories_guidance = self.get_categories_prompt()\n",
    "\n",
    "            return f\"\"\"\n",
    "            Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "            Keep responses concise, specific, and actionable.\n",
    "\n",
    "            Guidelines:\n",
    "            - Call summary should be factual and highlight key interactions\n",
    "            - Topics and categories MUST match the following valid mappings:\n",
    "            {categories_guidance}\n",
    "            - Coaching points should be specific and actionable\n",
    "            - All responses must follow the exact structure specified\n",
    "            - Ensure all lists have the specified maximum number of items\n",
    "            - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "            Transcript:\n",
    "            {transcript}\n",
    "\n",
    "            Required Output Structure:\n",
    "            {{\n",
    "                \"call_summary\": {{\n",
    "                    \"summary\": \"3-4 line overview of the call\"\n",
    "                }},\n",
    "                \"call_topic\": {{\n",
    "                    \"primary_topic\": \"Main topic of discussion\",\n",
    "                    \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                    \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "                }},\n",
    "                \"agent_coaching\": {{\n",
    "                    \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                    \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                    \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                    \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            1. Maintain exact JSON structure\n",
    "            2. No additional fields or comments\n",
    "            3. No markdown formatting\n",
    "            4. Ensure all arrays have the exact number of items specified\n",
    "            5. Keep all text concise and professional\n",
    "            6. Do not mention any PII information such as Customer Name etc.\n",
    "            7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "            \"\"\"\n",
    "\n",
    "        def extract_json(self, response):\n",
    "            \"\"\"Extract valid JSON from response\"\"\"\n",
    "            try:\n",
    "                match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "                if match:\n",
    "                    json_str = match.group(1)\n",
    "                else:\n",
    "                    json_str = response.strip()\n",
    "                return json.loads(json_str)\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"extract_json() failed: {str(e)}\")\n",
    "                \n",
    "\n",
    "        def validate_response(self, response_json, contact_id = None):\n",
    "            \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "            try:\n",
    "                # First validate basic structure with Pydantic\n",
    "                analysis = TranscriptAnalysis(**response_json)\n",
    "\n",
    "                # Then validate category mapping\n",
    "                analysis.call_topic.validate_category_mapping(self.category_validator)\n",
    "\n",
    "                return analysis\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_response() failed: {str(e)}\")\n",
    "                   \n",
    "\n",
    "        def extract_genai_kpis(self, transcript, contact_id = None):\n",
    "            \"\"\"Extract KPIs from transcript with validation\"\"\"\n",
    "            try:\n",
    "                # Generate prompt\n",
    "                prompt = self.create_prompt(transcript)\n",
    "\n",
    "                # Get response from Gemini\n",
    "                response = self.model.generate_content(\n",
    "                    prompt\n",
    "                    # generation_config=self.generation_config,\n",
    "                    # safety_settings=self.safety_settings\n",
    "                )\n",
    "\n",
    "\n",
    "                logger.debug(f\"Gemini API Response: {response}\")\n",
    "\n",
    "                # Parse JSON response\n",
    "                response_json = self.extract_json(response.text)\n",
    "\n",
    "                # Validate response structure and categories\n",
    "                validated_response = self.validate_response(response_json, contact_id)\n",
    "\n",
    "                return validated_response.model_dump()\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"extract_genai_kpis() failed: {str(e)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe Inter Call\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def create_inter_call_df(\n",
    "        contact_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        ac_last_modified_date,\n",
    "        df_intra_call,\n",
    "        gcp_project_id,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "            logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "            extractor = KPIExtractor(gcp_project_id, gcp_project_location, df_cat_subcat_mapping)\n",
    "            transcript = \" \".join(df_intra_call.caption)\n",
    "            call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "            logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "\n",
    "            inter_call_dict = {}\n",
    "            inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "            inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "            inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "            inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "            inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "            # inter_call_dict['category_generated'] = call_gen_kpis['call_topic']['category']\n",
    "            inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "            # inter_call_dict['sub_category_generated'] = call_gen_kpis['call_topic']['sub_category']\n",
    "            inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "            df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "\n",
    "            df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "            df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "            df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "            df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "            # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "            # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "            df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "            df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "            df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "            logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "\n",
    "            return df_inter_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_inter_call_df() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Process Single Transcript\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def process_single_transcript(\n",
    "        pipeline_run_name,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        transcript_path,\n",
    "        tokenizer,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping\n",
    "    ):\n",
    "        try:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(transcript_path)\n",
    "            transcript_data = json.loads(blob. download_as_text())\n",
    "\n",
    "            contact_id = transcript_path.split('/')[2].split('analysis')[0].strip('_')\n",
    "            ac_last_modified_date = datetime.strptime(\n",
    "                transcript_path.split('/')[2].split('analysis_')[-1].split('.')[0].replace('_', ':'),\n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            )\n",
    "            logger.info(f\"{contact_id}: started processing\")\n",
    "\n",
    "            df_intra_call = create_intra_call_df(\n",
    "                contact_id,\n",
    "                gcp_project_id,\n",
    "                vai_gcs_bucket,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                tokenizer\n",
    "            )\n",
    "\n",
    "            df_inter_call = create_inter_call_df(\n",
    "                contact_id,\n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                ac_last_modified_date,\n",
    "                df_intra_call,\n",
    "                gcp_project_id,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping\n",
    "            )\n",
    "\n",
    "            if not df_intra_call.empty and not df_inter_call.empty:\n",
    "                csv_path_df_intra_call = f\"gs://{vai_gcs_bucket}/{gcs_intra_call_dfs_folder}/{contact_id}_df_intra_call.csv\"\n",
    "                df_intra_call.to_csv(csv_path_df_intra_call, index=False)\n",
    "                logger.info(f\"{contact_id}: Persisted: {contact_id}_df_intra_call.csv\")\n",
    "\n",
    "                csv_path_df_inter_call = f\"gs://{vai_gcs_bucket}/{gcs_inter_call_dfs_folder}/{contact_id}_df_inter_call.csv\"\n",
    "                df_inter_call.to_csv(csv_path_df_inter_call, index=False)\n",
    "                logger.info(f\"{contact_id}: Persisted: {contact_id}_df_inter_call.csv\")\n",
    "\n",
    "                logger.info(f\"{contact_id}: Processing Complete\")\n",
    "                logger.info(\"\")\n",
    "                logger.info(\"\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            return None # Continue processing other files\n",
    "    \n",
    "    def merge_and_save_transcripts(\n",
    "            bucket_name,\n",
    "            input_folder,\n",
    "            output_folder,\n",
    "            output_file\n",
    "        ):\n",
    "        try:\n",
    "            \"\"\"Reads, merges all files in a GCS folder, and saves the master DataFrame as CSV.\"\"\"\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(bucket_name)\n",
    "\n",
    "            dfs = [\n",
    "                pd.read_parquet(bucket.blob(blob.name).open(\"rb\")) if blob.name.endswith(\".parquet\") \n",
    "                else pd.read_csv(bucket.blob(blob.name).open(\"r\")) \n",
    "                for blob in bucket.list_blobs(prefix=input_folder) \n",
    "                if blob.name.endswith(('.csv', '.parquet'))\n",
    "            ]\n",
    "\n",
    "            if dfs:\n",
    "                master_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Convert DataFrame to CSV in-memory\n",
    "                csv_buffer = io.StringIO()\n",
    "                master_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "                # Upload CSV to GCS\n",
    "                bucket.blob(f\"{output_folder}/{output_file}\").upload_from_string(\n",
    "                    csv_buffer.getvalue(), content_type=\"text/csv\"\n",
    "                )\n",
    "                logger.info(f\"Completed: merging and writing {output_file} to {output_folder}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {input_folder}: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Calling\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    gcs_stagging_folder=f\"{pipeline_run_name}/Stagging\"\n",
    "    gcs_errored_folder=f\"{pipeline_run_name}/Errored\"\n",
    "    gcs_transcripts_folder=f\"{pipeline_run_name}/Transcripts\"\n",
    "    gcs_intra_call_dfs_folder=f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "    gcs_inter_call_dfs_folder=f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "    transcripts_list = fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder\n",
    "    )\n",
    "\n",
    "    df_cat_subcat_mapping = fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view\n",
    "    )\n",
    "    \n",
    "    # Thread-safe error storage\n",
    "    error_list = []\n",
    "    error_lock = threading.Lock()\n",
    "    \n",
    "    \"\"\"Runs file processing in multiple threads while handling errors.\"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallelism) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_transcript,\n",
    "                pipeline_run_name,\n",
    "                gcp_project_id, \n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                gcs_errored_folder,\n",
    "                gcs_intra_call_dfs_folder,\n",
    "                gcs_inter_call_dfs_folder,\n",
    "                transcript_path,\n",
    "                tokenizer,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping\n",
    "            ) for transcript_path in transcripts_list\n",
    "        ]\n",
    "        \n",
    "        # results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        # print(\"Processing Completed:\", results)\n",
    "        \n",
    "    logger.info(f\"Completed: bulk processing calls#: {len(transcripts_list)}\")\n",
    "            \n",
    "    # Step 3: Merge all outputs into master files after processing\n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_intra_call_df.csv\"\n",
    "    )\n",
    "    \n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_inter_call_df.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d365f-c880-4ace-970d-a5618d503ad6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Write data to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87ce6cc8-40f2-42d0-b6be-8cd43cd52357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_run_name = VAI_GCP_PIPELINE_RUN_NAME\n",
    "# print(pipeline_run_name)\n",
    "# gcp_project_id = VAI_GCP_PROJECT_ID\n",
    "# print(gcp_project_id)\n",
    "# gcp_project_location = VAI_GCP_PROJECT_LOCATION\n",
    "# print(gcp_project_location)\n",
    "# vai_gcs_bucket = VAI_GCP_PIPELINE_BUCKET\n",
    "# print(vai_gcs_bucket)\n",
    "# gcs_stagging_folder =f\"{pipeline_run_name}/Stagging\"\n",
    "# print(gcs_stagging_folder)\n",
    "# gcs_errored_folder =f\"{pipeline_run_name}/Errored\"\n",
    "# print(gcs_errored_folder)\n",
    "# snf_account = VAI_SNF_ACCOUNT\n",
    "# print(snf_account)\n",
    "# snf_user = VAI_SNF_USER\n",
    "# print(snf_user)\n",
    "# snf_private_key = VAI_SNF_PRIVATE_KEY\n",
    "# # print(snf_private_key)\n",
    "# snf_private_key_pwd = VAI_SNF_PRIVATE_KEY_PWD\n",
    "# print(snf_private_key_pwd)\n",
    "# snf_warehouse = VAI_SNF_WAREHOUSE\n",
    "# print(snf_warehouse)\n",
    "# snf_schema = VAI_SNF_SCHEMA\n",
    "# print(snf_schema)\n",
    "# snf_database = VAI_SNF_DATABASE\n",
    "# print(snf_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92da9211-4634-4f47-9226-0bf6e6d7a165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    # base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voice-ai/voice-ai-docker-image:latest\"\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-2\"\n",
    ")\n",
    "def write_data_to_snowflake(\n",
    "    pipeline_run_name: str,\n",
    "    gcp_project_id: str,\n",
    "    gcp_project_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    snf_account: str,\n",
    "    snf_user: str,\n",
    "    snf_private_key: str,\n",
    "    snf_private_key_pwd: str,\n",
    "    snf_warehouse: str,\n",
    "    snf_database: str,\n",
    "    snf_schema: str\n",
    "):\n",
    "    import io, logging\n",
    "    import pytz\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from google.cloud import storage\n",
    "    import snowflake.connector as sc\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "    \n",
    "    # Set up logging\n",
    "    def setup_logger():\n",
    "        \"\"\"Set up a logger for the pipeline run.\"\"\"\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "    logger = setup_logger()\n",
    "    \n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "    def insert_new_records(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_databse,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        df\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "\n",
    "        Steps:\n",
    "        1. Fetches existing IDs from table.\n",
    "        2. Filters out rows with existing IDs from DataFrame.\n",
    "        3. Adds 'LOAD_DATE_UTC' column with current UTC timestamp.\n",
    "        4. Inserts only new records.\n",
    "\n",
    "        Args:\n",
    "            conn: Snowflake connection object.\n",
    "            table_name (str): Name of the target table.\n",
    "            df (pd.DataFrame): DataFrame containing the data (must have 'CONTACT_ID' column).\n",
    "\n",
    "        Returns:\n",
    "            int: Number of inserted records.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_databse,\n",
    "                'schema': snf_schema\n",
    "            }\n",
    "\n",
    "            conn = sc.connect(**conn_params)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Step 1: Get existing IDs from Snowflake table\n",
    "            cursor.execute(f\"SELECT DISTINCT(CONTACT_ID) FROM {table_name}\")\n",
    "            existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "            # Step 2: Filter DataFrame to keep only new records\n",
    "            new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "\n",
    "            if new_records_df.empty:\n",
    "                logger.info(\"No new records to insert\")\n",
    "                return 0\n",
    "\n",
    "            # Step 3: Add UTC timestamp column\n",
    "            utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_records_df = new_records_df.copy()  # Avoid modifying original df\n",
    "            new_records_df[\"LOAD_DATE\"] = utc_now  # Add new column\n",
    "\n",
    "            # Step 4: Insert new records into Snowflake\n",
    "            success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "\n",
    "            logger.info(f\"Inserted {nrows} new records with UTC load date\")\n",
    "            logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return nrows\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "\n",
    "    # Function to read CSV from GCS\n",
    "    def read_gcs_csv(file_path):\n",
    "        blob = bucket.blob(file_path)\n",
    "        csv_data = blob.download_as_text()\n",
    "        return pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "    try:\n",
    "        gcs_stagging_folder=f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_errored_folder=f\"{pipeline_run_name}/Errored\"\n",
    "        \n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "\n",
    "        # Read Inter & Intra Call DataFrames\n",
    "        inter_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_inter_call_df.csv\")\n",
    "        inter_call_df.columns = inter_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "        intra_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_intra_call_df.csv\")\n",
    "        intra_call_df.columns = intra_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "\n",
    "        logger.info(f\"Started: writing data to snowflake.\")\n",
    "        table_name ='SRC_GCP_INTER_CALLS'    \n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            inter_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTER_CALLS: Inserted records #{len(inter_call_df)}\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        table_name ='SRC_GCP_INTRA_CALLS'\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            intra_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTRA_CALLS: Inserted records #{len(intra_call_df)}\")\n",
    "        logger.info(f\"Completed: writing data to snowflake.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"VAI Audio to KPI Pipeline\",\n",
    "    description=\"Process Amazon Audio Transcripts into KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    gcp_project_id: str,\n",
    "    gcp_project_location: str,\n",
    "    vai_gcs_bucket: str,\n",
    "    aws_access_key: str,\n",
    "    aws_secret_key: str,\n",
    "    s3_analysis_bucket: str,\n",
    "    s3_transcripts_location: str,\n",
    "    time_interval: int,\n",
    "    snf_account: str,\n",
    "    snf_user: str,\n",
    "    snf_private_key: str,\n",
    "    snf_private_key_pwd: str,\n",
    "    snf_warehouse: str,\n",
    "    snf_database: str,\n",
    "    snf_schema: str,\n",
    "    snf_catsubcat_databse: str,\n",
    "    snf_catsubcat_schema: str,\n",
    "    snf_catsubcat_view: str,\n",
    "    max_parallelism: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline to:\n",
    "    1. List calls from S3 and download them to GCS.\n",
    "    2. Process each transcript in parallel using Kubeflow Pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: List and Download Calls from S3 to GCS\n",
    "    get_calls_to_process = list_calls_s3_download_calls_gcs(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        vai_gcs_bucket=vai_gcs_bucket,\n",
    "        aws_access_key=aws_access_key,\n",
    "        aws_secret_key=aws_secret_key,\n",
    "        s3_analysis_bucket=s3_analysis_bucket,\n",
    "        s3_transcripts_location=s3_transcripts_location,\n",
    "        time_interval=time_interval\n",
    "    )\n",
    "\n",
    "    # Step 2: Parallel Process Transcripts (linked to Step 1)\n",
    "    process_calls = process_transcripts(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        gcp_project_id=gcp_project_id,\n",
    "        gcp_project_location=gcp_project_location,\n",
    "        vai_gcs_bucket=vai_gcs_bucket,\n",
    "        snf_account=snf_account,\n",
    "        snf_user=snf_user,\n",
    "        snf_private_key=snf_private_key,\n",
    "        snf_private_key_pwd=snf_private_key_pwd,\n",
    "        snf_warehouse=snf_warehouse,\n",
    "        snf_catsubcat_databse=snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema=snf_catsubcat_schema,\n",
    "        snf_catsubcat_view=snf_catsubcat_view,\n",
    "        max_parallelism=max_parallelism\n",
    "    )\n",
    "    \n",
    "    # Enforce sequential execution\n",
    "    process_calls.after(get_calls_to_process)\n",
    "    \n",
    "    persist_to_snowflake = write_data_to_snowflake(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        gcp_project_id=gcp_project_id,\n",
    "        gcp_project_location=gcp_project_location,\n",
    "        vai_gcs_bucket=vai_gcs_bucket,\n",
    "        snf_account=snf_account,\n",
    "        snf_user=snf_user,\n",
    "        snf_private_key=snf_private_key,\n",
    "        snf_private_key_pwd=snf_private_key_pwd,\n",
    "        snf_warehouse=snf_warehouse,\n",
    "        snf_database=snf_database,\n",
    "        snf_schema=snf_schema\n",
    "    )\n",
    "    \n",
    "    # Enforce sequential execution\n",
    "    persist_to_snowflake.after(process_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, f'{VAI_GCP_PIPELINE_NAME}.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38487e72-e52a-43c7-917f-0d305ad627a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8625e81-29a4-44f6-90bd-c2889745f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=VAI_GCP_PROJECT_ID, location=VAI_GCP_PROJECT_LOCATION)\n",
    "\n",
    "# Create pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    job_id = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    template_path = f\"{VAI_GCP_PIPELINE_NAME}.yaml\",\n",
    "    pipeline_root = f\"gs://{VAI_GCP_PIPELINE_BUCKET}\",\n",
    "    project = VAI_GCP_PROJECT_ID,\n",
    "    location = VAI_GCP_PROJECT_LOCATION,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        \"pipeline_run_name\": VAI_GCP_PIPELINE_RUN_NAME,\n",
    "        \"gcp_project_id\": VAI_GCP_PROJECT_ID,\n",
    "        \"gcp_project_location\": VAI_GCP_PROJECT_LOCATION,\n",
    "        \"vai_gcs_bucket\": VAI_GCP_PIPELINE_BUCKET,\n",
    "        \"aws_access_key\": VAI_AWS_ACCESS_KEY,\n",
    "        \"aws_secret_key\": VAI_AWS_SECRET_KEY,\n",
    "        \"s3_analysis_bucket\": VAI_S3_ANALYSIS_BUCKET,\n",
    "        \"s3_transcripts_location\": VAI_S3_TRANSCRIPTS_LOCATION,\n",
    "        \"time_interval\": 2,\n",
    "        \"snf_account\": VAI_SNF_ACCOUNT,\n",
    "        \"snf_user\": VAI_SNF_USER,\n",
    "        \"snf_private_key\": VAI_SNF_PRIVATE_KEY,\n",
    "        \"snf_private_key_pwd\": VAI_SNF_PRIVATE_KEY_PWD,\n",
    "        \"snf_warehouse\": VAI_SNF_WAREHOUSE,\n",
    "        \"snf_database\": VAI_SNF_DATABASE,\n",
    "        \"snf_schema\": VAI_SNF_SCHEMA,\n",
    "        \"snf_catsubcat_databse\": VAI_SNF_DATABASE,\n",
    "        \"snf_catsubcat_schema\": VAI_SNF_CATSUBCAT_SCHEMA,\n",
    "        \"snf_catsubcat_view\": VAI_SNF_CATSUBCAT_VIEW,\n",
    "        \"max_parallelism\": 10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ac4-60c0-4607-aabf-23f5f31f9ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e56a67-28e5-4d7f-82a2-263e6d06e901",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3a463-dbc3-465c-a7c3-9459f26b2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vai-pipeline-run-2025-03-10-19-04-36?project=275963620760\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-03-10-19-04-36\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362431c-209a-4f36-b98a-b1894eb429d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f08ee6e-68a9-4c66-be23-c2db71c0df49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b18bf-ff1c-4550-a5c4-8b7f798aed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = kfp.Client()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
