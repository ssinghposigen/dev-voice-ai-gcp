{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8025b5fb-fd59-434f-bc3f-8494f28a5938",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b638b6-ef08-41b9-a041-45f127086797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kfp import dsl\n",
    "# from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import dlp_v2\n",
    "\n",
    "from typing import List, Dict\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json, os, ast, re\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd, numpy as np\n",
    "from scipy.special import softmax\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import pytz\n",
    "\n",
    "import snowflake.connector as sc\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "# Sentiments\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3429b-0929-47b5-a74c-cfc68592cd1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946af2f5-518e-4ba0-ab7b-d0b1aa58862a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Temporary secrets manager\n",
    "with open(\"../../sun/secrets/configs.json\", 'r') as secrets_file:\n",
    "    configs = json.load(secrets_file)\n",
    "\n",
    "loc_logs = configs.get(\"loc_logs\")\n",
    "excel_path = configs.get(\"excel_path\")\n",
    "\n",
    "aws_access_key = configs.get(\"aws_access_key\")\n",
    "aws_secret_key = configs.get(\"aws_secret_key\")\n",
    "\n",
    "# AWS\n",
    "s3_source_bucket = configs.get('s3_source_bucket')\n",
    "s3_transcripts_location = configs.get('s3_transcripts_location')\n",
    "\n",
    "# GCP\n",
    "gcp_project_id=configs.get('gcp_project_id')\n",
    "gcp_prjct_location=configs.get('gcp_prjct_location')\n",
    "\n",
    "# Snowflake\n",
    "private_key_file = configs.get('snowflakegcp_rsa_key')\n",
    "private_key_file_pwd = configs.get('snf_ssh_key_pass')\n",
    "\n",
    "# Define the Snowflake View containing category mappings\n",
    "snf_catsubcat_view = configs.get('snf_catsubcat_view')\n",
    "\n",
    "# # Sentiment Scores\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d7215-5372-442a-a5aa-eeb293a23af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bc7ee-2926-48ba-935d-94c524f01794",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Misc Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f21a74-5498-4144-bb15-90c63137e182",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initiate Master Inter and Intra Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29239bcb-462a-450b-bfb7-057149a85c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_master_dataframes():\n",
    "    if os.path.isfile(\"df_intra_calls_data.csv\"):\n",
    "        logger.info(\"df_intra_calls_data.csv exists.\") \n",
    "        df_intra_calls_data = pd.read_csv(\"df_intra_calls_data.csv\")\n",
    "        df_intra_calls_data.CONTACT_ID = df_intra_calls_data.CONTACT_ID.astype('string')\n",
    "    else:\n",
    "        logger.info(\"df_intra_calls_data.csv does not exists.\")\n",
    "        df_intra_calls_data = pd.DataFrame()\n",
    "\n",
    "    if os.path.isfile(\"df_inter_calls_data.csv\"):\n",
    "        logger.info(\"df_inter_calls_data.csv exists.\")\n",
    "        df_inter_calls_data = pd.read_csv(\"df_inter_calls_data.csv\")\n",
    "        df_inter_calls_data.CONTACT_ID = df_inter_calls_data.CONTACT_ID.astype('string')\n",
    "    else:\n",
    "        logger.info(\"df_inter_calls_data.csv does not exists.\")\n",
    "        df_inter_calls_data = pd.DataFrame()\n",
    "\n",
    "    return df_intra_calls_data, df_inter_calls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3884-9917-4dca-a767-1821346c3dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function: Listing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673b28d9-bc11-4373-936f-a62b4f05d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_new_transcripts_from_folderlist(\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    source_bucket,\n",
    "    custom_location,\n",
    "    folderlist\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch audio file metadata from S3 folders using pagination.\n",
    "    \"\"\"\n",
    "\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    # logger.info(\"Fetching New Transcripts to process...\")\n",
    "    all_files = []\n",
    "\n",
    "    # Fetch files from each folder in the list\n",
    "    for folder in folderlist:\n",
    "        try:\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=source_bucket, Prefix=folder)\n",
    "\n",
    "            for page in pages:\n",
    "                for obj in page.get('Contents', []):\n",
    "                    file_path = obj['Key']\n",
    "                    s3_ts = obj['LastModified']\n",
    "\n",
    "                    # Skip non-JSON files\n",
    "                    if not file_path.endswith('.json'):\n",
    "                        continue\n",
    "\n",
    "                    call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                    call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"))\n",
    "\n",
    "                    all_files.append({\n",
    "                        'File': file_path,\n",
    "                        'ID': call_id,\n",
    "                        'File_Timestamp': call_timestamp,\n",
    "                        'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                        'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                        'S3_Timestamp': s3_ts,\n",
    "                        'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                        'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                    })\n",
    "\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error accessing S3: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_files:\n",
    "        df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "        df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "    else:\n",
    "        df_calls_list = pd.DataFrame()\n",
    "\n",
    "    return df_calls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f442dc8-9426-4fea-80ef-0536f84b3de6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function: Read Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84739f6-1bde-4609-89b1-05728ac7ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript_from_s3(aws_access_key: str, aws_secret_key: str, s3_source_bucket: str, file_key):\n",
    "    \"\"\"\n",
    "    Read Transcript JSON content from a specific file in S3.\n",
    "    \n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_key: Full path/key of the JSON file\n",
    "    :return: Parsed JSON content\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = s3_client.get_object(Bucket=s3_source_bucket, Key=file_key)\n",
    "        \n",
    "        # Read the content\n",
    "        json_content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Parse JSON\n",
    "        return json.loads(json_content)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.info\n",
    "        logger.error(f\"Error reading Transcript JSON file {file_key}: {e}\")\n",
    "        logger.info(\"\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec06649-7cf7-45c6-b291-adb94e9f5d19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetching Category, Sub-Category Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08e18a5-924e-45b2-84eb-407a0d395089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_category_mapping_from_snowflake(catsubcat_conn_params):\n",
    "    \"\"\"Fetch Category-Subcategory mapping from a Snowflake View and return as DataFrame.\"\"\"\n",
    "    try:\n",
    "        conn = sc.connect(**catsubcat_conn_params)\n",
    "        query = f\"SELECT CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error fetching category mapping from Snowflake: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c36e3-63c8-4ae5-b1f0-12ca874abd90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "# Create Intra-call Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844bcee3-af6b-4d31-a33e-908b1883e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def millis_to_hhmmss(millis):\n",
    "    \"\"\"Convert milliseconds to mm:ss format\"\"\"\n",
    "    total_seconds = int(millis / 1000)\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = total_seconds // 60\n",
    "    seconds = total_seconds % 60\n",
    "    return f\"{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    try:\n",
    "        # Parse time string using datetime\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M:%S')\n",
    "        # Convert to timedelta and extract total seconds\n",
    "        total_seconds = time_obj.minute * 60 + time_obj.second\n",
    "        return total_seconds\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "def process_transcript(\n",
    "    transcript_data: dict,\n",
    "    contact_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pre-process the transcript loaded from S3 Buckets:\n",
    "    1. Load the transcript as Pandas Dataframe.\n",
    "    2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "    3. Format the time in minutes and seconds.\n",
    "    4. Rename the columns for better understanding.\n",
    "    \"\"\"\n",
    "    # Load the Transcript as Pandas Dataframe\n",
    "    transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "    # Select the relevant Columns\n",
    "    columns_to_select = [\n",
    "        'BeginOffsetMillis',\n",
    "        'EndOffsetMillis',\n",
    "        'ParticipantId',\n",
    "        'Content'\n",
    "    ]\n",
    "    formatted_df = transcript_df[columns_to_select].copy()\n",
    "    \n",
    "    # Optionally rename columns to reflect their new format\n",
    "    formatted_df = formatted_df.rename(columns={\n",
    "        'BeginOffsetMillis': 'Begin_Offset',\n",
    "        'EndOffsetMillis': 'End_Offset',\n",
    "        'Content': 'caption',\n",
    "        'Sentiment': 'sentiment_label',\n",
    "        'ParticipantId': 'speaker_tag'\n",
    "    })\n",
    "\n",
    "    # Inserting the Call ID:\n",
    "    formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "    formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "    return formatted_df\n",
    "\n",
    "def get_sentiment_label(row):\n",
    "    # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "    if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "        return 'Positive'\n",
    "    elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def get_sentiment_scores(text_list):\n",
    "    dict_sentiments = []\n",
    "    for text in text_list:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model_sentiment(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "        merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "        dict_sentiments.append(merged_dict)\n",
    "\n",
    "    df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "    df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "    \n",
    "    return df_dict_sentiments\n",
    "\n",
    "def get_different_times(intra_call):\n",
    "    # Apply formatting to both time columns\n",
    "    intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "    # intra_call['Begin_Offset'] = intra_call['Begin_Offset'].apply(millis_to_hhmmss)\n",
    "    intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "    # intra_call['End_Offset'] = intra_call['End_Offset'].apply(millis_to_hhmmss)\n",
    "    intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "    intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "    intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "    intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "    intra_call['load_date'] = datetime.now()\n",
    "\n",
    "    # Dropping time formatted columns\n",
    "    intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "    return intra_call\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def mask_cvv_contextually(df):\n",
    "    \"\"\"Masks CVVs with contextual awareness.\"\"\"\n",
    "    cvv_context = False\n",
    "    indices_to_mask = []\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 3  # Number of lines to wait before resetting context\n",
    "    context_counter = 0\n",
    "\n",
    "    cvv_patterns = [\n",
    "        r'\\b\\d{3}\\b',\n",
    "        r'\\b\\d{4}\\b'\n",
    "    ]\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        if re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card|the 3 digit code)\\b', caption, re.IGNORECASE):\n",
    "            cvv_context = True\n",
    "            indices_to_mask = []\n",
    "            context_counter = 0\n",
    "        elif cvv_context:\n",
    "            clean_caption = re.sub(r'[^0-9]', '', caption)  # Extract numbers\n",
    "            if clean_caption:\n",
    "                indices_to_mask.append(i)\n",
    "                context_counter += 1\n",
    "\n",
    "                for pattern in cvv_patterns:\n",
    "                    if re.search(pattern, clean_caption):\n",
    "                        for idx in indices_to_mask:\n",
    "                            masked_captions[idx] = \"[CVV_REDACTED]\"\n",
    "                        cvv_context = False\n",
    "                        indices_to_mask = []\n",
    "                        break #break the for loop, as we have found the matching pattern\n",
    "                else:\n",
    "                    if context_counter > context_timeout:\n",
    "                        cvv_context = False\n",
    "                        indices_to_mask = []\n",
    "            else:\n",
    "                context_counter += 1\n",
    "                if context_counter > context_timeout:\n",
    "                    cvv_context = False\n",
    "                    indices_to_mask = []\n",
    "\n",
    "        else:\n",
    "            cvv_context = False\n",
    "            indices_to_mask = []\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "def mask_expiration_date_contextually(df):\n",
    "    \"\"\"Masks expiration dates with contextual awareness, redacting only the pattern.\"\"\"\n",
    "    exp_date_context = False\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 4  # Number of lines to wait before resetting context\n",
    "    context_counter = 0\n",
    "\n",
    "    exp_patterns = [\n",
    "        r'\\b(0[1-9]|1[0-2])\\s*/\\s*(\\d{2}|\\d{4})\\b',  # MM/YY, MM/YYYY\n",
    "        r'\\b([1-9])\\s*/\\s*(\\d{2}|\\d{4})\\b',  # M/YY, M/YYYY\n",
    "        r'\\b(0[1-9]|1[0-2])(\\d{2}|\\d{4})\\b',  # MMYY, MMYYYY\n",
    "        r'\\b([1-9])(\\d{2}|\\d{4})\\b',  # MYY, MYYY\n",
    "        r'\\b(0[1-9]|1[0-2])\\s+(\\d{2}|\\d{4})\\b',  # MM DD, MM YYYY\n",
    "        r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*(?:\\d{2,4})?\\b', # Month name followed by day number and optional year\n",
    "        r'\\b(0[1-9]|1[0-2]):(\\d{2}|\\d{4})\\b', #MM:YY, MM:YYYY\n",
    "        r'\\b(0[1-9]|1[0-2])[-/](\\d{2}|\\d{4})\\b|\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*\\d{2,4}\\b' #DLP regex\n",
    "    ]\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        if re.search(r'\\b(?:expiration date|exp date|expiry date|Expiration is)\\b', caption, re.IGNORECASE):\n",
    "            exp_date_context = True\n",
    "            context_counter = 0\n",
    "            for pattern in exp_patterns:\n",
    "                match = re.search(pattern, caption)\n",
    "                if match:\n",
    "                    masked_captions[i] = re.sub(re.escape(match.group(0)), \"[EXPIRY_DATE_REDACTED]\", masked_captions[i])\n",
    "                    exp_date_context = False\n",
    "                    context_counter = 0\n",
    "                    break\n",
    "\n",
    "        elif exp_date_context:\n",
    "            if context_counter <= context_timeout:\n",
    "                for pattern in exp_patterns:\n",
    "                    match = re.search(pattern, caption)\n",
    "                    if match:\n",
    "                        masked_captions[i] = re.sub(re.escape(match.group(0)), \"[EXPIRY_DATE_REDACTED]\", masked_captions[i])\n",
    "                        exp_date_context = False\n",
    "                        context_counter = 0\n",
    "                        break\n",
    "                else:\n",
    "                    context_counter += 1\n",
    "            else:\n",
    "                exp_date_context = False\n",
    "                context_counter = 0\n",
    "        else:\n",
    "            context_counter = 0\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "def mask_card_numbers_contextually(df):\n",
    "    \"\"\"Masks card numbers across multiple lines, redacting only the numbers.\"\"\"\n",
    "\n",
    "    card_number_context = False\n",
    "    concatenated_number = \"\"\n",
    "    indices_to_mask = []\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 5  # Number of lines to wait before resetting context\n",
    "    context_counter = 0\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        cleaned_caption = re.sub(r'[^0-9]', '', caption)  # Extract only digits\n",
    "\n",
    "        # Detect context where card number is mentioned\n",
    "        if re.search(r'\\b(?:card number|credit card|new card number|card details|that is)\\b', caption, re.IGNORECASE):\n",
    "            card_number_context = True\n",
    "            concatenated_number = \"\"\n",
    "            indices_to_mask = [i]\n",
    "            context_counter = 0\n",
    "\n",
    "        elif card_number_context:\n",
    "            if cleaned_caption:  # If line contains numbers, capture them\n",
    "                concatenated_number += cleaned_caption\n",
    "                indices_to_mask.append(i)\n",
    "\n",
    "            context_counter += 1\n",
    "\n",
    "        # If total digits collected suggest a credit card number, redacting them\n",
    "        if len(concatenated_number) >= 13 and len(concatenated_number) <= 19:\n",
    "            for idx in indices_to_mask:\n",
    "                numbers = re.findall(r'\\d+', df['caption'][idx])\n",
    "                for num in numbers:\n",
    "                    masked_captions[idx] = re.sub(re.escape(num), \"[CARD_NUMBER_REDACTED]\", masked_captions[idx])\n",
    "            card_number_context = False  # Reset context\n",
    "            concatenated_number = \"\"\n",
    "            indices_to_mask = []\n",
    "        elif context_counter > context_timeout:\n",
    "            card_number_context = False\n",
    "            concatenated_number = \"\"\n",
    "            indices_to_mask = []\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mask_emails_contextually(df):\n",
    "    \"\"\"Masks email addresses spoken across multiple lines with contextual awareness.\"\"\"\n",
    "    \n",
    "    email_context = False\n",
    "    email_parts = []\n",
    "    indices_to_mask = []\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 5  \n",
    "    context_counter = 0\n",
    "\n",
    "    # Improved regex to match emails correctly (including optional spaces around '@' and '.')\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@\\s*[A-Za-z0-9.-]+\\s*\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        # Remove spaces around '@' and '.' to catch incorrectly spaced emails\n",
    "        cleaned_caption = re.sub(r'\\s*@\\s*', '@', caption)\n",
    "        cleaned_caption = re.sub(r'\\s*\\.\\s*', '.', cleaned_caption)\n",
    "\n",
    "        # Immediate masking if full email is found in one line\n",
    "        if re.search(email_pattern, cleaned_caption):\n",
    "            masked_captions[i] = re.sub(email_pattern, \"[EMAIL_REDACTED]\", cleaned_caption)\n",
    "            email_context = False  \n",
    "            continue  \n",
    "\n",
    "        # Detect email context\n",
    "        if re.search(r'\\b(?:email|email address|send to|mail to)\\b', cleaned_caption, re.IGNORECASE):\n",
    "            email_context = True\n",
    "            email_parts = []\n",
    "            indices_to_mask = []\n",
    "            context_counter = 0\n",
    "\n",
    "        elif email_context:\n",
    "            # Capture words containing '@' or adjacent to it\n",
    "            potential_parts = re.findall(r'\\b[A-Za-z0-9._%+-]+(?:@|(?:@[A-Za-z0-9.-]+))?\\b', cleaned_caption)\n",
    "            valid_parts = [part for part in potential_parts if '@' in part or len(email_parts) > 0]\n",
    "\n",
    "            if valid_parts:\n",
    "                indices_to_mask.append(i)\n",
    "                context_counter += 1\n",
    "                email_parts.extend(valid_parts)\n",
    "                \n",
    "                if re.search(email_pattern, \"\".join(email_parts)):\n",
    "                    for idx in indices_to_mask:\n",
    "                        masked_captions[idx] = \"[EMAIL_REDACTED]\"\n",
    "                    email_context = False\n",
    "                    email_parts = []\n",
    "                    indices_to_mask = []\n",
    "                    break  \n",
    "            else:\n",
    "                context_counter += 1\n",
    "                if context_counter > context_timeout:\n",
    "                    email_context = False\n",
    "                    email_parts = []\n",
    "                    indices_to_mask = []\n",
    "\n",
    "        else:\n",
    "            email_context = False\n",
    "            email_parts = []\n",
    "            indices_to_mask = []\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    " \n",
    "\n",
    "def mask_check_number(df):\n",
    "    \"\"\"Masks check numbers with contextual awareness.\"\"\"\n",
    "    check_context = False\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 3  # Adjust as needed\n",
    "    context_counter = 0\n",
    "\n",
    "    # Regex to capture check numbers (4-8 digits is common)\n",
    "    check_number_regex = r'\\b\\d{4,8}\\b'\n",
    "\n",
    "    for i, caption in enumerate(masked_captions):\n",
    "        if re.search(r'\\b(?:check number|cheque number|check #|cheque #)\\b', caption, re.IGNORECASE):\n",
    "            check_context = True\n",
    "            context_counter = 0\n",
    "            numbers = re.findall(check_number_regex, caption)\n",
    "            if numbers:\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[CHECK_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "        elif check_context:\n",
    "            numbers = re.findall(check_number_regex, caption)\n",
    "            if numbers:\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[CHECK_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            context_counter += 1\n",
    "            if context_counter > context_timeout:\n",
    "                check_context = False\n",
    "\n",
    "        else:\n",
    "            check_context = False\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "def mask_routing_number(df):\n",
    "    \"\"\"Masks routing numbers with contextual awareness.\"\"\"\n",
    "    routing_context = False\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 3\n",
    "    context_counter = 0\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        if re.search(r'\\b(?:routing number|ABA number|bank routing|bank details)\\b', caption, re.IGNORECASE):\n",
    "            routing_context = True\n",
    "            context_counter = 0\n",
    "            numbers = re.findall(r'\\b\\d{9}\\b', caption)\n",
    "            for num in numbers:\n",
    "                masked_captions[i] = re.sub(re.escape(num), \"[ROUTING_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "        elif routing_context:\n",
    "            numbers = re.findall(r'\\b\\d{9}\\b', caption)  # Extract 9-digit numbers\n",
    "            if numbers:\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[ROUTING_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            context_counter += 1\n",
    "            if context_counter > context_timeout:\n",
    "                routing_context = False\n",
    "\n",
    "        else:\n",
    "            routing_context = False\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "def mask_account_number(df):\n",
    "    \"\"\"Masks account numbers with contextual awareness.\"\"\"\n",
    "    account_context = False\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    context_timeout = 5\n",
    "    context_counter = 0\n",
    "\n",
    "    for i, caption in enumerate(df['caption']):\n",
    "        if re.search(r'\\b(?:account number|bank account|checking account|savings account|bank details)\\b', caption, re.IGNORECASE):\n",
    "            account_context = True\n",
    "            context_counter = 0\n",
    "            numbers = re.findall(r'\\b\\d{6,18}\\b', caption)\n",
    "            for num in numbers:\n",
    "                masked_captions[i] = re.sub(re.escape(num), \"[ACCOUNT_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "        elif account_context:\n",
    "            numbers = re.findall(r'\\b\\d{6,18}\\b', caption)  # Extract 6-18 digit numbers\n",
    "            if numbers:\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[ACCOUNT_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            context_counter += 1\n",
    "            if context_counter > context_timeout:\n",
    "                account_context = False\n",
    "\n",
    "        else:\n",
    "            account_context = False\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "def mask_card_ending(df):\n",
    "    \"\"\"Masks card endings with proximity-aware date exclusion.\"\"\"\n",
    "    masked_captions = df['caption'].astype(str).tolist()\n",
    "    address_keywords = [\"billing address\", \"address\", \"zip code\", \"postal code\", \"street\", \"city\", \"state\"]\n",
    "    date_keywords = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "    proximity_window = 50  # Adjust as needed\n",
    "\n",
    "    for i, caption in enumerate(masked_captions):\n",
    "        caption_lower = caption.lower()\n",
    "\n",
    "        # Check for \"card ending in\" context\n",
    "        if \"card ending in\" in caption_lower:\n",
    "            match = re.search(r'card ending in\\s*(\\d{4,6})\\b', caption_lower)\n",
    "            if match:\n",
    "                ending = match.group(1)\n",
    "                ending_index = caption_lower.find(match.group(0))\n",
    "\n",
    "                # Check for date keywords within the proximity window\n",
    "                date_found_nearby = False\n",
    "                for date_keyword in date_keywords:\n",
    "                    if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                        date_found_nearby = True\n",
    "                        break\n",
    "\n",
    "                if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                    masked_captions[i] = re.sub(re.escape(ending), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                continue\n",
    "\n",
    "        # Check for \"on the card\" context\n",
    "        if \"on the card\" in caption_lower:\n",
    "            match = re.search(r'on the card\\s*(\\d{4,6})\\b', caption_lower)\n",
    "            if match:\n",
    "                ending = match.group(1)\n",
    "                ending_index = caption_lower.find(match.group(0))\n",
    "\n",
    "                # Check for date keywords within the proximity window\n",
    "                date_found_nearby = False\n",
    "                for date_keyword in date_keywords:\n",
    "                    if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                        date_found_nearby = True\n",
    "                        break\n",
    "\n",
    "                if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                    masked_captions[i] = re.sub(re.escape(ending), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                continue\n",
    "\n",
    "        # Check for other card ending keywords\n",
    "        card_ending_keywords = [\"ending in\", \"ending with\", \"ends in\"]\n",
    "        for keyword in card_ending_keywords:\n",
    "            if keyword in caption_lower:\n",
    "                numbers = re.findall(r'\\b\\d{4,6}\\b', caption)\n",
    "                if numbers:\n",
    "                    for num in numbers:\n",
    "                        ending_index = caption_lower.find(keyword)\n",
    "                        date_found_nearby = False\n",
    "                        for date_keyword in date_keywords:\n",
    "                            if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                                date_found_nearby = True\n",
    "                                break\n",
    "\n",
    "                        if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                            masked_captions[i] = re.sub(re.escape(num), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                break\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def mask_account_ending(df):\n",
    "    \"\"\"Masks account endings with context and non-digit character check.\"\"\"\n",
    "    masked_captions = df['caption'].astype(str).tolist()\n",
    "\n",
    "    for i, caption in enumerate(masked_captions):\n",
    "        try:\n",
    "            match = re.search(r'(?i)(account ending in|account ending with|account ends in)\\s(\\d{4,6})\\b', caption)\n",
    "            if match:\n",
    "                ending = match.group(2)\n",
    "                ending_index = match.start(2)\n",
    "\n",
    "                # Check for non-digit characters before or after the ending digits\n",
    "                before = caption[:ending_index].strip()\n",
    "                after = caption[ending_index + len(ending):].strip()\n",
    "\n",
    "                if not before or not after:\n",
    "                    # If there's nothing before or after, it's likely just the ending digits.\n",
    "                    masked_captions[i] = re.sub(re.escape(ending), \"[ACCOUNT_ENDING_REDACTED]\", masked_captions[i])\n",
    "                else:\n",
    "                    # check if the surrounding characters are part of the context.\n",
    "                    if re.search(r'(?i)(account ending in|account ending with|account ends in)', before) or re.search(r'(?i)(account ending in|account ending with|account ends in)', after):\n",
    "                        masked_captions[i] = re.sub(re.escape(ending), \"[ACCOUNT_ENDING_REDACTED]\", masked_captions[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing line {i}: {e}\")\n",
    "\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"=================Adress Masking===========\"\"\"\n",
    "\n",
    "def mask_address(df):\n",
    "    \"\"\"Redacts addresses from a DataFrame's 'caption' column.\"\"\"\n",
    "    def clean_text(text):\n",
    "        \"\"\"Remove extra spaces and fix uppercase letter spacing.\"\"\"\n",
    "        text = re.sub(r'\\b([A-Z])(?:\\s([A-Z]))+\\b', lambda m: ''.join(m.group().split()), text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    STATE_PATTERN = r'\\b(?:Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|' \\\n",
    "                    r'Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|' \\\n",
    "                    r'Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|' \\\n",
    "                    r'North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|' \\\n",
    "                    r'South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b'\n",
    "\n",
    "    CITY_STATE_ZIP_PATTERN = r'\\b[A-Za-z]+(?:\\s[A-Za-z]+)*,?\\s(?:' + \\\n",
    "                              r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|' + \\\n",
    "                              r'Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|' + \\\n",
    "                              r'Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|' + \\\n",
    "                              r'Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|' + \\\n",
    "                              r'Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\s*\\d{5}(?:-\\d{4})?\\b'\n",
    "\n",
    "    ZIPCODE_PATTERN = r'(?<!\\d{3}-\\d{3}-)\\b\\d{5}(?:-\\d{4})?\\b(?!-\\d{3})'\n",
    "\n",
    "    HOUSE_NUMBER_PATTERN = r'\\b\\d{1,5}(?=\\s+[A-Za-z]+\\s+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct)\\b)'\n",
    "\n",
    "    STREET_SUFFIX_PATTERN = r'\\b\\d{1,5}\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct)\\b'\n",
    "    SANDWICHED_PATTERN = r'\\[ADDRESS_REDACTED\\](?:,)?\\s+(.*?)\\s+\\[ADDRESS_REDACTED\\]|\\[ADDRESS_REDACTED\\]\\s+in\\s+([A-Za-z\\s]+)\\b'\n",
    "\n",
    "    masked_captions = df['caption'].tolist()\n",
    "    for i, caption in enumerate(masked_captions):\n",
    "        caption = clean_text(caption)\n",
    "        caption = re.sub(CITY_STATE_ZIP_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        caption = re.sub(ZIPCODE_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        caption = re.sub(STATE_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        caption = re.sub(STREET_SUFFIX_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        caption = re.sub(HOUSE_NUMBER_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        caption = re.sub(SANDWICHED_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "        masked_captions[i] = caption\n",
    "    df['caption'] = masked_captions\n",
    "    return df\n",
    "\n",
    "\n",
    "def mask_pii_in_captions(contact_id, df, project_id):\n",
    "    \"\"\"Masks PII data in the 'caption' column, removing post-processing phone unredaction.\"\"\"\n",
    "    logger.info(f\"{contact_id}: Masking PII Data\")\n",
    "\n",
    "    masked_df = df.copy()\n",
    "    masked_df['original_index'] = masked_df.index\n",
    "    masked_df['previous_caption'] = masked_df['caption'].shift(1)\n",
    "\n",
    "    # Apply contextual card number, expiration date, and CVV redaction FIRST\n",
    "    masked_df = mask_card_numbers_contextually(masked_df)\n",
    "    masked_df = mask_cvv_contextually(masked_df)\n",
    "    masked_df = mask_expiration_date_contextually(masked_df)\n",
    "    masked_df = mask_emails_contextually(masked_df)\n",
    "    masked_df = mask_routing_number(masked_df)\n",
    "    masked_df = mask_account_number(masked_df)\n",
    "    masked_df = mask_card_ending(masked_df)\n",
    "    masked_df = mask_account_ending(masked_df)\n",
    "    masked_df = mask_address(masked_df)\n",
    "    masked_df = mask_check_number(masked_df)\n",
    "    \n",
    "    cvv_requested = False\n",
    "    exp_requested = False\n",
    "\n",
    "    def preprocess_text(row):\n",
    "        nonlocal cvv_requested, exp_requested\n",
    "        result = row['caption']\n",
    "        if not re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card)\\b', row['caption'], re.IGNORECASE):\n",
    "            cvv_requested = False\n",
    "        if not re.search(r'\\b(?:exp|expires|expiration|expiry)\\b', row['caption'], re.IGNORECASE):\n",
    "            exp_requested = False\n",
    "\n",
    "        if re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card)\\b', row['caption'], re.IGNORECASE):\n",
    "            cvv_requested = True\n",
    "        if re.search(r'\\b(?:exp|expires|expiration|expiry)\\b', row['caption'], re.IGNORECASE):\n",
    "            exp_requested = True\n",
    "        return result\n",
    "\n",
    "    masked_df['caption'] = masked_df.apply(preprocess_text, axis=1)\n",
    "    \n",
    "\n",
    "    masked_df['marked_caption'] = masked_df.index.astype(str) + \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n",
    "    all_captions = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n",
    "\n",
    "\n",
    "    dlp_client = dlp_v2.DlpServiceClient()\n",
    "    parent = f\"projects/{project_id}/locations/global\"\n",
    "\n",
    "    inspect_config = {\n",
    "        \"info_types\": [\n",
    "            {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "            {\"name\": \"STREET_ADDRESS\"},\n",
    "            \n",
    "            {\"name\": \"IP_ADDRESS\"},\n",
    "            {\"name\": \"DATE_OF_BIRTH\"},\n",
    "            {\"name\": \"PHONE_NUMBER\"}, # add phone number to inspect\n",
    "            {\"name\": \"EMAIL_ADDRESS\"} # added email address\n",
    "        ],\n",
    "        \"custom_info_types\": [\n",
    "            {\n",
    "                \"info_type\": {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                \"regex\": {\"pattern\": r'\\b(0[1-9]|1[0-2])[-/](\\d{2}|\\d{4})\\b|\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*\\d{2,4}\\b'},\n",
    "                \"likelihood\": dlp_v2.Likelihood.POSSIBLE\n",
    "            }\n",
    "        ],\n",
    "        \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE\n",
    "    }\n",
    "\n",
    "    deidentify_config = {\n",
    "        \"info_type_transformations\": {\n",
    "            \"transformations\": [\n",
    "                {\n",
    "                    \"info_types\": [\n",
    "                        {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                        {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                        {\"name\": \"STREET_ADDRESS\"},\n",
    "                        \n",
    "                        {\"name\": \"IP_ADDRESS\"},\n",
    "                        {\"name\": \"DATE_OF_BIRTH\"},\n",
    "                        {\"name\": \"EMAIL_ADDRESS\"} # added email address\n",
    "                    ],\n",
    "                    \"primitive_transformation\": {\n",
    "                        \"replace_config\": {\"new_value\": {\"string_value\": \"[REDACTED]\"}}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = dlp_client.deidentify_content(\n",
    "            request={\n",
    "                \"parent\": parent,\n",
    "                \"deidentify_config\": deidentify_config,\n",
    "                \"inspect_config\": inspect_config,\n",
    "                \"item\": {\"value\": all_captions}\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{contact_id}: Error in DLP API call: {e}\")\n",
    "        return df\n",
    "\n",
    "    processed_content = response.item.value\n",
    "    processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\n\")\n",
    "\n",
    "    processed_dict = {\n",
    "        int(parts[0]): parts[1]\n",
    "        for record in processed_records\n",
    "        if (parts := record.split(\"|||SEPARATOR|||\", 1)) and len(parts) == 2\n",
    "    }\n",
    "\n",
    "    masked_df['caption'] = masked_df.apply(\n",
    "        lambda row: processed_dict.get(row['original_index'], row['caption']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    masked_df.drop(['original_index', 'marked_caption', 'previous_caption'], axis=1, inplace=True)\n",
    "\n",
    "    logger.info(f\"{contact_id}: Completed Masking PII Data\")\n",
    "\n",
    "    return masked_df\n",
    "\n",
    "    \n",
    "def create_intra_call_df(aws_access_key: str, aws_secret_key: str, transcript_data: dict, contact_id: str, gcp_project_id: str):\n",
    "    intra_call = process_transcript(transcript_data, contact_id)        \n",
    "    df_sentiment_scores = get_sentiment_scores(intra_call.caption.to_list())\n",
    "    intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "    intra_call = get_different_times(intra_call)\n",
    "    intra_call = mask_pii_in_captions(contact_id, intra_call, gcp_project_id)\n",
    "    \n",
    "    return intra_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3e107-9441-4b0c-a9aa-52afc93f0750",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create Inter-call Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d8a881-3e83-4780-94e6-94f7c907a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryValidator:\n",
    "    def __init__(self, cat_subcat_mapping):\n",
    "        \"\"\"\n",
    "        Initialize with category mapping from a Snowflake View.\n",
    "        :param snowflake_conn_params: Dictionary containing Snowflake connection details.\n",
    "        :param view_name: Name of the Snowflake View containing category mappings.\n",
    "        \"\"\"\n",
    "        self.category_mapping = cat_subcat_mapping\n",
    "        self.valid_categories = set(self.category_mapping['CATEGORY'].unique())\n",
    "        self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "    def _create_category_mapping(self):\n",
    "        \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "        mapping = {}\n",
    "        for _, row in self.category_mapping.iterrows():\n",
    "            if row['CATEGORY'] not in mapping:\n",
    "                mapping[row['CATEGORY']] = set()\n",
    "            mapping[row['CATEGORY']].add(row['SUBCATEGORY'])\n",
    "        return mapping\n",
    "\n",
    "    def validate_category(self, category: str) -> bool:\n",
    "        \"\"\"Check if category is valid.\"\"\"\n",
    "        return category in self.valid_categories\n",
    "\n",
    "    def validate_subcategory(self, category: str, subcategory: str) -> bool:\n",
    "        \"\"\"Check if subcategory is valid for given category.\"\"\"\n",
    "        return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "\n",
    "    def get_valid_subcategories(self, category: str):\n",
    "        \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "        return self.category_subcategory_map.get(category, set())\n",
    "\n",
    "class CallSummary(BaseModel):\n",
    "    summary: str = Field(..., max_length=500)\n",
    "\n",
    "class CallTopic(BaseModel):\n",
    "    primary_topic: str = Field(..., max_length=100)\n",
    "    category: str = Field(..., max_length=100)\n",
    "    sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "    def validate_category_mapping(self, category_validator: CategoryValidator) -> bool:\n",
    "        \"\"\"Validate category and subcategory against mapping\"\"\"\n",
    "        if not category_validator.validate_category(self.category):\n",
    "            logger.error(f\"Invalid category: {self.category}\")\n",
    "        if not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "            logger.error(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'\")\n",
    "\n",
    "class AgentCoaching(BaseModel):\n",
    "    strengths: List[str] = Field(..., max_items=3)\n",
    "    improvement_areas: List[str] = Field(..., max_items=3)\n",
    "    specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "    skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "class TranscriptAnalysis(BaseModel):\n",
    "    call_summary: CallSummary\n",
    "    call_topic: CallTopic\n",
    "    agent_coaching: AgentCoaching\n",
    "\n",
    "class KPIExtractor:\n",
    "    def __init__(self, project_id: str, location: str, excel_path: str):\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "        self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "        self.category_validator = CategoryValidator(cat_subcat_mapping)\n",
    "        \n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 1024,\n",
    "            \"top_p\": 0.8,\n",
    "            \"top_k\": 40,\n",
    "            \"response_format\": \"json\"\n",
    "        }\n",
    "        \n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        }\n",
    "\n",
    "    \n",
    "    def get_categories_prompt(self) -> str:\n",
    "        \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "        categories_prompt = []\n",
    "        \n",
    "        for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "            if category is None:  # Skip if category is None\n",
    "                continue\n",
    "            \n",
    "            # Ensure subcategories are valid (remove None values)\n",
    "            valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "            \n",
    "            if valid_subcategories:\n",
    "                subcats = ', '.join(sorted(valid_subcategories))\n",
    "            else:\n",
    "                subcats = \"No defined subcategories\"\n",
    "            \n",
    "            categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "        \n",
    "        return '\\n'.join(categories_prompt)\n",
    "\n",
    "    def create_prompt(self, transcript: str) -> str:\n",
    "        \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "        categories_guidance = self.get_categories_prompt()\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "        Keep responses concise, specific, and actionable.\n",
    "    \n",
    "        Guidelines:\n",
    "        - Call summary should be factual and highlight key interactions\n",
    "        - Topics and categories MUST match the following valid mappings:\n",
    "        {categories_guidance}\n",
    "        - If the calls are received, forwarded or reached to a voicemail then always map:\n",
    "            Category=\"Unsuccessful Contact\" and Sub-Category=\"Voicemail\"\n",
    "        - Coaching points should be specific and actionable\n",
    "        - All responses must follow the exact structure specified\n",
    "        - Ensure all lists have the specified maximum number of items\n",
    "        - All text fields must be clear, professional, and free of fluff\n",
    "    \n",
    "        Transcript:\n",
    "        {transcript}\n",
    "    \n",
    "        Required Output Structure:\n",
    "        {{\n",
    "            \"call_summary\": {{\n",
    "                \"summary\": \"3-4 line overview of the call\"\n",
    "            }},\n",
    "            \"call_topic\": {{\n",
    "                \"primary_topic\": \"Main topic of discussion\",\n",
    "                \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "            }},\n",
    "            \"agent_coaching\": {{\n",
    "                \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "            }}\n",
    "        }}\n",
    "    \n",
    "        Rules:\n",
    "        1. Maintain exact JSON structure\n",
    "        2. No additional fields or comments\n",
    "        3. No markdown formatting\n",
    "        4. Ensure all arrays have the exact number of items specified\n",
    "        5. Keep all text concise and professional\n",
    "        6. Do not mention any PII information such as Customer Name etc.\n",
    "        7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "        \"\"\"\n",
    "    \n",
    "    def extract_json(self, response: str) -> Dict:\n",
    "        \"\"\"Extract valid JSON from response\"\"\"\n",
    "        match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "        else:\n",
    "            json_str = response.strip()\n",
    "        \n",
    "        try:\n",
    "           return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "           logger.error(\"Invalid JSON response\")\n",
    "           pass\n",
    "\n",
    "    def validate_response(self, response_json: Dict, contact_id: str = None) -> TranscriptAnalysis:\n",
    "        \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "        try:\n",
    "            # First validate basic structure with Pydantic\n",
    "            analysis = TranscriptAnalysis(**response_json)\n",
    "            \n",
    "            # Then validate category mapping\n",
    "            analysis.call_topic.validate_category_mapping(self.category_validator)\n",
    "            \n",
    "            return analysis\n",
    "        except ValidationError as e:\n",
    "            logger.error(f\"{contact_id if contact_id else ''}: Pydantic validation error - {e}\")\n",
    "            pass\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"{contact_id if contact_id else ''}: Category validation error - {e}\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    def extract_genai_kpis(\n",
    "       self,\n",
    "       transcript: str,\n",
    "       contact_id: str = None\n",
    "    ):\n",
    "        \"\"\"Extract KPIs from transcript with validation and retries\"\"\"\n",
    "        max_retries = 3\n",
    "        attempt = 0\n",
    "    \n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                # Generate prompt\n",
    "                prompt = self.create_prompt(transcript)\n",
    "    \n",
    "                # Get response from Gemini\n",
    "                response = self.model.generate_content(prompt)\n",
    "    \n",
    "                # Parse JSON response\n",
    "                response_json = self.extract_json(response.text)\n",
    "    \n",
    "                # If response is empty, retry\n",
    "                if not response_json or \"NA\" in response_json.values():\n",
    "                    logger.warning(f\"Attempt {attempt + 1}: Gemini returned NA or empty response. Retrying...\")\n",
    "                    attempt += 1\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "                    continue\n",
    "    \n",
    "                # Validate response\n",
    "                validated_response = self.validate_response(response_json, contact_id)\n",
    "    \n",
    "                if validated_response:\n",
    "                    return validated_response.model_dump()\n",
    "    \n",
    "                logger.warning(f\"Attempt {attempt + 1}: Invalid response structure. Retrying...\")\n",
    "                attempt += 1\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt + 1}: Error extracting KPIs: {str(e)}\")\n",
    "                attempt += 1\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "    \n",
    "        logger.error(f\"Failed to extract valid KPIs after {max_retries} attempts.\")\n",
    "        return {\"error\": \"Failed to extract KPIs after multiple attempts\"}\n",
    "\n",
    "def dict_to_newline_string(data: dict) -> str:\n",
    "    \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for key, value in data.items():\n",
    "        formatted_str += f\"{key}:\\n\"\n",
    "        for item in value:\n",
    "            formatted_str += f\"  - {item}\\n\"\n",
    "    return formatted_str.strip()\n",
    "\n",
    "def create_inter_call_df(\n",
    "    gcp_project_id: str,\n",
    "    gcp_prjct_location: str,\n",
    "    df_intra_call: pd.DataFrame,\n",
    "    transcript_data: dict,\n",
    "    ac_last_modified_date: datetime,\n",
    "    cat_subcat_mapping: pd.DataFrame\n",
    "):\n",
    "    try:\n",
    "        contact_id = df_intra_call.contact_id.unique        \n",
    "    \n",
    "        # logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "        extractor = KPIExtractor(gcp_project_id, gcp_prjct_location, excel_path)\n",
    "        transcript = \" \".join(df_intra_call.caption)\n",
    "        call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "        # logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "    \n",
    "        # logger.info(f\"{contact_id}: Creating Inter Call df\")\n",
    "        inter_call_dict = {}\n",
    "        inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "        inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "        inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "        inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "        inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "        inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "        inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "        \n",
    "        df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "        \n",
    "        # Replace values where Categories are not in allowed list\n",
    "        allowed_categories = cat_subcat_mapping['CATEGORY'].drop_duplicates().to_list()\n",
    "        df_inter_call.loc[\n",
    "            ~df_inter_call['category'].isin(allowed_categories) | df_inter_call['category'].isna(),\n",
    "            ['category', 'sub_category']\n",
    "        ] = 'Unspecified'\n",
    "    \n",
    "        # Add metadata from AWS\n",
    "        # df_inter_call['account_id'] = transcript_data['AccountId']\n",
    "        df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "        df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "        df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "        df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "        df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "        df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "        df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "        df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "        # df_inter_call['load_date'] = datetime.now()\n",
    "        \n",
    "        return df_inter_call\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info\n",
    "        logger.error(f\"{contact_id}: Error Creating Intra Call df: {e}\")\n",
    "        logger.info(\"\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc4160-5de2-42de-bd6f-e66070409da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Writing Dataframe to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb02876-9062-47ca-890b-feace9e4e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_records(conn, table_name, df):\n",
    "    \"\"\"\n",
    "    Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetches existing IDs from table.\n",
    "    2. Filters out rows with existing IDs from DataFrame.\n",
    "    3. Adds 'LOAD_DATE_UTC' column with current UTC timestamp.\n",
    "    4. Inserts only new records.\n",
    "\n",
    "    Args:\n",
    "        conn: Snowflake connection object.\n",
    "        table_name (str): Name of the target table.\n",
    "        df (pd.DataFrame): DataFrame containing the data (must have 'CONTACT_ID' column).\n",
    "\n",
    "    Returns:\n",
    "        int: Number of inserted records.\n",
    "    \"\"\"    \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Step 1: Get existing IDs from Snowflake table\n",
    "    cursor.execute(f\"SELECT DISTINCT(CONTACT_ID) FROM {table_name}\")\n",
    "    existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "    \n",
    "    # Step 2: Filter DataFrame to keep only new records\n",
    "    new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "    \n",
    "    if new_records_df.empty:\n",
    "        logger.info(\"No new records to insert\")\n",
    "        return 0\n",
    "    \n",
    "    # Step 3: Add UTC timestamp column\n",
    "    utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    new_records_df = new_records_df.copy()  # Avoid modifying original df\n",
    "    new_records_df[\"LOAD_DATE\"] = utc_now  # Add new column\n",
    "\n",
    "    # Step 4: Insert new records into Snowflake\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "    \n",
    "    logger.info(f\"Inserted {nrows} new records with UTC load date\")\n",
    "    logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "    \n",
    "    cursor.close()\n",
    "    return nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edab60-7bed-42f8-a9d6-3224fcd3044f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5526c69a-444f-434a-9f45-5b4f782121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(data_frame, columns_to_check):\n",
    "    \"\"\"\n",
    "    Dedupes the final Dataframes to be written to the Snowflake Tables\n",
    "    \"\"\"\n",
    "    # Remove duplicate records based on the specified columns, keeping the first occurrence\n",
    "    df_cleaned = data_frame.drop_duplicates(subset=columns_to_check, keep=\"first\")\n",
    "\n",
    "    # (Optional) Reset index after removing duplicates\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e49a1-0c68-4284-9f22-52037ebb0242",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exception Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1dafb-7ae2-4261-ad69-24529b979f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb462a9e-aee0-4d8b-9eac-aa7ce2659532",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Logging Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c393b25d-1a31-43af-ad83-e9b52dd24852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_file):\n",
    "    \"\"\"\n",
    "    Sets up a logger that writes to both file and console with timestamp.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Name of the log file to write to.\n",
    "\n",
    "    Returns:\n",
    "        logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('voice_ai_logger')\n",
    "\n",
    "    # Reset handlers if already exist\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Set log level\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # # Create a stream handler (console)\n",
    "    # console_handler = logging.StreamHandler()\n",
    "    # console_handler.setFormatter(formatter)\n",
    "    # logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def reset_logging():\n",
    "    \"\"\"Removes all logging handlers and resets the logger.\"\"\"\n",
    "    # Get all loggers\n",
    "    loggers = list(logging.root.manager.loggerDict.values())\n",
    "    \n",
    "    # Include root logger explicitly\n",
    "    loggers.append(logging.getLogger())\n",
    "\n",
    "    for logger in loggers:\n",
    "        if isinstance(logger, logging.Logger):  # Ensure it's a valid logger instance\n",
    "            for handler in logger.handlers[:]:  # Copy list to avoid modification issues\n",
    "                logger.removeHandler(handler)\n",
    "                handler.close()\n",
    "\n",
    "            logger.setLevel(logging.NOTSET)\n",
    "            logger.propagate = False  # Avoid duplicate logs from propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d121fa-be88-424a-9d70-507b203a1196",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763a4ba2-8b9b-4346-8dde-ea2e0cccd5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analysis/Voice/2025/04/09/', 'Analysis/Voice/2025/04/08/']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Get data for last 2 days\n",
    "num_days = 2\n",
    "dates_to_check = []\n",
    "for i in range(num_days):\n",
    "    check_date = current_date - timedelta(days=i)\n",
    "    dates_to_check.append(check_date)\n",
    "\n",
    "# Get data for the last 12 hrs:\n",
    "start_datetime = current_datetime - timedelta(hours=12)\n",
    "\n",
    "# Process each date\n",
    "folderlist = []\n",
    "for date in dates_to_check:\n",
    "    year = str(date.year)\n",
    "    month = f\"{date.month:02d}\"\n",
    "    day = f\"{date.day:02d}\"\n",
    "    \n",
    "    # Construct the prefix for S3 listing\n",
    "    prefix = f\"{s3_transcripts_location}/{year}/{month}/{day}/\"\n",
    "    folderlist.append(prefix)\n",
    "folderlist[:num_days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5058ce-32f4-4205-8ba9-8746e6806285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reset_logging()\n",
    "\n",
    "# Setup logger\n",
    "log_file='voice_ai_runtime_Masking.logs'\n",
    "logger = setup_logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96881f94-bfd4-46b3-b88f-32e1e76a99fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account': 'XV37144.us-central1.gcp',\n",
       " 'user': 'GCP_INTEGRATION',\n",
       " 'private_key_file': '../../sun/secrets/snowflakegcp_rsa_key.p8',\n",
       " 'private_key_file_pwd': '$07@rF0r@77!',\n",
       " 'warehouse': 'DATAPLATR',\n",
       " 'database': 'POSIGEN_DEV',\n",
       " 'schema': 'SIGMA_CX'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catsubcat_conn_params = {\n",
    "    'account': configs.get('snf_account'),\n",
    "    'user': configs.get('snf_user'),\n",
    "    'private_key_file': configs.get('snf_private_key_file'),\n",
    "    'private_key_file_pwd':configs.get('snf_private_key_pwd'),\n",
    "    'warehouse': configs.get('snf_warehouse'),\n",
    "    'database': 'POSIGEN_DEV',\n",
    "    'schema': configs.get('snf_catsubcat_schema')\n",
    "}\n",
    "\n",
    "catsubcat_conn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91536490-e111-49cd-891d-88e18410d09f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_832911/3783910305.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Fetching Category, Sub-category Mapping.\")\n",
    "cat_subcat_mapping = fetch_category_mapping_from_snowflake(catsubcat_conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "826d0836-e233-4ce4-88f2-0411353dc5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the transcripts in to_process_folder\n",
    "df_list_transcripts = list_new_transcripts_from_folderlist(aws_access_key, aws_secret_key, s3_source_bucket, s3_transcripts_location, folderlist[:num_days])\n",
    "df_list_transcripts.to_csv(\"df_list_transcripts.csv\")\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(f\"Transcripts to process: {len(df_list_transcripts)}\")\n",
    "# logger.info(df_list_transcripts.groupby(['File_Date']).size().reset_index(name='frequency'))\n",
    "# logger.info(df_list_transcripts[:3000].groupby(['File_Date', 'Time_Bin']).size().reset_index(name='Count'))\n",
    "logger.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684dfce0-3dc8-42b2-a0eb-07d01808d140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called: To make sure previous calls are loaded into memory again.\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initiating Master DataFrames\n",
    "print(\"Called: To make sure previous calls are loaded into memory again.\")\n",
    "logger.info(\"Called: Initiate Master Dataframes\")\n",
    "df_intra_calls_data, df_inter_calls_data = initiate_master_dataframes()\n",
    "print(len(df_inter_calls_data))\n",
    "print(len(df_intra_calls_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb7b03f4-87ed-4f08-8328-5006e4534573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are transcripts to be processed\n",
    "if len(df_list_transcripts) == 0:\n",
    "    logger.info(\"No Transcripts to Process\")\n",
    "    logger.info(\"\")\n",
    "\n",
    "else:\n",
    "    for transcript in df_list_transcripts.File.to_list():\n",
    "        logger.info(\"--------------------------\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # get the call ID\n",
    "            contact_id = transcript.split('/')[-1].split('.')[0].split('analysis')[0].strip('_')\n",
    "            ac_last_modified_date = datetime.strptime(\n",
    "                                            transcript.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), \n",
    "                                            \"%Y-%m-%dT%H:%M:%S\"\n",
    "                                        ).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "            # Check if Call Already Processed\n",
    "            if (len(df_intra_calls_data) > 0 and contact_id in df_intra_calls_data.CONTACT_ID.unique()) and (len(df_inter_calls_data) > 0 and contact_id in df_inter_calls_data.CONTACT_ID.unique()):\n",
    "                logger.info(f\"{contact_id}: Call already Processed.\")\n",
    "                # break\n",
    "    \n",
    "            else:\n",
    "                # get the audio transcript file name\n",
    "                logger.info(f\"{contact_id}: Processing\") \n",
    "                \n",
    "                # Get the Transcript file from S3 Bucket\n",
    "                logger.info(f\"{contact_id}: Fetching Transcript from S3\")\n",
    "                transcript_data = fetch_transcript_from_s3(aws_access_key, aws_secret_key, s3_source_bucket, transcript)\n",
    "                logger.info(f\"{contact_id}: Successfully fetched the Transcript from S3 {len(transcript_data)}\")\n",
    "            \n",
    "                if transcript_data: \n",
    "                    # Create the Inter Call KPIs\n",
    "                    logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "                    df_intra_call = create_intra_call_df(aws_access_key, aws_secret_key, transcript_data, contact_id, gcp_project_id)\n",
    "                    logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "                        \n",
    "                    # Create the Intra Call KPIs\n",
    "                    logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "                    df_inter_call = create_inter_call_df(gcp_project_id, gcp_prjct_location, df_intra_call, transcript_data, ac_last_modified_date, cat_subcat_mapping)\n",
    "                    logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "\n",
    "\n",
    "                    # ###============================================================###\n",
    "                    # Save DataFrames only when both are having data\n",
    "                    if not df_intra_call.empty and not df_inter_call.empty:\n",
    "                        # Appending to Intra-calls Master DataFrame\n",
    "                        df_intra_call.columns = df_intra_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "                        df_intra_calls_data = pd.concat([df_intra_calls_data, df_intra_call], ignore_index=True)\n",
    "                        df_intra_calls_data.to_csv(\"df_intra_calls_data.csv\", index=False)\n",
    "                        logger.info(f\"{contact_id}: Persisted df_intra_calls_data to CSV.\")\n",
    "\n",
    "                        # Appending to Inter-calls Master DataFrame\n",
    "                        df_inter_call.columns = df_inter_call.columns.str.upper()  # Capitalising Column names for Snowflake\n",
    "                        df_inter_calls_data = pd.concat([df_inter_calls_data, df_inter_call], ignore_index=True)\n",
    "                        df_inter_calls_data.to_csv(\"df_inter_calls_data.csv\", index=False)\n",
    "                        logger.info(f\"{contact_id}: Persisted df_intra_calls_data to CSV.\")\n",
    "                        # logger.info(f\"{contact_id}: Processing Complete\")\n",
    "\n",
    "                    else:\n",
    "                        if df_intra_call.empty:\n",
    "                            logger.error(\"Intra Call DataFrame was not created successfully.\")\n",
    "                        if df_inter_call.empty:\n",
    "                            logger.error(\"Inter Call DataFrame was not created successfully.\")\n",
    "            \n",
    "            if len(df_inter_calls_data)%20 == 0:\n",
    "                logger.info(\"--------------------------\")\n",
    "                logger.info(f\"Processed {len(df_inter_calls_data)} files\")\n",
    "                logger.info(\"--------------------------\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"--------------------------\")\n",
    "            logger.error(f\"{contact_id}: Exception - {e}\")\n",
    "            logger.error(\"--------------------------\")\n",
    "            continue\n",
    "\n",
    "            \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time  # Time taken for this iteration\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        logger.info(f\"{contact_id}: Processed Call #{len(df_inter_calls_data)} in {int(minutes)} min {seconds:.2f} sec elapsed\")\n",
    "        logger.info(\"--------------------------\")\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"\")\n",
    "        \n",
    "    logger.info(f\"Removing duplicates in df_inter_calls_data.\")\n",
    "    columns_to_check = [\"CONTACT_ID\"]\n",
    "    df_inter_calls_data = handle_duplicates(df_inter_calls_data, columns_to_check)\n",
    "    logger.info(f\"Removing duplicates in df_intra_calls_data.\")\n",
    "    columns_to_check = [\"CONTACT_ID\", \"SPEAKER_TAG\", \"CAPTION\", \"START_TIME_SECOND\", \"END_TIME_SECOND\"]\n",
    "    df_intra_calls_data = handle_duplicates(df_intra_calls_data, columns_to_check)\n",
    "\n",
    "    logger.info(f\"Completed processing {len(df_list_transcripts)} Calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07521d9f-3814-48ef-a2f3-e03588a477d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "607cfd7e-d1d4-4044-abec-e1389ef9cd20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account': 'XV37144.us-central1.gcp',\n",
       " 'user': 'GCP_INTEGRATION',\n",
       " 'private_key_file': '../../sun/secrets/snowflakegcp_rsa_key.p8',\n",
       " 'private_key_file_pwd': '$07@rF0r@77!',\n",
       " 'warehouse': 'DATAPLATR',\n",
       " 'database': 'POSIGEN_DEV',\n",
       " 'schema': 'SRC_GCP'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_params = {\n",
    "    'account': configs.get('snf_account'),\n",
    "    'user': configs.get('snf_user'),\n",
    "    'private_key_file': configs.get('snf_private_key_file'),\n",
    "    'private_key_file_pwd':configs.get('snf_private_key_pwd'),\n",
    "    'warehouse': configs.get('snf_warehouse'),\n",
    "    'database': 'POSIGEN_DEV',\n",
    "    'schema': configs.get('snf_schema')\n",
    "}\n",
    "\n",
    "conn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc97fce-6ed9-4ab5-977e-0ddc46d4fa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logger.info(f\"Writing Dataframe to Snowflake.\")\n",
    "conn = sc.connect(**conn_params)\n",
    "\n",
    "table_name ='SRC_GCP_INTER_CALLS_TEMP'    \n",
    "logger.info(f\"Writing data to table: {conn_params['database']}.{table_name}\")\n",
    "insert_new_records(conn, table_name, df_inter_calls_data)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time  # Time taken for this iteration\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "logger.info(f\"SRC_GCP_INTER_CALLS: Inserted records #{len(df_inter_calls_data)} in {int(minutes)} min {seconds:.2f} sec elapsed\")\n",
    "        \n",
    "start_time = time.time()\n",
    "logger.info(f\"Writing data to table: {conn_params['database']}.{table_name}\")\n",
    "table_name ='SRC_GCP_INTRA_CALLS_TEMP'\n",
    "insert_new_records(conn, table_name, df_intra_calls_data)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time  # Time taken for this iteration\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "logger.info(f\"SRC_GCP_INTRA_CALLS: Inserted records #{len(df_intra_calls_data)} in {int(minutes)} min {seconds:.2f} sec elapsed\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee75bc0-131a-43ab-8e29-b9ad64e09f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30c5ecd-5659-4c60-b7f9-e0c0203508de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "Unsuccessful Contact          1037\n",
       "Customer Inquiry               245\n",
       "Billing                        219\n",
       "Production                     119\n",
       "Transfer                        42\n",
       "Damage                          31\n",
       "Customer Requested Removal      18\n",
       "Monitoring Portal               15\n",
       "Performance                     15\n",
       "Pre Board                        7\n",
       "Unavoidable Casualty             7\n",
       "Monitoring                       6\n",
       "Admin                            4\n",
       "Performance Guaranteee           1\n",
       "Energy Efficiency                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inter_calls_data.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75572be-3975-4660-931a-4511719f5563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
