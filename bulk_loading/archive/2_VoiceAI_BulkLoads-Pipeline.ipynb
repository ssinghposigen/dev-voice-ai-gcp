{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "from datetime import datetime, timezone, UTC, timedelta\n",
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "import json, logging\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    ")\n",
    "def list_download_calls_s3_to_gcs(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    ") -> int:\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import logging, json\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import logging as cloud_logging\n",
    "    from datetime import datetime, timedelta, timezone, UTC\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Definitions\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "    def setup_logger(log_file):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "\n",
    "    def generate_gcs_folders(    \n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    ):\n",
    "        try:\n",
    "             # Setup logger\n",
    "            logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "            gcs_folders = {}\n",
    "            gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "            gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "            gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "            gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "            gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "            gcs_folders['gcs_logs_folder'] = f\"{pipeline_run_name}/Logs\"\n",
    "\n",
    "            # Initialize GCS Client\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "            # Create empty folders directly\n",
    "            for folder in gcs_folders.values():\n",
    "                blob = bucket.blob(f\"{folder}/\")\n",
    "                blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "                logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "            logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "            return gcs_folders\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "\n",
    "\n",
    "    def generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(\"Started: generating S3 folder prefix.\")\n",
    "            # Get current date and time\n",
    "            # current_datetime = datetime.now()\n",
    "            current_datetime = datetime.now(timezone.utc) - timedelta(days=5) # ToTest\n",
    "\n",
    "            # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "            if current_datetime.hour == 0:\n",
    "                adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "            else:\n",
    "                adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "            # Extract year, month, and day from the adjusted date\n",
    "            year = str(adjusted_datetime.year)\n",
    "            month = f\"{adjusted_datetime.month:02d}\"\n",
    "            day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "            # Construct the prefix for S3 listing\n",
    "            prefix = f\"{year}/{month}/{day}/\"\n",
    "            logger.info(f\"Completed: generating S3 folder prefix {prefix}.\")\n",
    "\n",
    "            return prefix\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "\n",
    "    def get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "        time_interval\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "            # Initialize S3 Client\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key\n",
    "            )\n",
    "\n",
    "            all_files = []\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "            # Get current UTC time (timezone-aware)\n",
    "            # current_time = datetime.now(timezone.utc)\n",
    "            current_time = datetime.now(timezone.utc) + timedelta(hours=10) # ToTest\n",
    "\n",
    "            # Calculate the time threshold (2 hours before the current time)\n",
    "            time_threshold = current_time - timedelta(hours=time_interval)\n",
    "            logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "            all_files = []\n",
    "\n",
    "            for page in pages:\n",
    "                for obj in page.get('Contents', []):\n",
    "                    file_path = obj['Key']\n",
    "                    s3_ts = obj['LastModified']\n",
    "\n",
    "                    # Extract timestamp from filename\n",
    "                    try:\n",
    "                        # Skip non-JSON files\n",
    "                        if file_path.endswith('.json'):\n",
    "                            call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                            call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                            # Compare only the time part\n",
    "                            if call_timestamp.time() <= time_threshold.time():\n",
    "                                all_files.append({\n",
    "                                    'File': file_path,\n",
    "                                    'Call_ID': call_id,\n",
    "                                    'File_Timestamp': call_timestamp,\n",
    "                                    'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                    'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                    'S3_Timestamp': s3_ts,\n",
    "                                    'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                    'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            if all_files:\n",
    "                df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "                df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "                # Subset the DataFrame for only the most recent 2 hours bin\n",
    "                df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "\n",
    "                # Write the DataFrame to GCS\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "                csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "                df_calls_list.to_csv(csv_path, index=False)\n",
    "                logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "                logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "                return df_calls_list\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"0 Files fetched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    def download_transcripts_to_gcs(\n",
    "        file,\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        s3_client,\n",
    "        s3_analysis_bucket\n",
    "    ):\n",
    "        \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "        local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "        gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "        try:\n",
    "            # Download file from S3\n",
    "            s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "            # Upload to GCS\n",
    "            blob = gcs_bucket.blob(gcs_blob_path)\n",
    "            blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "            return file, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "            handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            return None, file\n",
    "\n",
    "\n",
    "    # ========================================================\n",
    "    # Variables\n",
    "    # ========================================================\n",
    "    log_file = f\"{pipeline_run_name}.logs\"\n",
    "    logger = setup_logger(log_file)\n",
    "\n",
    "    logger.info(\"============================================================================\")\n",
    "    logger.info(\"COMPONENT: Fetch Transcripts from S3 into GCS.\")\n",
    "    logger.info(\"============================================================================\")\n",
    "\n",
    "    # Fetch Configs\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "\n",
    "    time_interval = 2\n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "    aws_access_key = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "    aws_secret_key = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "    s3_analysis_bucket = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "    s3_transcripts_location = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "\n",
    "    # Generate required GCS folder paths\n",
    "    gcs_folders = generate_gcs_folders(pipeline_run_name, vai_gcs_bucket)\n",
    "\n",
    "    gcs_staging_folder = gcs_folders[\"gcs_staging_folder\"]\n",
    "    gcs_transcripts_folder = gcs_folders[\"gcs_transcripts_folder\"]\n",
    "    gcs_errored_folder = gcs_folders[\"gcs_errored_folder\"]\n",
    "    gcs_logs_folder = gcs_folders[\"gcs_logs_folder\"]\n",
    "\n",
    "    # Generate S3 Prefix\n",
    "    s3_prefix = generate_s3_folder_prefix(\n",
    "        pipeline_run_name, vai_gcs_bucket, gcs_errored_folder\n",
    "    )\n",
    "\n",
    "    # ========================================================\n",
    "    # Fetch Calls List from S3\n",
    "    # ========================================================\n",
    "    df_calls_list = get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "        time_interval\n",
    "    )\n",
    "\n",
    "    call_count = len(df_calls_list)\n",
    "\n",
    "    if call_count > 0:\n",
    "        files_list = df_calls_list.File.to_list()\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\", aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        success_downloads = []\n",
    "        failed_downloads = []\n",
    "\n",
    "        # Start Multithreaded Download\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            logger.info(f\"Started: Bulk download to GCS transcripts#: {call_count}\")\n",
    "\n",
    "            future_to_file = {\n",
    "                executor.submit(\n",
    "                    download_transcripts_to_gcs,\n",
    "                    file,\n",
    "                    pipeline_run_name,\n",
    "                    vai_gcs_bucket,\n",
    "                    gcs_staging_folder,\n",
    "                    gcs_errored_folder,\n",
    "                    gcs_transcripts_folder,\n",
    "                    s3_client,\n",
    "                    s3_analysis_bucket\n",
    "                ): file for file in files_list\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_file):\n",
    "                try:\n",
    "                    success, failed = future.result()  # Get results\n",
    "\n",
    "                    if success:\n",
    "                        success_downloads.append(success)\n",
    "                    if failed:\n",
    "                        failed_downloads.append(failed)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "        logger.info(\n",
    "            f\"Completed: Bulk download to GCS transcripts, \"\n",
    "            f\"Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logger.info(\"No Calls to Process.\")\n",
    "\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "    blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "    blob.upload_from_filename(log_file, checksum=None)\n",
    "\n",
    "    return call_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Parallel Process Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @dsl.component(\n",
    "#     base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    "# )\n",
    "# def process_transcripts(\n",
    "#     pipeline_run_name: str,\n",
    "#     project_id: str,\n",
    "#     secret_id: str,\n",
    "#     version_id: str\n",
    "# ):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import threading\n",
    "    import concurrent.futures\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.special import softmax\n",
    "    import logging\n",
    "    import re, os, json, io\n",
    "    from datetime import datetime, timezone, UTC\n",
    "    from typing import List, Dict\n",
    "\n",
    "    from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "    import snowflake.connector as sc\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import dlp_v2\n",
    "    from google.cloud import logging as cloud_logging\n",
    "\n",
    "    import vertexai\n",
    "    import vertexai.preview.generative_models as generative_models\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "    # Sentiments\n",
    "    from transformers import pipeline\n",
    "    from transformers import AutoTokenizer, AutoConfig\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    # Initialize Google Cloud Logging client\n",
    "    # cloud_logging_client = cloud_logging.Client()\n",
    "    # cloud_logging_client.setup_logging()\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Exception hanlding mechanism\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "\n",
    "    def setup_logger(\n",
    "        log_file\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(log_file)\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            # Remove any existing handlers (to prevent duplicate logging)\n",
    "            if logger.hasHandlers():\n",
    "                logger.handlers.clear()\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Function to create thread-specific log files\n",
    "    def setup_thread_logger(\n",
    "        contact_id\n",
    "    ):\n",
    "        \"\"\"Create a separate log file for each transcript.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "        log_filename = f\"{contact_id}_{timestamp}.log\"\n",
    "        log_filepath = os.path.join(temp_log_folder, log_filename)\n",
    "\n",
    "        thread_logger = logging.getLogger(log_filename)\n",
    "        thread_logger.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "        # Remove handlers to prevent duplication\n",
    "        if thread_logger.hasHandlers():\n",
    "            thread_logger.handlers.clear()\n",
    "\n",
    "        file_handler = logging.FileHandler(log_filepath)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        thread_logger.addHandler(file_handler)\n",
    "\n",
    "        return thread_logger, log_filepath\n",
    "\n",
    "\n",
    "    def merge_logs(log_files, master_log_file, master_logger):\n",
    "        \"\"\"Merge all thread logs into the master log file, sorting by timestamp.\"\"\"\n",
    "        if not log_files:\n",
    "            master_logger.warning(\"No log files found to merge.\")\n",
    "            return\n",
    "\n",
    "        # Filter valid log files and handle NoneType values\n",
    "        valid_logs = [\n",
    "            log_file for log_file in log_files\n",
    "            if isinstance(log_file, str) and log_file.endswith(\".log\") and os.path.exists(log_file)\n",
    "        ]\n",
    "\n",
    "        if not valid_logs:\n",
    "            master_logger.warning(\"No valid log files to merge.\")\n",
    "            return\n",
    "\n",
    "        # Sort logs based on timestamps in filenames (assuming format: transcript_YYYYMMDD_HHMMSS.log)\n",
    "        sorted_logs = sorted(valid_logs, key=lambda x: os.path.basename(x).split(\"_\")[-1].replace(\".log\", \"\"))\n",
    "\n",
    "        with open(master_log_file, \"a\") as master_log:  # Open in append mode\n",
    "            for log_file in sorted_logs:\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as thread_log:\n",
    "                        master_log.write(thread_log.read() + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    master_logger.error(f\"Error reading {log_file}: {e}\")\n",
    "\n",
    "        master_logger.info(f\"All thread logs merged into: {master_log_file}\")\n",
    "\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message,\n",
    "        logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {str(e)}\")\n",
    "\n",
    "    def fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        master_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        List all files in a GCS bucket, handling pagination.\n",
    "\n",
    "        :param bucket_name: Name of the GCS bucket\n",
    "        :param prefix: (Optional) Folder path to filter files\n",
    "        :return: List of file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            master_logger.info(f\"Fetching Transcripts from GCS: {gcs_transcripts_folder}\")\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blobs_iterator = bucket.list_blobs(prefix=gcs_transcripts_folder)  # GCS handles pagination internally\n",
    "\n",
    "            transcripts_list = []\n",
    "            for page in blobs_iterator.pages:  # Handling pagination\n",
    "                for blob in page:\n",
    "                    if not blob.name.endswith(\"/\"):\n",
    "                        transcripts_list.append(blob.name)\n",
    "                        # transcripts_list.append(os.path.basename(blob.name))\n",
    "            master_logger.info(f\"Completed: Fetching Transcripts from GCS #: {len(transcripts_list)}\")\n",
    "            return transcripts_list\n",
    "\n",
    "        except Exception as e:            \n",
    "            master_logger.info(f\"Exception in: fetch_transcripts_from_gcs. {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view,\n",
    "        master_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            # Step 3: Connect to Snowflake\n",
    "            catsubcat_conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_catsubcat_databse,\n",
    "                'schema': snf_catsubcat_schema\n",
    "            }\n",
    "\n",
    "            # Connect to Snowflake\n",
    "            conn = sc.connect(**catsubcat_conn_params)\n",
    "\n",
    "            # Fetch data from Snowflake\n",
    "            query = f\"SELECT CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            conn.close()\n",
    "            master_logger.info(\"Completed: Fetching Category, Sub-Category Mapping.\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            master_logger.info(f\"Exception in: fetch_category_mapping_from_snowflake. {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Intra Call \n",
    "    ========================================================\n",
    "    \"\"\"            \n",
    "    def mask_pii_in_captions(\n",
    "        contact_id,\n",
    "        df,\n",
    "        project_id,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Masks PII data in the 'caption' column of a pandas DataFrame using Google Cloud DLP API.\n",
    "\n",
    "        Args:\n",
    "            contact_id: Identifier for logging purposes\n",
    "            df (pandas.DataFrame): DataFrame with a 'caption' column to process\n",
    "            project_id (str): Your Google Cloud project ID\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with masked PII in the 'caption' column\n",
    "        \"\"\"\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Masking PII Data\")\n",
    "\n",
    "            # Create a copy of the DataFrame to avoid modifying the original\n",
    "            masked_df = df.copy()\n",
    "\n",
    "            # Add unique markers to each caption to identify them after processing\n",
    "            masked_df['original_index'] = masked_df.index\n",
    "            masked_df['marked_caption'] = masked_df.index.astype(str) + \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n",
    "\n",
    "            # Concatenate all captions for bulk processing\n",
    "            all_captions = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n",
    "\n",
    "            # Initialize DLP client\n",
    "            dlp_client = dlp_v2.DlpServiceClient()\n",
    "\n",
    "            # Specify the parent resource name\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "\n",
    "            # Custom dictionary detector for PosiGen\n",
    "            posigen_dictionary = {\n",
    "                \"info_type\": {\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"},\n",
    "                \"dictionary\": {\n",
    "                    \"word_list\": {\n",
    "                        \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Configure inspection config with rule set for exclusions\n",
    "            inspect_config = {\n",
    "                \"info_types\": [\n",
    "                    {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                    {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                    {\"name\": \"STREET_ADDRESS\"},\n",
    "                    {\"name\": \"IP_ADDRESS\"},\n",
    "                    {\"name\": \"DATE_OF_BIRTH\"}\n",
    "                ],\n",
    "                \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE,\n",
    "                \"custom_info_types\": [posigen_dictionary],\n",
    "                \"rule_set\": [\n",
    "                    {\n",
    "                        \"info_types\": [{\"name\": \"CUSTOM_DICTIONARY_POSIGEN\"}],\n",
    "                        \"rules\": [\n",
    "                            {\n",
    "                                \"exclusion_rule\": {\n",
    "                                    \"matching_type\": dlp_v2.MatchingType.MATCHING_TYPE_FULL_MATCH,\n",
    "                                    \"dictionary\": {\n",
    "                                        \"word_list\": {\n",
    "                                            \"words\": [\"posigen\", \"Posigen\", \"PosiGen\", \"POSIGEN\"]\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Configure deidentification to use \"[REDACTED]\" instead of asterisks\n",
    "            deidentify_config = {\n",
    "                \"info_type_transformations\": {\n",
    "                    \"transformations\": [\n",
    "                        {\n",
    "                            \"info_types\": [\n",
    "                                {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                                {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                                {\"name\": \"STREET_ADDRESS\"},\n",
    "                                {\"name\": \"IP_ADDRESS\"},\n",
    "                                {\"name\": \"DATE_OF_BIRTH\"}\n",
    "                            ],\n",
    "                            \"primitive_transformation\": {\n",
    "                                \"replace_config\": {\n",
    "                                    \"new_value\": {\"string_value\": \"[REDACTED]\"}\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Create deidentify request\n",
    "            item = {\"value\": all_captions}\n",
    "\n",
    "            # Call the DLP API\n",
    "            try:\n",
    "                response = dlp_client.deidentify_content(\n",
    "                    request={\n",
    "                        \"parent\": parent,\n",
    "                        \"deidentify_config\": deidentify_config,\n",
    "                        \"inspect_config\": inspect_config,\n",
    "                        \"item\": item,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                thread_logger.error(f\"{contact_id}: Error in DLP API call: {e}\")\n",
    "                return df  # Return original DataFrame if masking fails\n",
    "\n",
    "            # Get processed content and split by record boundaries\n",
    "            processed_content = response.item.value\n",
    "            processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\n\")\n",
    "\n",
    "            # Create mapping from original indices to processed captions\n",
    "            processed_dict = {}\n",
    "            for record in processed_records:\n",
    "                parts = record.split(\"|||SEPARATOR|||\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    idx, content = parts\n",
    "                    processed_dict[int(idx)] = content\n",
    "\n",
    "            # Update the DataFrame with redacted content\n",
    "            masked_df['caption'] = masked_df.apply(\n",
    "                lambda row: processed_dict.get(row['original_index'], row['caption']), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Additional processing to mask all digits with asterisks\n",
    "            def mask_digits(text):\n",
    "                \"\"\"Replaces digits with asterisks while preserving '[REDACTED]' markers.\"\"\"\n",
    "                if not isinstance(text, str):\n",
    "                    return text\n",
    "                parts = text.split(\"[REDACTED]\")\n",
    "                for i in range(len(parts)):\n",
    "                    parts[i] = re.sub(r'\\d', '*', parts[i])\n",
    "                return \"[REDACTED]\".join(parts)\n",
    "\n",
    "            # Apply the digit masking function to each processed caption\n",
    "            masked_df['caption'] = masked_df['caption'].apply(mask_digits)\n",
    "\n",
    "            # Drop temporary columns\n",
    "            masked_df.drop(['original_index', 'marked_caption'], axis=1, inplace=True)\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Completed Masking PII Data\")\n",
    "            return masked_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"mask_pii_in_captions() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def get_sentiment_label(row):\n",
    "        try:\n",
    "            # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "            if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "                return 'Positive'\n",
    "            elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "                return 'Negative'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_label() failed: {str(e)}\")\n",
    "\n",
    "    def get_different_times(\n",
    "        intra_call,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            # Apply formatting to both time columns\n",
    "            intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "            intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "            intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "            intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "            intra_call['load_date'] = datetime.now()\n",
    "\n",
    "            # Dropping time formatted columns\n",
    "            intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "            return intra_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_different_times() failed: {str(e)}\")\n",
    "\n",
    "    def get_sentiment_scores(\n",
    "        contact_id,\n",
    "        text_list,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Calculating Caption Sentiments.\")\n",
    "            dict_sentiments = []\n",
    "            for text in text_list:\n",
    "                encoded_input = tokenizer(text, return_tensors='pt')\n",
    "                output = model_sentiment(**encoded_input)\n",
    "                scores = output[0][0].detach().numpy()\n",
    "                scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "                merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "                dict_sentiments.append(merged_dict)\n",
    "\n",
    "            df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "            df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "            thread_logger.info(f\"{contact_id}: Completed calculating Caption Sentiments.\")\n",
    "\n",
    "            return df_dict_sentiments\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_scores() failed: {str(e)}\")\n",
    "\n",
    "    def process_transcript(\n",
    "        contact_id,\n",
    "        transcript_data,\n",
    "        tokenizer,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Pre-process the transcript loaded from S3 Buckets:\n",
    "        1. Load the transcript as Pandas Dataframe.\n",
    "        2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "        3. Format the time in minutes and seconds.\n",
    "        4. Rename the columns for better understanding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Loading the Transcript as Pandas Dataframe.\")\n",
    "            transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "            # Select the relevant Columns\n",
    "            columns_to_select = [\n",
    "                'BeginOffsetMillis',\n",
    "                'EndOffsetMillis',\n",
    "                'ParticipantId',\n",
    "                'Content'\n",
    "            ]\n",
    "            formatted_df = transcript_df[columns_to_select].copy()\n",
    "\n",
    "            # Optionally rename columns to reflect their new format\n",
    "            formatted_df = formatted_df.rename(columns={\n",
    "                'BeginOffsetMillis': 'Begin_Offset',\n",
    "                'EndOffsetMillis': 'End_Offset',\n",
    "                'Content': 'caption',\n",
    "                'Sentiment': 'sentiment_label',\n",
    "                'ParticipantId': 'speaker_tag'\n",
    "            })\n",
    "\n",
    "            # Inserting the Call ID:\n",
    "            formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "            formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Returning formated DataFrame.\")\n",
    "            return formatted_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"process_transcript() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def create_intra_call_df(\n",
    "        contact_id,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        tokenizer,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "            intra_call = process_transcript(\n",
    "                contact_id,\n",
    "                transcript_data,\n",
    "                tokenizer,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            df_sentiment_scores = get_sentiment_scores(\n",
    "                contact_id,\n",
    "                intra_call.caption.to_list(),\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "            intra_call = get_different_times(\n",
    "                intra_call,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            intra_call = mask_pii_in_captions(\n",
    "                contact_id,\n",
    "                intra_call,\n",
    "                gcp_project_id,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "\n",
    "            return intra_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_intra_call_df() failed: {str(e)}\")\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Inter Call \n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def dict_to_newline_string(data):\n",
    "        \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "        try:\n",
    "            formatted_str = \"\"\n",
    "            for key, value in data.items():\n",
    "                formatted_str += f\"{key}:\\n\"\n",
    "                for item in value:\n",
    "                    formatted_str += f\"  - {item}\\n\"\n",
    "            return formatted_str.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"dict_to_newline_string() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    class CategoryValidator:\n",
    "        def __init__(self, df_cat_subcat_mapping):\n",
    "            \"\"\"\n",
    "            Initialize with category mapping from a Pandas DataFrame.\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame containing 'CATEGORY' and 'SUBCATEGORY' columns.\n",
    "            \"\"\"\n",
    "            self.df_cat_subcat_mapping = df_cat_subcat_mapping  # Ensure only the correct DataFrame is used\n",
    "            self.valid_categories = set(df_cat_subcat_mapping['CATEGORY'].dropna().unique())\n",
    "            self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "        def _create_category_mapping(self):\n",
    "            \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "            try:\n",
    "                mapping = {}\n",
    "                for _, row in self.df_cat_subcat_mapping.dropna().iterrows():\n",
    "                    category = row['CATEGORY']\n",
    "                    subcategory = row['SUBCATEGORY']\n",
    "\n",
    "                    if category not in mapping:\n",
    "                        mapping[category] = set()\n",
    "\n",
    "                    if subcategory:  # Only add non-null subcategories\n",
    "                        mapping[category].add(subcategory)\n",
    "\n",
    "                return mapping\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"_create_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "        def validate_category(self, category: str) -> bool:\n",
    "            \"\"\"Check if category is valid.\"\"\"\n",
    "            try:\n",
    "                return category in self.valid_categories\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category() failed: {str(e)}\")\n",
    "\n",
    "        def validate_subcategory(self, category: str, subcategory: str) -> bool:\n",
    "            \"\"\"Check if subcategory is valid for the given category.\"\"\"\n",
    "            try:\n",
    "                return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_subcategory() failed: {str(e)}\")\n",
    "\n",
    "        def get_valid_subcategories(self, category: str) -> set:\n",
    "            \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "            try:\n",
    "                return self.category_subcategory_map.get(category, set())\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_valid_subcategories() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    class CallSummary(BaseModel):\n",
    "        summary: str = Field(..., max_length=500)\n",
    "\n",
    "    class CallTopic(BaseModel):\n",
    "        primary_topic: str = Field(..., max_length=100)\n",
    "        category: str = Field(..., max_length=100)\n",
    "        sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "        def validate_category_mapping(\n",
    "            self,\n",
    "            category_validator: CategoryValidator,\n",
    "            thread_logger\n",
    "        ):\n",
    "            \"\"\"Validate category and subcategory against mapping. Replace with 'Unspecified' if invalid.\"\"\"\n",
    "            try:\n",
    "                if not category_validator.validate_category(self.category):\n",
    "                    thread_logger.warning(f\"Invalid category: {self.category}. Replacing with 'Unspecified'.\")\n",
    "                    self.category = \"Unspecified\"\n",
    "                    self.sub_category = \"Unspecified\"\n",
    "                elif not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "                    thread_logger.warning(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'. Replacing subcategory with 'Unspecified'.\")\n",
    "                    self.sub_category = \"Unspecified\"\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "    class AgentCoaching(BaseModel):\n",
    "        strengths: List[str] = Field(..., max_items=3)\n",
    "        improvement_areas: List[str] = Field(..., max_items=3)\n",
    "        specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "        skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "    class TranscriptAnalysis(BaseModel):\n",
    "        call_summary: CallSummary\n",
    "        call_topic: CallTopic\n",
    "        agent_coaching: AgentCoaching\n",
    "\n",
    "    class KPIExtractor:\n",
    "        def __init__(\n",
    "            self,\n",
    "            project_id: str,\n",
    "            location: str,\n",
    "            df_cat_subcat_mapping,\n",
    "            thread_logger\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize the KPIExtractor with Vertex AI model and category validator.\n",
    "            :param project_id: GCP Project ID\n",
    "            :param location: GCP Region\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame with 'CATEGORY' and 'SUBCATEGORY'\n",
    "            \"\"\"\n",
    "            vertexai.init(project=project_id, location=location)\n",
    "            self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "            self.category_validator = CategoryValidator(df_cat_subcat_mapping)\n",
    "\n",
    "            self.generation_config = {\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 1024,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "\n",
    "            self.safety_settings = {\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            }\n",
    "\n",
    "\n",
    "        def get_categories_prompt(self) -> str:\n",
    "            \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "            try:\n",
    "                categories_prompt = []\n",
    "\n",
    "                for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "                    if category is None:  # Skip if category is None\n",
    "                        continue\n",
    "\n",
    "                    # Ensure subcategories are valid (remove None values)\n",
    "                    valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "\n",
    "                    if valid_subcategories:\n",
    "                        subcats = ', '.join(sorted(valid_subcategories))\n",
    "                    else:\n",
    "                        subcats = \"No defined subcategories\"\n",
    "\n",
    "                    categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "\n",
    "                return '\\n'.join(categories_prompt)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_categories_prompt() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "        def create_prompt(self, transcript):\n",
    "            \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "            categories_guidance = self.get_categories_prompt()\n",
    "\n",
    "            return f\"\"\"\n",
    "            Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "            Keep responses concise, specific, and actionable.\n",
    "\n",
    "            Guidelines:\n",
    "            - Call summary should be factual and highlight key interactions\n",
    "            - Topics and categories MUST match the following valid mappings:\n",
    "            {categories_guidance}\n",
    "            - Coaching points should be specific and actionable\n",
    "            - All responses must follow the exact structure specified\n",
    "            - Ensure all lists have the specified maximum number of items\n",
    "            - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "            Transcript:\n",
    "            {transcript}\n",
    "\n",
    "            Required Output Structure:\n",
    "            {{\n",
    "                \"call_summary\": {{\n",
    "                    \"summary\": \"3-4 line overview of the call\"\n",
    "                }},\n",
    "                \"call_topic\": {{\n",
    "                    \"primary_topic\": \"Main topic of discussion\",\n",
    "                    \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                    \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "                }},\n",
    "                \"agent_coaching\": {{\n",
    "                    \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                    \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                    \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                    \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            1. Maintain exact JSON structure\n",
    "            2. No additional fields or comments\n",
    "            3. No markdown formatting\n",
    "            4. Ensure all arrays have the exact number of items specified\n",
    "            5. Keep all text concise and professional\n",
    "            6. Do not mention any PII information such as Customer Name etc.\n",
    "            7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "            \"\"\"\n",
    "\n",
    "        def extract_json(self, response):\n",
    "            \"\"\"Extract valid JSON from response\"\"\"\n",
    "            try:\n",
    "                match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "                if match:\n",
    "                    json_str = match.group(1)\n",
    "                else:\n",
    "                    json_str = response.strip()\n",
    "                return json.loads(json_str)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"extract_json() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "        def validate_response(\n",
    "            self,\n",
    "            response_json,\n",
    "            thread_logger,\n",
    "            contact_id = None        \n",
    "        ):\n",
    "            \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "            try:\n",
    "                # First validate basic structure with Pydantic\n",
    "                analysis = TranscriptAnalysis(**response_json)\n",
    "\n",
    "                # Then validate category mapping\n",
    "                analysis.call_topic.validate_category_mapping(self.category_validator, thread_logger)\n",
    "\n",
    "                return analysis\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_response() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "        def extract_genai_kpis(self, transcript, contact_id = None):\n",
    "            \"\"\"Extract KPIs from transcript with validation\"\"\"\n",
    "            try:\n",
    "                # Generate prompt\n",
    "                prompt = self.create_prompt(transcript)\n",
    "\n",
    "                # Get response from Gemini\n",
    "                response = self.model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=self.generation_config,\n",
    "                    safety_settings=self.safety_settings\n",
    "                )\n",
    "\n",
    "                # Parse JSON response\n",
    "                response_json = self.extract_json(response.text)\n",
    "\n",
    "                # Validate response structure and categories\n",
    "                validated_response = self.validate_response(response_json, contact_id)\n",
    "\n",
    "                return validated_response.model_dump()\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"extract_genai_kpis() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe Inter Call\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def create_inter_call_df(\n",
    "        contact_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        ac_last_modified_date,\n",
    "        df_intra_call,\n",
    "        gcp_project_id,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "            thread_logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "            extractor = KPIExtractor(\n",
    "                gcp_project_id,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping,\n",
    "                thread_logger\n",
    "            )\n",
    "            transcript = \" \".join(df_intra_call.caption)\n",
    "            call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "            thread_logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "\n",
    "            inter_call_dict = {}\n",
    "            inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "            inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "            inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "            inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "            inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "            inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "            inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "            df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "            # Replace values where Categories are not in allowed list\n",
    "            allowed_categories = df_cat_subcat_mapping['CATEGORY'].drop_duplicates().to_list()\n",
    "            df_inter_call.loc[\n",
    "                ~df_inter_call['category'].isin(allowed_categories) | df_inter_call['category'].isna(),\n",
    "                ['category', 'sub_category']\n",
    "            ] = 'Unspecified'\n",
    "\n",
    "            df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "            df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "            df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "            df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "            # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "            # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "            df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "            df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "            df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "            thread_logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "\n",
    "            return df_inter_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_inter_call_df() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Process Single Transcript\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def process_single_transcript(\n",
    "        pipeline_run_name,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_logs_folder,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        transcript_path,\n",
    "        tokenizer,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping\n",
    "    ):\n",
    "        contact_id = transcript_path.split('/')[-1].split('analysis')[0].strip('_')\n",
    "        ac_last_modified_date = datetime.strptime(\n",
    "                transcript_path.split('/')[-1].split('analysis_')[-1].split('.')[0].replace('_', ':'),\n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            )\n",
    "\n",
    "        thread_logger, log_filepath = setup_thread_logger(contact_id)\n",
    "\n",
    "        try:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(transcript_path)\n",
    "            transcript_data = json.loads(blob.download_as_text())\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: started processing\")\n",
    "\n",
    "            df_intra_call = create_intra_call_df(\n",
    "                contact_id,\n",
    "                gcp_project_id,\n",
    "                vai_gcs_bucket,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                tokenizer,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            df_inter_call = create_inter_call_df(\n",
    "                contact_id,\n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                ac_last_modified_date,\n",
    "                df_intra_call,\n",
    "                gcp_project_id,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            if not df_intra_call.empty and not df_inter_call.empty:\n",
    "                csv_path_df_intra_call = f\"gs://{vai_gcs_bucket}/{gcs_intra_call_dfs_folder}/{contact_id}_df_intra_call.csv\"\n",
    "                df_intra_call.to_csv(csv_path_df_intra_call, index=False)\n",
    "                thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_intra_call.csv\")\n",
    "\n",
    "                csv_path_df_inter_call = f\"gs://{vai_gcs_bucket}/{gcs_inter_call_dfs_folder}/{contact_id}_df_inter_call.csv\"\n",
    "                df_inter_call.to_csv(csv_path_df_inter_call, index=False)\n",
    "                thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_inter_call.csv\")\n",
    "\n",
    "                thread_logger.info(f\"{contact_id}: Processing Complete\")\n",
    "                thread_logger.info(\"\")\n",
    "                thread_logger.info(\"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e), thread_logger)\n",
    "            return None # Continue processing other files\n",
    "\n",
    "        return log_filepath\n",
    "\n",
    "    def merge_and_save_transcripts(\n",
    "        bucket_name,\n",
    "        input_folder,\n",
    "        output_folder,\n",
    "        output_file,\n",
    "        master_logger\n",
    "    ):\n",
    "        try:\n",
    "            \"\"\"Reads, merges all files in a GCS folder, and saves the master DataFrame as CSV.\"\"\"\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(bucket_name)\n",
    "\n",
    "            dfs = [\n",
    "                pd.read_parquet(bucket.blob(blob.name).open(\"rb\")) if blob.name.endswith(\".parquet\") \n",
    "                else pd.read_csv(bucket.blob(blob.name).open(\"r\")) \n",
    "                for blob in bucket.list_blobs(prefix=input_folder) \n",
    "                if blob.name.endswith(('.csv', '.parquet'))\n",
    "            ]\n",
    "\n",
    "            if dfs:\n",
    "                master_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Convert DataFrame to CSV in-memory\n",
    "                csv_buffer = io.StringIO()\n",
    "                master_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "                # Upload CSV to GCS\n",
    "                bucket.blob(f\"{output_folder}/{output_file}\").upload_from_string(\n",
    "                    csv_buffer.getvalue(), content_type=\"text/csv\"\n",
    "                )\n",
    "                master_logger.info(f\"Completed: merging and writing {output_file} to {output_folder}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            master_logger.error(f\"Error processing {input_folder}: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Variables\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "    \n",
    "    # GCP Configuration\n",
    "    gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "    gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "    # Pipeline Configuration\n",
    "    gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "    gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "    gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "    gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "    gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "    gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "    # Snowflake Configuration\n",
    "    snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "    snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "    snf_private_key = configs.get(\"private_key\")\n",
    "    snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "    snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "    snf_catsubcat_databse = configs.get(\"VAI_SNF_CATSUBCAT_DATABASE\")\n",
    "    snf_catsubcat_schema = configs.get(\"VAI_SNF_CATSUBCAT_SCHEMA\")\n",
    "    snf_catsubcat_view = configs.get(\"VAI_SNF_CATSUBCAT_VIEW\")\n",
    "\n",
    "    # Max parallelism for multi-threading\n",
    "    max_parallelism = 10\n",
    "\n",
    "    # Step 2: Download Master Log File from GCS\n",
    "    master_log_file = f\"{pipeline_run_name}.logs\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(vai_gcs_bucket)\n",
    "    blob = bucket.blob(f\"{gcs_logs_folder}/{master_log_file}\")\n",
    "    # Download master log file\n",
    "    blob.download_to_filename(master_log_file)\n",
    "\n",
    "    master_logger = setup_logger(master_log_file)\n",
    "    master_logger.info(\"\")\n",
    "    master_logger.info(\"\")\n",
    "    master_logger.info(\"============================================================================\")\n",
    "    master_logger.info(\"COMPONENT: Process Transcripts.\")\n",
    "    master_logger.info(\"============================================================================\")\n",
    "    master_logger.info(\"Fetched Master Log File from GCS bucket.\")\n",
    "\n",
    "    temp_log_folder = \"temp_logs\"\n",
    "    os.makedirs(temp_log_folder, exist_ok=True)\n",
    "\n",
    "    df_cat_subcat_mapping = fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view,\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    transcripts_list = fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    master_logger.info(\"Starting the Multi-threading.\")\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    threads_log_files = []  # Store generated log files\n",
    "\n",
    "    # Multi-threaded execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallelism) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_transcript,\n",
    "                pipeline_run_name,\n",
    "                gcp_project_id, \n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                gcs_errored_folder,\n",
    "                gcs_logs_folder,\n",
    "                gcs_intra_call_dfs_folder,\n",
    "                gcs_inter_call_dfs_folder,\n",
    "                transcript_path,\n",
    "                tokenizer,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping\n",
    "            ) for transcript_path in transcripts_list[:2]     # ToTest\n",
    "        ]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            threads_log_files.append(future.result())\n",
    "\n",
    "    # Merge all threaded transcripts\n",
    "    threads_log_files = [file for file in threads_log_files if isinstance(file, str) and file.endswith(\".log\")]\n",
    "    merge_logs(\n",
    "        threads_log_files,\n",
    "        master_log_file,\n",
    "        master_logger\n",
    "    )\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    master_logger.info(\"Completed the Multi-threading.\")\n",
    "    master_logger.info(\"===================================================================\")\n",
    "\n",
    "    master_logger = setup_logger(master_log_file)\n",
    "\n",
    "    # Step 3: Merge all outputs into master files after processing\n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_intra_call_df.csv\",\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_inter_call_df.csv\",\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    # Upload the master log file back into GCS Bucket\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "    blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{master_log_file}\")\n",
    "    blob.upload_from_filename(master_log_file, checksum=None)\n",
    "    master_logger.info(\"Uploaded Master Log File back to GCS bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d365f-c880-4ace-970d-a5618d503ad6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Write data to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92da9211-4634-4f47-9226-0bf6e6d7a165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-voiceai/dev-voice-ai-docker-image:dev-4\"\n",
    ")\n",
    "def write_data_to_snowflake(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    import io, logging, json\n",
    "    import pytz\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    import snowflake.connector as sc\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "    def setup_logger(\n",
    "        log_file\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(log_file)\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            # Remove any existing handlers (to prevent duplicate logging)\n",
    "            if logger.hasHandlers():\n",
    "                logger.handlers.clear()\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message,\n",
    "        logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "\n",
    "    def insert_new_records(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_databse,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        df\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "\n",
    "        Steps:\n",
    "        1. Fetches existing IDs from table.\n",
    "        2. Filters out rows with existing IDs from DataFrame.\n",
    "        3. Adds 'LOAD_DATE_UTC' column with current UTC timestamp.\n",
    "        4. Inserts only new records.\n",
    "\n",
    "        Args:\n",
    "            conn: Snowflake connection object.\n",
    "            table_name (str): Name of the target table.\n",
    "            df (pd.DataFrame): DataFrame containing the data (must have 'CONTACT_ID' column).\n",
    "\n",
    "        Returns:\n",
    "            int: Number of inserted records.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_databse,\n",
    "                'schema': snf_schema\n",
    "            }\n",
    "\n",
    "            conn = sc.connect(**conn_params)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Step 1: Get existing IDs from Snowflake table for the last two days (current and previous day)\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT DISTINCT(CONTACT_ID) \n",
    "                FROM {table_name} \n",
    "                WHERE LOAD_DATE >= DATEADD(DAY, -1, CURRENT_DATE)\n",
    "            \"\"\")\n",
    "            existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "            # Step 2: Filter DataFrame to keep only new records\n",
    "            new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "\n",
    "            if new_records_df.empty:\n",
    "                logger.info(\"No new records to insert\")\n",
    "                return 0\n",
    "\n",
    "            # Step 3: Add UTC timestamp column\n",
    "            utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_records_df = new_records_df.copy()  # Avoid modifying original df\n",
    "            new_records_df[\"LOAD_DATE\"] = utc_now  # Add new column\n",
    "\n",
    "            # Step 4: Insert new records into Snowflake\n",
    "            success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "\n",
    "            logger.info(f\"Inserted {nrows} new records with UTC load date\")\n",
    "            logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return nrows\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e), logger)\n",
    "\n",
    "    # Function to read CSV from GCS\n",
    "    def read_gcs_csv(file_path):\n",
    "        blob = bucket.blob(file_path)\n",
    "        csv_data = blob.download_as_text()\n",
    "        return pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "    try:\n",
    "        configs = fetch_secrets(\n",
    "            project_id,\n",
    "            secret_id,\n",
    "            version_id\n",
    "        )\n",
    "\n",
    "        # GCP Configuration\n",
    "        gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "        gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "        vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "        # Pipeline Configuration\n",
    "        gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "        gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "        gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "        gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "        gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "        # Snowflake Configuration\n",
    "        snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "        snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "        snf_private_key = configs.get(\"private_key\")\n",
    "        snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "        snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "        snf_database = configs.get(\"VAI_SNF_DATABASE\")\n",
    "        snf_schema = configs.get(\"VAI_SNF_SCHEMA\")\n",
    "\n",
    "        # Step 2: Download Master Log File from GCS\n",
    "        log_file = f\"{pipeline_run_name}.logs\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "        # Download master log file\n",
    "        blob.download_to_filename(log_file)\n",
    "\n",
    "        logger = setup_logger(log_file)\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"============================================================================\")\n",
    "        logger.info(\"COMPONENT: Write Data to Snowflake.\")\n",
    "        logger.info(\"============================================================================\")\n",
    "        logger.info(\"Fetched Master Log File from GCS bucket.\")\n",
    "\n",
    "        # Read Inter & Intra Call DataFrames\n",
    "        inter_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_inter_call_df.csv\")\n",
    "        inter_call_df.columns = inter_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "        intra_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_intra_call_df.csv\")\n",
    "        intra_call_df.columns = intra_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "\n",
    "        logger.info(f\"Started: writing data to snowflake.\")\n",
    "        table_name ='SRC_GCP_INTER_CALLS'    \n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            inter_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTER_CALLS: Inserted records #{len(inter_call_df)}\")\n",
    "\n",
    "\n",
    "        logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "        table_name ='SRC_GCP_INTRA_CALLS'\n",
    "        insert_new_records(\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            intra_call_df\n",
    "        )\n",
    "        logger.info(f\"SRC_GCP_INTRA_CALLS: Inserted records #{len(intra_call_df)}\")\n",
    "        logger.info(f\"Completed: writing data to snowflake.\")\n",
    "\n",
    "    except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e), logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"VAI Audio to KPI Pipeline\",\n",
    "    description=\"Process Amazon Audio Transcripts into KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline to:\n",
    "    1. List calls from S3 and download them to GCS.\n",
    "    2. Process each transcript in parallel using Kubeflow Pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: List and Download Calls from S3 to GCS\n",
    "    get_calls_to_process = list_download_calls_s3_to_gcs(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        project_id=project_id,\n",
    "        secret_id=secret_id,\n",
    "        version_id=version_id\n",
    "    )\n",
    "\n",
    "    # Step 2 and 3 should run **only if** there are calls to process\n",
    "    with dsl.If(get_calls_to_process.output > 0):\n",
    "        # Step 2: Process Transcripts\n",
    "        process_calls = process_transcripts(\n",
    "            pipeline_run_name=pipeline_run_name,\n",
    "            project_id=project_id,\n",
    "            secret_id=secret_id,\n",
    "            version_id=version_id\n",
    "        )\n",
    "\n",
    "        # Step 3: Write Processed Data to Snowflake\n",
    "        persist_to_snowflake = write_data_to_snowflake(\n",
    "            pipeline_run_name=pipeline_run_name,\n",
    "            project_id=project_id,\n",
    "            secret_id=secret_id,\n",
    "            version_id=version_id\n",
    "        )\n",
    "\n",
    "        # Ensure Step 3 runs **after** Step 2\n",
    "        persist_to_snowflake.after(process_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, 'cx-voiceai-process-calls.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38487e72-e52a-43c7-917f-0d305ad627a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8625e81-29a4-44f6-90bd-c2889745f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = timestamp = datetime.now(UTC).strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "project_id = \"dev-posigen\"\n",
    "project_location = \"us-central1\"\n",
    "secret_id = \"dev-cx-voiceai\"\n",
    "version_id= \"2\"\n",
    "# pipeline_run_name = \"cx-voiceai-process-calls-2025-04-01-08-28-20\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"dev-posigen\", location=\"us-central1\")\n",
    "\n",
    "# Create pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    job_id = f\"vai-pipeline-run-{TIMESTAMP}\".lower(),\n",
    "    template_path = f\"cx-voiceai-process-calls.yaml\",\n",
    "    pipeline_root = f\"gs://dev-aws-connect-audio\",\n",
    "    project = project_id,\n",
    "    location = project_location,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        \"pipeline_run_name\": f\"cx-voiceai-process-calls-{TIMESTAMP}\",\n",
    "        \"project_id\": project_id,\n",
    "        \"secret_id\": secret_id,\n",
    "        \"version_id\": version_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ac4-60c0-4607-aabf-23f5f31f9ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e56a67-28e5-4d7f-82a2-263e6d06e901",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b3a463-dbc3-465c-a7c3-9459f26b2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vai-pipeline-run-2025-04-03-11-21-51?project=275963620760\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51 current state:\n",
      "3\n",
      "PipelineJob projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/275963620760/locations/us-central1/pipelineJobs/vai-pipeline-run-2025-04-03-11-21-51\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd766c-63d0-470c-8817-3c05404b747e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scheduling Pseudo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a74563-bd3f-4fe6-b9eb-8eb6876a211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "\n",
    "# def run_pipeline():\n",
    "#     job = aiplatform.PipelineJob(\n",
    "#         display_name=\"my_scheduled_pipeline\",\n",
    "#         template_path=\"gs://your-gcs-bucket/pipelines/pipeline.json\",\n",
    "#         pipeline_root=\"gs://your-gcs-bucket/pipelines/\",\n",
    "#         parameter_values={\"param1\": \"value1\"}  # Modify as needed\n",
    "#     )\n",
    "#     job.run()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a81834e-6460-43c4-9798-ff35a1e61d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud functions deploy trigger_vertex_pipeline \\\n",
    "#     --runtime python39 \\\n",
    "#     --trigger-http \\\n",
    "#     --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a618c29-b357-475a-9da8-9a8f2d9dabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud scheduler jobs create http pipeline-schedule-job \\\n",
    "#     --schedule=\"0 8 * * *\" \\\n",
    "#     --uri=\"https://us-central1-your-project.cloudfunctions.net/trigger_vertex_pipeline\" \\\n",
    "#     --http-method=POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce403f4b-1c6e-4d70-a9c3-4b55e99a88ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
