{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79809ff5-73ff-40ca-a54e-203cecd12fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import logging, json\n",
    "from google.cloud import secretmanager\n",
    "from google.cloud import storage\n",
    "from google.cloud import logging as cloud_logging\n",
    "from datetime import datetime, timedelta, timezone, UTC\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf51d2ae-5531-49b6-bc4c-283c6fcebbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id='dev-posigen'\n",
    "secret_id='dev-cx-voiceai'\n",
    "version_id='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9da3a2-6a4f-4571-a166-6e7719e47f45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function: Fetch Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a842d56a-8a16-490f-af78-793786b0e26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "\n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83cfb882-ec8c-427e-a9a2-eec6b076c940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/275963620760/secrets/dev-cx-voiceai/versions/2\"\n",
       "payload {\n",
       "  data: \"{\\\"VAI_GCP_PROJECT_ID\\\": \\\"dev-posigen\\\", \\\"GCP_PROJECT_LOCATION\\\": \\\"us-central1\\\", \\\"VAI_GCP_PIPELINE_BUCKET\\\": \\\"dev-aws-connect-audio\\\", \\\"VAI_GCP_PIPELINE_FOLDER_STAGGING\\\": \\\"Stagging\\\", \\\"VAI_GCP_PIPELINE_FOLDER_ERRORED\\\": \\\"Errored\\\", \\\"VAI_GCP_PIPELINE_NAME\\\": \\\"cx-voiceai-process-calls\\\", \\\"VAI_AWS_ACCESS_KEY\\\": \\\"AKIA5XPRFL2ZMKPR6QJ3\\\", \\\"VAI_AWS_SECRET_KEY\\\": \\\"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\\\", \\\"VAI_S3_ANALYSIS_BUCKET\\\": \\\"amazon-connect-39f6aa5d9242\\\", \\\"VAI_S3_TRANSCRIPTS_LOCATION\\\": \\\"Analysis/Voice\\\", \\\"VAI_SNF_ACCOUNT\\\": \\\"XV37144.us-central1.gcp\\\", \\\"VAI_SNF_USER\\\": \\\"GCP_INTEGRATION\\\", \\\"VAI_SNF_PRIVATE_KEY_FILE\\\": \\\"gs://dev-aws-connect-audio/1_secrets/snowflakegcp_rsa_key.p8\\\", \\\"VAI_SNF_PRIVATE_KEY_PWD\\\": \\\"$07@rF0r@77!\\\", \\\"VAI_SNF_WAREHOUSE\\\": \\\"DATAPLATR\\\", \\\"VAI_SNF_DATABASE\\\": \\\"POSIGEN_DEV\\\", \\\"VAI_SNF_SCHEMA\\\": \\\"SRC_GCP\\\", \\\"VAI_SNF_CATSUBCAT_DATABASE\\\": \\\"POSIGEN\\\", \\\"VAI_SNF_CATSUBCAT_SCHEMA\\\": \\\"SIGMA_CX\\\", \\\"VAI_SNF_CATSUBCAT_VIEW\\\": \\\"V_CX_CALL_CATEGORIES\\\", \\\"private_key\\\": \\\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\\\nMIIFJDBWBgkqhkiG9w0BBQ0wSTAxBgkqhkiG9w0BBQwwJAQQyKawzeU1Da8fXG8b\\\\nyiZjxgICCAAwDAYIKoZIhvcNAgkFADAUBggqhkiG9w0DBwQImBQ2VaO19yUEggTI\\\\nNfGOnw9rIKUlN+1/dfd3sr7egWksI5PhZLQWpX1W9jBj78DkPusIOJfYF4uu+PkX\\\\nv36hxsSlQbZFCzFh852R/mTKHbfQSu3fjSESGxWzsCbJNvC+NecL99xDTz+pvzlf\\\\nE5i+8U/hwo/G3QkaZ+M58I/Ihz7hJ1xsRtN50MR5zVXgtqNOFBG6nnVPlHMulGhZ\\\\nshxcZ9mF2FEDSeKy/SoFPMwRREo64P4qq4AtVRWfJGxoyf0IjTkq7JszUB7ZZghG\\\\nJxVFBF2BtTu7a2d4nyOzgjOOvVckiqWr+iPRepSwetOnVlkdb3xkJu2j2daOpP3i\\\\nsIruSrO1oLZUvA683oF+C3Njud6L5UVjoJ6D+uUpFaCNjL4HfoS00WXwLmgBwTYR\\\\nJjv1YmANqPy10E8IdfMfI4ktPJjkv63iyfQqPjfumWR20nmUcDOFmjfSXthMjOkS\\\\nkO5g78ldhxlVwcIX8qmuWtBrCeiGEJyc/3+MUdZUO/iduBha5DUfAe50qA26dWOc\\\\n5h0bzZCScJGoX0CTFiFzrdzYlOakc4o4w1Hr7Qsad4hh0W2O6JcuHbGNd/o/z4a6\\\\nCjN1hXYMaoTkoSHfQQzvki3MPL+hHmZe2u6lvs+twF41ZS3xUyasjoMltFbLHqtN\\\\nB8+CMfrPM2+O+H21j+0sIxpflEVgIlTzRmaF8fNm94MrTOEFagKQe8ng5+QQgVzi\\\\nUO3jm7A/r+QFFWxc7cgbeKDlHk40L7SV6i2/yu/NaGYYpMe2WmU9N83y883qJHi+\\\\nXw1FIV2jGpLxBvGVrpt7pIUilbzL59c1hZRhC3elzUA2MM/vmVS64lVM2jBa8H43\\\\nK/VHip66kOiqd6MfiWr8DTUUTMJCR3qeK9s+QYkE1cHBIN9wsU7fF/eUAFjv3rBF\\\\noSNyHe1GTy1A7pp3kOGda/Dek7FVskOTdHOu6YyA155MCvjEuBtDhTu9vhOgH9e9\\\\nOn8faWzcTNDPNfsQ94nZ0Ua9Q1f6v2hKykp+7GplKXGv85sf/pdbL/0xfasWbD9Q\\\\njMA5FEz5oK5JCIeXLXNK4Qyn8TsTlIOlkN2wbZI85pZQAA0kSJmGJP/5qtOnDDHI\\\\nRzz7AIISrximnDIMoBED5Rp8kPFXLtZiZVpZrF8BbOroA9RJU8ehb+Y605BxMDZb\\\\nbtpcrjwWKBZqXSS1eIFK/8QDzVyA0ole6XoaabmIrxuG57THRkHqyn6nWeRBun6z\\\\nA5LXcCrf1tRzuCXEKgYhDYTXBXPI29et7gZaKS1xOGYEutye0e1F0/UxjLfNkqqx\\\\nWexqRh6/mP8VRFlou1GwM8aG5LNgr025B1SLe1hEKpxv0ruxij9jZbAYq8w8MaL2\\\\ngUIVSQ/qB2fvLifk5InBKwXdbAxWZl9sLlfTobJ9HGrzHdLGIMAz+B6zdish38Sl\\\\n6HKH2wRmuYCciE1wY83OjUWPxxNoUrIAu5huQO5pggaC2rIvjBpNNbxqKYWhitt2\\\\nT7ZbTOXAcol/Nw2nzBwjInRaBXgwS+9Cs2JTrxlUqTUotKH6W9hi2/E0/b74VCAD\\\\nvmFMRy1WrcptDde2o6/iWBnbJEmzdhRAXMSNKGKIBaIHc5YgpFsladsCNvN3WDcB\\\\nasmKyHcK0ypjQdS78i/5BC+exu8Ln+kp\\\\n-----END ENCRYPTED PRIVATE KEY-----\\\\n\\\"}\"\n",
       "  data_crc32c: 799052364\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f78995b4-377c-4730-8a3b-08a320653cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8aada0-66fc-4187-92a3-5c140e66f110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64737b-08c4-4e0e-b28a-cd7d95f511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_file):\n",
    "    \"\"\"\n",
    "    Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Path of the log file.\n",
    "\n",
    "    Returns:\n",
    "        logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "        if not logger.handlers:  # Avoid adding multiple handlers\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "\n",
    "            # File Handler\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            # Console Handler\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize logger: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8757f6-41b0-43a5-bb53-f2f5ac151402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Handle Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee7064-a450-41a0-b018-a1077f0bd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exception(\n",
    "    file_id,\n",
    "    vai_gcs_bucket,\n",
    "    run_folder,\n",
    "    error_folder,\n",
    "    error_message\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d606d6-01ba-4464-9a26-6ac70897be67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035dd5f8-5f1a-43da-9801-be958d803071",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function: Generate GCS Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fa95e-05a3-4ec4-9638-c14509189a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gcs_folders(    \n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket\n",
    "):\n",
    "    try:\n",
    "         # Setup logger\n",
    "        logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "        gcs_folders = {}\n",
    "        gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "        gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "        gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "        gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "        gcs_folders['gcs_logs_folder'] = f\"{pipeline_run_name}/Logs\"\n",
    "\n",
    "        # Initialize GCS Client\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "        # Create empty folders directly\n",
    "        for folder in gcs_folders.values():\n",
    "            blob = bucket.blob(f\"{folder}/\")\n",
    "            blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "            logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "        logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "        return gcs_folders\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d216710-6994-4b68-a245-bbb985a6911a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Generate S3 Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db8c56-6cde-4902-827f-69e5cf9485cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s3_folder_prefix(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_errored_folder\n",
    "):\n",
    "    try:\n",
    "        logger.info(\"Started: generating S3 folder prefix.\")\n",
    "        # Get current date and time\n",
    "        current_datetime = datetime.now()\n",
    "\n",
    "        # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "        if current_datetime.hour == 0:\n",
    "            adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "        else:\n",
    "            adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "        # Extract year, month, and day from the adjusted date\n",
    "        year = str(adjusted_datetime.year)\n",
    "        month = f\"{adjusted_datetime.month:02d}\"\n",
    "        day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "        # Construct the prefix for S3 listing\n",
    "        prefix = f\"{year}/{month}/{day}/\"\n",
    "        logger.info(f\"Completed: generating S3 folder prefix {prefix}.\")\n",
    "\n",
    "        return prefix\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1291e-4a9e-44e8-8068-264a56d6dd92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Get List Calls to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ea29f-acb1-42d9-954a-7491011d4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_calls_to_process(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    s3_analysis_bucket,\n",
    "    s3_transcripts_location,\n",
    "    s3_prefix,\n",
    "    time_interval\n",
    "):\n",
    "    try:\n",
    "        logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "        # Initialize S3 Client\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        all_files = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "        # Get current UTC time (timezone-aware)\n",
    "        # current_time = datetime.now(timezone.utc)\n",
    "        current_time = datetime.now(timezone.utc) - timedelta(days=5) # ToTest\n",
    "\n",
    "        # Calculate the time threshold (2 hours before the current time)\n",
    "        time_threshold = current_time - timedelta(hours=time_interval) # ToTest\n",
    "        logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "        all_files = []\n",
    "\n",
    "        for page in pages:\n",
    "            for obj in page.get('Contents', []):\n",
    "                file_path = obj['Key']\n",
    "                s3_ts = obj['LastModified']\n",
    "\n",
    "                # Extract timestamp from filename\n",
    "                try:\n",
    "                    # Skip non-JSON files\n",
    "                    if file_path.endswith('.json'):\n",
    "                        call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                        call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                        # Compare only the time part\n",
    "                        if call_timestamp.time() <= time_threshold.time():\n",
    "                            all_files.append({\n",
    "                                'File': file_path,\n",
    "                                'Call_ID': call_id,\n",
    "                                'File_Timestamp': call_timestamp,\n",
    "                                'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                'S3_Timestamp': s3_ts,\n",
    "                                'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if all_files:\n",
    "            df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "            df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "            # Subset the DataFrame for only the most recent 2 hours bin\n",
    "            df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "            logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "\n",
    "            # Write the DataFrame to GCS\n",
    "            logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "            csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "            df_calls_list.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "            logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "            return df_calls_list\n",
    "\n",
    "        else:\n",
    "            logger.info(f\"0 Files fetched.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7e6b-b2f7-454d-bace-b10b782a74c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Download Transcripts to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df09b86-e30c-43f3-a925-c2ffdf44ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts_to_gcs(\n",
    "    file,\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    gcs_transcripts_folder,\n",
    "    s3_client,\n",
    "    s3_analysis_bucket\n",
    "):\n",
    "    \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "    local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "    gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "    try:\n",
    "        # Download file from S3\n",
    "        s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "        # Upload to GCS\n",
    "        blob = gcs_bucket.blob(gcs_blob_path)\n",
    "        blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "        return file, None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "        handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "        return None, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cafe67-ded8-404c-bf64-b07c41b65ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0cd14c-9f9a-4b97-b479-f0a425c48411",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d33b7e-f56c-430d-a4fe-815b2782e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Variables\n",
    "# ========================================================\n",
    "log_file = f\"{pipeline_run_name}.logs\"\n",
    "logger = setup_logger(log_file)\n",
    "\n",
    "logger.info(\"============================================================================\")\n",
    "logger.info(\"COMPONENT: Fetch Transcripts from S3 into GCS.\")\n",
    "logger.info(\"============================================================================\")\n",
    "\n",
    "# Fetch Configs\n",
    "configs = fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id\n",
    ")\n",
    "\n",
    "time_interval = 2\n",
    "vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "aws_access_key = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "aws_secret_key = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "s3_analysis_bucket = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "s3_transcripts_location = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "\n",
    "# Generate required GCS folder paths\n",
    "gcs_folders = generate_gcs_folders(pipeline_run_name, vai_gcs_bucket)\n",
    "\n",
    "gcs_staging_folder = gcs_folders[\"gcs_staging_folder\"]\n",
    "gcs_transcripts_folder = gcs_folders[\"gcs_transcripts_folder\"]\n",
    "gcs_errored_folder = gcs_folders[\"gcs_errored_folder\"]\n",
    "gcs_logs_folder = gcs_folders[\"gcs_logs_folder\"]\n",
    "\n",
    "# Generate S3 Prefix\n",
    "s3_prefix = generate_s3_folder_prefix(\n",
    "    pipeline_run_name, vai_gcs_bucket, gcs_errored_folder\n",
    ")\n",
    "\n",
    "# ========================================================\n",
    "# Fetch Calls List from S3\n",
    "# ========================================================\n",
    "df_calls_list = get_list_calls_to_process(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    s3_analysis_bucket,\n",
    "    s3_transcripts_location,\n",
    "    s3_prefix,\n",
    "    time_interval\n",
    ")\n",
    "\n",
    "call_count = len(df_calls_list)\n",
    "\n",
    "if call_count > 0:\n",
    "    files_list = df_calls_list.File.to_list()\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\", aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    success_downloads = []\n",
    "    failed_downloads = []\n",
    "\n",
    "    # Start Multithreaded Download\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        logger.info(f\"Started: Bulk download to GCS transcripts#: {call_count}\")\n",
    "\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                download_transcripts_to_gcs,\n",
    "                file,\n",
    "                pipeline_run_name,\n",
    "                vai_gcs_bucket,\n",
    "                gcs_staging_folder,\n",
    "                gcs_errored_folder,\n",
    "                gcs_transcripts_folder,\n",
    "                s3_client,\n",
    "                s3_analysis_bucket\n",
    "            ): file for file in files_list\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_file):\n",
    "            try:\n",
    "                success, failed = future.result()  # Get results\n",
    "\n",
    "                if success:\n",
    "                    success_downloads.append(success)\n",
    "                if failed:\n",
    "                    failed_downloads.append(failed)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    logger.info(\n",
    "        f\"Completed: Bulk download to GCS transcripts, \"\n",
    "        f\"Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    logger.info(\"No Calls to Process.\")\n",
    "\n",
    "gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "blob.upload_from_filename(log_file, checksum=None)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
