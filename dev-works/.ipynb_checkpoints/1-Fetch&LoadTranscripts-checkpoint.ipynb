{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79809ff5-73ff-40ca-a54e-203cecd12fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import logging, json\n",
    "from google.cloud import secretmanager\n",
    "from google.cloud import storage\n",
    "from google.cloud import logging as cloud_logging\n",
    "from datetime import datetime, timedelta, timezone, UTC\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9da3a2-6a4f-4571-a166-6e7719e47f45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function: Fetch Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a842d56a-8a16-490f-af78-793786b0e26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "\n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78995b4-377c-4730-8a3b-08a320653cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8aada0-66fc-4187-92a3-5c140e66f110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64737b-08c4-4e0e-b28a-cd7d95f511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_file):\n",
    "    \"\"\"\n",
    "    Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Path of the log file.\n",
    "\n",
    "    Returns:\n",
    "        logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "        if not logger.handlers:  # Avoid adding multiple handlers\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "\n",
    "            # File Handler\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            # Console Handler\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize logger: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8757f6-41b0-43a5-bb53-f2f5ac151402",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function: Handle Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee7064-a450-41a0-b018-a1077f0bd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exception(\n",
    "    file_id,\n",
    "    vai_gcs_bucket,\n",
    "    run_folder,\n",
    "    error_folder,\n",
    "    error_message\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d606d6-01ba-4464-9a26-6ac70897be67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035dd5f8-5f1a-43da-9801-be958d803071",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Generate GCS Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fa95e-05a3-4ec4-9638-c14509189a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gcs_folders(    \n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket\n",
    "):\n",
    "    try:\n",
    "         # Setup logger\n",
    "        logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "        gcs_folders = {}\n",
    "        gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "        gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "        gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "        gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "        gcs_folders['gcs_logs_folder'] = f\"{pipeline_run_name}/Logs\"\n",
    "\n",
    "        # Initialize GCS Client\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "        # Create empty folders directly\n",
    "        for folder in gcs_folders.values():\n",
    "            blob = bucket.blob(f\"{folder}/\")\n",
    "            blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "            logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "        logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "        return gcs_folders\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d216710-6994-4b68-a245-bbb985a6911a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Generate S3 Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db8c56-6cde-4902-827f-69e5cf9485cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s3_folder_prefix(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_errored_folder\n",
    "):\n",
    "    try:\n",
    "        logger.info(\"Started: generating S3 folder prefix.\")\n",
    "        # Get current date and time\n",
    "        current_datetime = datetime.now()\n",
    "\n",
    "        # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "        if current_datetime.hour == 0:\n",
    "            adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "        else:\n",
    "            adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "        # Extract year, month, and day from the adjusted date\n",
    "        year = str(adjusted_datetime.year)\n",
    "        month = f\"{adjusted_datetime.month:02d}\"\n",
    "        day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "        # Construct the prefix for S3 listing\n",
    "        prefix = f\"{year}/{month}/{day}/\"\n",
    "        logger.info(f\"Completed: generating S3 folder prefix {prefix}.\")\n",
    "\n",
    "        return prefix\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1291e-4a9e-44e8-8068-264a56d6dd92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Get List Calls to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ea29f-acb1-42d9-954a-7491011d4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_calls_to_process(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    s3_analysis_bucket,\n",
    "    s3_transcripts_location,\n",
    "    s3_prefix,\n",
    "    time_interval\n",
    "):\n",
    "    try:\n",
    "        logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "        # Initialize S3 Client\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key,\n",
    "            aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        all_files = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "        # Get current UTC time (timezone-aware)\n",
    "        # current_time = datetime.now(timezone.utc)\n",
    "        current_time = datetime.now(timezone.utc) - timedelta(days=5) # ToTest\n",
    "\n",
    "        # Calculate the time threshold (2 hours before the current time)\n",
    "        time_threshold = current_time - timedelta(hours=time_interval) # ToTest\n",
    "        logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "        all_files = []\n",
    "\n",
    "        for page in pages:\n",
    "            for obj in page.get('Contents', []):\n",
    "                file_path = obj['Key']\n",
    "                s3_ts = obj['LastModified']\n",
    "\n",
    "                # Extract timestamp from filename\n",
    "                try:\n",
    "                    # Skip non-JSON files\n",
    "                    if file_path.endswith('.json'):\n",
    "                        call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                        call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                        # Compare only the time part\n",
    "                        if call_timestamp.time() <= time_threshold.time():\n",
    "                            all_files.append({\n",
    "                                'File': file_path,\n",
    "                                'Call_ID': call_id,\n",
    "                                'File_Timestamp': call_timestamp,\n",
    "                                'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                'S3_Timestamp': s3_ts,\n",
    "                                'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if all_files:\n",
    "            df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "            df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "            # Subset the DataFrame for only the most recent 2 hours bin\n",
    "            df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "            logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "\n",
    "            # Write the DataFrame to GCS\n",
    "            logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "            csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "            df_calls_list.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "            logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "            return df_calls_list\n",
    "\n",
    "        else:\n",
    "            logger.info(f\"0 Files fetched.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7e6b-b2f7-454d-bace-b10b782a74c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Function: Download Transcripts to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df09b86-e30c-43f3-a925-c2ffdf44ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcripts_to_gcs(\n",
    "    file,\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    gcs_transcripts_folder,\n",
    "    s3_client,\n",
    "    s3_analysis_bucket\n",
    "):\n",
    "    \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "    local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "    gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "    try:\n",
    "        # Download file from S3\n",
    "        s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "        # Upload to GCS\n",
    "        blob = gcs_bucket.blob(gcs_blob_path)\n",
    "        blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "        return file, None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "        handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "        return None, file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0cd14c-9f9a-4b97-b479-f0a425c48411",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d33b7e-f56c-430d-a4fe-815b2782e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Variables\n",
    "# ========================================================\n",
    "log_file = f\"{pipeline_run_name}.logs\"\n",
    "logger = setup_logger(log_file)\n",
    "\n",
    "logger.info(\"============================================================================\")\n",
    "logger.info(\"COMPONENT: Fetch Transcripts from S3 into GCS.\")\n",
    "logger.info(\"============================================================================\")\n",
    "\n",
    "# Fetch Configs\n",
    "configs = fetch_secrets(\n",
    "    project_id,\n",
    "    secret_id,\n",
    "    version_id\n",
    ")\n",
    "\n",
    "time_interval = 2\n",
    "vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "aws_access_key = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "aws_secret_key = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "s3_analysis_bucket = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "s3_transcripts_location = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "\n",
    "# Generate required GCS folder paths\n",
    "gcs_folders = generate_gcs_folders(pipeline_run_name, vai_gcs_bucket)\n",
    "\n",
    "gcs_staging_folder = gcs_folders[\"gcs_staging_folder\"]\n",
    "gcs_transcripts_folder = gcs_folders[\"gcs_transcripts_folder\"]\n",
    "gcs_errored_folder = gcs_folders[\"gcs_errored_folder\"]\n",
    "gcs_logs_folder = gcs_folders[\"gcs_logs_folder\"]\n",
    "\n",
    "# Generate S3 Prefix\n",
    "s3_prefix = generate_s3_folder_prefix(\n",
    "    pipeline_run_name, vai_gcs_bucket, gcs_errored_folder\n",
    ")\n",
    "\n",
    "# ========================================================\n",
    "# Fetch Calls List from S3\n",
    "# ========================================================\n",
    "df_calls_list = get_list_calls_to_process(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_staging_folder,\n",
    "    gcs_errored_folder,\n",
    "    aws_access_key,\n",
    "    aws_secret_key,\n",
    "    s3_analysis_bucket,\n",
    "    s3_transcripts_location,\n",
    "    s3_prefix,\n",
    "    time_interval\n",
    ")\n",
    "\n",
    "call_count = len(df_calls_list)\n",
    "\n",
    "if call_count > 0:\n",
    "    files_list = df_calls_list.File.to_list()\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\", aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    success_downloads = []\n",
    "    failed_downloads = []\n",
    "\n",
    "    # Start Multithreaded Download\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        logger.info(f\"Started: Bulk download to GCS transcripts#: {call_count}\")\n",
    "\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                download_transcripts_to_gcs,\n",
    "                file,\n",
    "                pipeline_run_name,\n",
    "                vai_gcs_bucket,\n",
    "                gcs_staging_folder,\n",
    "                gcs_errored_folder,\n",
    "                gcs_transcripts_folder,\n",
    "                s3_client,\n",
    "                s3_analysis_bucket\n",
    "            ): file for file in files_list\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_file):\n",
    "            try:\n",
    "                success, failed = future.result()  # Get results\n",
    "\n",
    "                if success:\n",
    "                    success_downloads.append(success)\n",
    "                if failed:\n",
    "                    failed_downloads.append(failed)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    logger.info(\n",
    "        f\"Completed: Bulk download to GCS transcripts, \"\n",
    "        f\"Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    logger.info(\"No Calls to Process.\")\n",
    "\n",
    "gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "blob.upload_from_filename(log_file, checksum=None)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
