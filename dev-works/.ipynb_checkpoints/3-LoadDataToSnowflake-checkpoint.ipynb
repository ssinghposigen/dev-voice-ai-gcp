{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eb341d-6567-4b33-8f16-a3e7fda18dd2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaa923-6833-4a0b-8a2c-dc5cd1ab1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, logging, json\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from google.cloud import secretmanager\n",
    "from google.cloud import storage\n",
    "import snowflake.connector as sc\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from cryptography.hazmat.primitives import serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846b5c8-d639-46d9-b9f5-4cf0275bc77c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function: Fetch Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7faac1c-f166-490f-a9a8-0130bb4ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Access a secret from Google Secret Manager\n",
    "\n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID\n",
    "        secret_id: The ID of the secret to access\n",
    "        version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "    Returns:\n",
    "        The secret payload as a string\n",
    "    \"\"\"\n",
    "    # Create the Secret Manager client\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "    # Decode and parse the JSON payload\n",
    "    secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "    try:\n",
    "        return json.loads(secret_payload)  # Convert string to JSON\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The secret payload is not a valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076fcb9-ab1a-409e-ba1f-d44ea80ad54a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558b08a-1ae1-4e08-911b-cd82f6974956",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c650e28-6b74-44a8-ad8d-6db25acf8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(\n",
    "    log_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "    Args:\n",
    "        log_file (str): Path of the log file.\n",
    "\n",
    "    Returns:\n",
    "        logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger = logging.getLogger(log_file)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "        # Remove any existing handlers (to prevent duplicate logging)\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "\n",
    "        if not logger.handlers:  # Avoid adding multiple handlers\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "\n",
    "            # File Handler\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            # Console Handler\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize logger: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da85c5-8c7c-4c79-be46-58b50a804a8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Handle Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36affd-7598-4b9b-9541-ab639476267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_exception(\n",
    "    file_id,\n",
    "    vai_gcs_bucket,\n",
    "    run_folder,\n",
    "    error_folder,\n",
    "    error_message,\n",
    "    logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "        logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "        gcs_client = storage.Client()\n",
    "        bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(error_df_path)\n",
    "\n",
    "        if blob.exists():\n",
    "            error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "        else:\n",
    "            error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "        error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "        error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "        logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to error tracking file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1241fbd-cbea-43d4-8bd8-ec2e8cc8c788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d70b4-754e-4d2d-87c9-ae00ada7fcff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Read CSV from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dde0ee-adbc-4bd3-a1a2-efdc425374e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gcs_csv(file_path):\n",
    "    blob = bucket.blob(file_path)\n",
    "    csv_data = blob.download_as_text()\n",
    "    return pd.read_csv(io.StringIO(csv_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241cb5d-64e1-4011-947a-dc6a1b8801dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function: Insert New Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731beffc-340c-45af-afa8-d449ebc913b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_records(\n",
    "    pipeline_run_name,\n",
    "    vai_gcs_bucket,\n",
    "    gcs_stagging_folder,\n",
    "    gcs_errored_folder,\n",
    "    snf_account,\n",
    "    snf_user,\n",
    "    snf_private_key,\n",
    "    snf_private_key_pwd,\n",
    "    snf_warehouse,\n",
    "    snf_databse,\n",
    "    snf_schema,\n",
    "    table_name,\n",
    "    df\n",
    "):\n",
    "    \"\"\"\n",
    "    Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "\n",
    "    Steps:\n",
    "    1. Fetches existing IDs from table.\n",
    "    2. Filters out rows with existing IDs from DataFrame.\n",
    "    3. Adds 'LOAD_DATE_UTC' column with current UTC timestamp.\n",
    "    4. Inserts only new records.\n",
    "\n",
    "    Args:\n",
    "        conn: Snowflake connection object.\n",
    "        table_name (str): Name of the target table.\n",
    "        df (pd.DataFrame): DataFrame containing the data (must have 'CONTACT_ID' column).\n",
    "\n",
    "    Returns:\n",
    "        int: Number of inserted records.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "    :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "    :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "    :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "    :return: Pandas DataFrame with category mappings.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load & Decrypt the Private Key\n",
    "        snf_private_key = serialization.load_pem_private_key(\n",
    "            snf_private_key.encode(),\n",
    "            password=snf_private_key_pwd.encode(),\n",
    "            backend=None  # Default backend\n",
    "        )\n",
    "\n",
    "        # Step 2: Convert to Snowflake Compatible Format\n",
    "        pkey_bytes = snf_private_key.private_bytes(\n",
    "            encoding=serialization.Encoding.DER,\n",
    "            format=serialization.PrivateFormat.PKCS8,\n",
    "            encryption_algorithm=serialization.NoEncryption(),\n",
    "        )\n",
    "\n",
    "        conn_params = {\n",
    "            'account': snf_account,\n",
    "            'user': snf_user,\n",
    "            'private_key': snf_private_key,\n",
    "            'warehouse': snf_warehouse,\n",
    "            'database': snf_databse,\n",
    "            'schema': snf_schema\n",
    "        }\n",
    "\n",
    "        conn = sc.connect(**conn_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Step 1: Get existing IDs from Snowflake table for the last two days (current and previous day)\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT DISTINCT(CONTACT_ID) \n",
    "            FROM {table_name} \n",
    "            WHERE LOAD_DATE >= DATEADD(DAY, -1, CURRENT_DATE)\n",
    "        \"\"\")\n",
    "        existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "        # Step 2: Filter DataFrame to keep only new records\n",
    "        new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)]\n",
    "\n",
    "        if new_records_df.empty:\n",
    "            logger.info(\"No new records to insert\")\n",
    "            return 0\n",
    "\n",
    "        # Step 3: Add UTC timestamp column\n",
    "        utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        new_records_df = new_records_df.copy()  # Avoid modifying original df\n",
    "        new_records_df[\"LOAD_DATE\"] = utc_now  # Add new column\n",
    "\n",
    "        # Step 4: Insert new records into Snowflake\n",
    "        success, nchunks, nrows, _ = write_pandas(conn, new_records_df, table_name)\n",
    "\n",
    "        logger.info(f\"Inserted {nrows} new records with UTC load date\")\n",
    "        logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return nrows\n",
    "\n",
    "    except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e), logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fdfc1-b4d8-4ba9-9b95-9d92cd917b70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc276-8622-4803-a2bb-95c945ff6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "\n",
    "    # GCP Configuration\n",
    "    gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "    gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "    # Pipeline Configuration\n",
    "    gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "    gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "    gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "    gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "    gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "    gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "    # Snowflake Configuration\n",
    "    snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "    snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "    snf_private_key = configs.get(\"private_key\")\n",
    "    snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "    snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "    snf_database = configs.get(\"VAI_SNF_DATABASE\")\n",
    "    snf_schema = configs.get(\"VAI_SNF_SCHEMA\")\n",
    "\n",
    "    # Step 2: Download Master Log File from GCS\n",
    "    log_file = f\"{pipeline_run_name}.logs\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(vai_gcs_bucket)\n",
    "    blob = bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "    # Download master log file\n",
    "    blob.download_to_filename(log_file)\n",
    "\n",
    "    logger = setup_logger(log_file)\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"============================================================================\")\n",
    "    logger.info(\"COMPONENT: Write Data to Snowflake.\")\n",
    "    logger.info(\"============================================================================\")\n",
    "    logger.info(\"Fetched Master Log File from GCS bucket.\")\n",
    "\n",
    "    # Read Inter & Intra Call DataFrames\n",
    "    inter_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_inter_call_df.csv\")\n",
    "    inter_call_df.columns = inter_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "    intra_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_intra_call_df.csv\")\n",
    "    intra_call_df.columns = intra_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "\n",
    "    logger.info(f\"Started: writing data to snowflake.\")\n",
    "    table_name ='SRC_GCP_INTER_CALLS'    \n",
    "    logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "    insert_new_records(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_database,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        inter_call_df\n",
    "    )\n",
    "    logger.info(f\"SRC_GCP_INTER_CALLS: Inserted records #{len(inter_call_df)}\")\n",
    "\n",
    "\n",
    "    logger.info(f\"Writing data to table: {snf_database}.{table_name}\")\n",
    "    table_name ='SRC_GCP_INTRA_CALLS'\n",
    "    insert_new_records(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_database,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        intra_call_df\n",
    "    )\n",
    "    logger.info(f\"SRC_GCP_INTRA_CALLS: Inserted records #{len(intra_call_df)}\")\n",
    "    logger.info(f\"Completed: writing data to snowflake.\")\n",
    "\n",
    "except Exception as e:\n",
    "        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a229e-4193-4210-82ea-c966cbb8c907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
