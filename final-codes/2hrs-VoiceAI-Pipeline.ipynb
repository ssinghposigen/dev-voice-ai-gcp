{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ac08b-3980-422d-a165-df3fbaa733f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5cfd177-51e0-4c1e-8afe-26f581538106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "from datetime import datetime, timezone, UTC, timedelta\n",
    "import kfp\n",
    "from kfp import dsl, compiler, components\n",
    "import json, logging\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6044-2073-4ba7-b4d4-bccfa22d8109",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Component: Listing new Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e570f4d8-4ad7-4f1e-88ce-bf6f46fb19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-cx-voiceai/dev-cx-voiceai-docker-image:dev-1\"\n",
    ")\n",
    "def list_download_calls_s3_to_gcs(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    ") -> int:\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import logging, json\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import logging as cloud_logging\n",
    "    from datetime import datetime, timedelta, timezone, UTC\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Skipping checksum validation\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function Definitions\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "    def setup_logger(log_file):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(\"vertex_pipeline_logger\")\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "\n",
    "    def generate_gcs_folders(    \n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket\n",
    "    ):\n",
    "        try:\n",
    "             # Setup logger\n",
    "            logging.info(\"Started: generating GCS pipeline folders.\")\n",
    "            gcs_folders = {}\n",
    "            gcs_folders['gcs_staging_folder'] = f\"{pipeline_run_name}/Stagging\"\n",
    "            gcs_folders['gcs_intra_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "            gcs_folders['gcs_inter_call_dfs_folder'] = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "            gcs_folders['gcs_transcripts_folder'] = f\"{pipeline_run_name}/Transcripts\"\n",
    "            gcs_folders['gcs_errored_folder'] = f\"{pipeline_run_name}/Errored\"\n",
    "            gcs_folders['gcs_logs_folder'] = f\"{pipeline_run_name}/Logs\"\n",
    "\n",
    "            # Initialize GCS Client\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "\n",
    "            # Create empty folders directly\n",
    "            for folder in gcs_folders.values():\n",
    "                blob = bucket.blob(f\"{folder}/\")\n",
    "                blob.upload_from_string(\"\", content_type=\"application/x-www-form-urlencoded\")\n",
    "                logging.info(f\"Created folder: {folder}\")\n",
    "\n",
    "            logging.info(\"Completed: generating GCS pipeline folders.\")\n",
    "            return gcs_folders\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e))\n",
    "\n",
    "\n",
    "    def generate_s3_folder_prefix(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_errored_folder\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(\"Started: generating S3 folder prefix.\")\n",
    "            # Get current date and time\n",
    "            current_datetime = datetime.now()\n",
    "\n",
    "            # Check if the run is around midnight (e.g., between 00:00 and 01:00)\n",
    "            if current_datetime.hour == 0:\n",
    "                adjusted_datetime = current_datetime - timedelta(days=1)  # Move to the previous day\n",
    "            else:\n",
    "                adjusted_datetime = current_datetime  # Keep the current day\n",
    "\n",
    "            # Extract year, month, and day from the adjusted date\n",
    "            year = str(adjusted_datetime.year)\n",
    "            month = f\"{adjusted_datetime.month:02d}\"\n",
    "            day = f\"{adjusted_datetime.day:02d}\"\n",
    "\n",
    "            # Construct the prefix for S3 listing\n",
    "            prefix = f\"{year}/{month}/{day}/\"\n",
    "            logger.info(f\"Completed: generating S3 folder prefix {prefix}\")\n",
    "\n",
    "            return prefix\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "\n",
    "    def get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "        time_interval\n",
    "    ):\n",
    "        try:\n",
    "            logger.info(f\"Started: listing calls from: {s3_transcripts_location}/{s3_prefix}\")\n",
    "            # Initialize S3 Client\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key\n",
    "            )\n",
    "\n",
    "            all_files = []\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=s3_analysis_bucket, Prefix=f\"{s3_transcripts_location}/{s3_prefix}\")\n",
    "\n",
    "            # Get current UTC time (timezone-aware)\n",
    "            current_time = datetime.now(timezone.utc)\n",
    "            \n",
    "            # Calculate the time threshold (2 hours before the current time)\n",
    "            time_threshold = current_time - timedelta(hours=time_interval)\n",
    "            logger.info(f\"Fetching Calls between: {time_threshold.time()} and {current_time.time()}\")\n",
    "\n",
    "            all_files = []\n",
    "\n",
    "            for page in pages:\n",
    "                for obj in page.get('Contents', []):\n",
    "                    file_path = obj['Key']\n",
    "                    s3_ts = obj['LastModified']\n",
    "\n",
    "                    # Extract timestamp from filename\n",
    "                    try:\n",
    "                        # Skip non-JSON files\n",
    "                        if file_path.endswith('.json'):\n",
    "                            call_id = file_path.split('/')[-1].split(\"_analysis_\")[0]\n",
    "                            call_timestamp = pd.to_datetime(file_path.split('analysis_')[-1].split('.')[0].replace('Z', \"\"), utc=True)\n",
    "\n",
    "                            # Compare only the time part\n",
    "                            if call_timestamp.time() <= time_threshold.time():\n",
    "                                all_files.append({\n",
    "                                    'File': file_path,\n",
    "                                    'Call_ID': call_id,\n",
    "                                    'File_Timestamp': call_timestamp,\n",
    "                                    'File_Date': call_timestamp.date().strftime('%Y-%m-%d'),\n",
    "                                    'File_Time': call_timestamp.time().strftime('%H:%M:%S'),\n",
    "                                    'S3_Timestamp': s3_ts,\n",
    "                                    'S3_Date': s3_ts.strftime('%Y-%m-%d'),\n",
    "                                    'S3_Time': s3_ts.strftime('%H:%M:%S')\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Skipping file {file_path} due to timestamp parsing error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            if all_files:\n",
    "                df_calls_list = pd.DataFrame(all_files).sort_values(['File_Timestamp'], ascending=False)\n",
    "                df_calls_list['Time_Bin'] = df_calls_list['File_Timestamp'].dt.floor('2h')\n",
    "                # Subset the DataFrame for only the most recent 2 hours bin\n",
    "                df_calls_list = df_calls_list[df_calls_list['Time_Bin'] == df_calls_list['Time_Bin'].max()]\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "\n",
    "                # Write the DataFrame to GCS\n",
    "                logger.info(f\"Files to process for the last 2 hours: {len(df_calls_list)}\")\n",
    "                csv_path = f\"gs://{vai_gcs_bucket}/{gcs_folders['gcs_staging_folder']}/{pipeline_run_name}_transcripts_to_process.csv\"\n",
    "                df_calls_list.to_csv(csv_path, index=False)\n",
    "                logger.info(f\"Written Transcripts list to GCS: {csv_path}\")\n",
    "                logger.info(f\"Completed: listing calls to process Calls#: {len(df_calls_list)}\")\n",
    "\n",
    "                return df_calls_list\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"0 Files fetched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "    def download_transcripts_to_gcs(\n",
    "        file,\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        s3_client,\n",
    "        s3_analysis_bucket\n",
    "    ):\n",
    "        \"\"\"Download transcript from S3 and upload to GCS.\"\"\"\n",
    "\n",
    "        local_file_path = f\"/tmp/{file.split('/')[-1]}\"  # Temporary local storage\n",
    "        gcs_blob_path = f\"{gcs_transcripts_folder}/{file.split('/')[-1]}\"\n",
    "        gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "\n",
    "        try:\n",
    "            # Download file from S3\n",
    "            s3_client.download_file(s3_analysis_bucket, file, local_file_path)\n",
    "\n",
    "            # Upload to GCS\n",
    "            blob = gcs_bucket.blob(gcs_blob_path)\n",
    "            blob.upload_from_filename(local_file_path, checksum=None)\n",
    "\n",
    "            return file, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: Failed to process {file} -> {str(e)}\")\n",
    "            handle_exception(file, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "            return None, file\n",
    "\n",
    "\n",
    "    # ========================================================\n",
    "    # Variables\n",
    "    # ========================================================\n",
    "    log_file = f\"{pipeline_run_name}.logs\"\n",
    "    logger = setup_logger(log_file)\n",
    "\n",
    "    logger.info(\"============================================================================\")\n",
    "    logger.info(\"COMPONENT: Fetch Transcripts from S3 into GCS.\")\n",
    "    logger.info(\"============================================================================\")\n",
    "\n",
    "    # Fetch Configs\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "\n",
    "    time_interval = 2\n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "    aws_access_key = configs.get(\"VAI_AWS_ACCESS_KEY\")\n",
    "    aws_secret_key = configs.get(\"VAI_AWS_SECRET_KEY\")\n",
    "    s3_analysis_bucket = configs.get(\"VAI_S3_ANALYSIS_BUCKET\")\n",
    "    s3_transcripts_location = configs.get(\"VAI_S3_TRANSCRIPTS_LOCATION\")\n",
    "\n",
    "    # Generate required GCS folder paths\n",
    "    gcs_folders = generate_gcs_folders(pipeline_run_name, vai_gcs_bucket)\n",
    "\n",
    "    gcs_staging_folder = gcs_folders[\"gcs_staging_folder\"]\n",
    "    gcs_transcripts_folder = gcs_folders[\"gcs_transcripts_folder\"]\n",
    "    gcs_errored_folder = gcs_folders[\"gcs_errored_folder\"]\n",
    "    gcs_logs_folder = gcs_folders[\"gcs_logs_folder\"]\n",
    "\n",
    "    # Generate S3 Prefix\n",
    "    s3_prefix = generate_s3_folder_prefix(\n",
    "        pipeline_run_name, vai_gcs_bucket, gcs_errored_folder\n",
    "    )\n",
    "\n",
    "    # ========================================================\n",
    "    # Fetch Calls List from S3\n",
    "    # ========================================================\n",
    "    df_calls_list = get_list_calls_to_process(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_staging_folder,\n",
    "        gcs_errored_folder,\n",
    "        aws_access_key,\n",
    "        aws_secret_key,\n",
    "        s3_analysis_bucket,\n",
    "        s3_transcripts_location,\n",
    "        s3_prefix,\n",
    "        time_interval\n",
    "    )\n",
    "\n",
    "    call_count = len(df_calls_list)\n",
    "\n",
    "    if call_count > 0:\n",
    "        files_list = df_calls_list.File.to_list()\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\", aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key\n",
    "        )\n",
    "\n",
    "        success_downloads = []\n",
    "        failed_downloads = []\n",
    "\n",
    "        # Start Multithreaded Download\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            logger.info(f\"Started: Bulk download to GCS transcripts#: {call_count}\")\n",
    "\n",
    "            future_to_file = {\n",
    "                executor.submit(\n",
    "                    download_transcripts_to_gcs,\n",
    "                    file,\n",
    "                    pipeline_run_name,\n",
    "                    vai_gcs_bucket,\n",
    "                    gcs_staging_folder,\n",
    "                    gcs_errored_folder,\n",
    "                    gcs_transcripts_folder,\n",
    "                    s3_client,\n",
    "                    s3_analysis_bucket\n",
    "                ): file for file in files_list\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_file):\n",
    "                try:\n",
    "                    success, failed = future.result()  # Get results\n",
    "\n",
    "                    if success:\n",
    "                        success_downloads.append(success)\n",
    "                    if failed:\n",
    "                        failed_downloads.append(failed)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected Error: {str(e)}\")\n",
    "                    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e))\n",
    "\n",
    "        logger.info(\n",
    "            f\"Completed: Bulk download to GCS transcripts, \"\n",
    "            f\"Success#: {len(success_downloads)}, Failed#: {len(failed_downloads)}\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logger.info(\"No Calls to Process.\")\n",
    "\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "    blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "    blob.upload_from_filename(log_file, checksum=None)\n",
    "\n",
    "    return call_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c203-f605-4bfb-b5b9-e4b15791c4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Parallel Process Call Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13426e4d-a28c-4416-a24b-228df8f6da01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-cx-voiceai/dev-cx-voiceai-docker-image:dev-1\"\n",
    ")\n",
    "def process_transcripts(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import threading\n",
    "    import concurrent.futures\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.special import softmax\n",
    "    import logging\n",
    "    import re, os, json, io\n",
    "    from datetime import datetime, timezone, UTC\n",
    "    from typing import List, Dict\n",
    "\n",
    "    from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "    import snowflake.connector as sc\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import dlp_v2\n",
    "    from google.cloud import logging as cloud_logging\n",
    "\n",
    "    import vertexai\n",
    "    import vertexai.preview.generative_models as generative_models\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "\n",
    "    # Sentiments\n",
    "    from transformers import pipeline\n",
    "    from transformers import AutoTokenizer, AutoConfig\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Exception hanlding mechanism\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "\n",
    "    def setup_logger(\n",
    "        log_file\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(log_file)\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            # Remove any existing handlers (to prevent duplicate logging)\n",
    "            if logger.hasHandlers():\n",
    "                logger.handlers.clear()\n",
    "\n",
    "            if not logger.handlers:  # Avoid adding multiple handlers\n",
    "                formatter = logging.Formatter(\n",
    "                    '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "                )\n",
    "\n",
    "                # File Handler\n",
    "                file_handler = logging.FileHandler(log_file)\n",
    "                file_handler.setLevel(logging.INFO)\n",
    "                file_handler.setFormatter(formatter)\n",
    "                logger.addHandler(file_handler)\n",
    "\n",
    "                # Console Handler\n",
    "                console_handler = logging.StreamHandler()\n",
    "                console_handler.setLevel(logging.INFO)\n",
    "                console_handler.setFormatter(formatter)\n",
    "                logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Function to create thread-specific log files\n",
    "    def setup_thread_logger(\n",
    "        contact_id\n",
    "    ):\n",
    "        \"\"\"Create a separate log file for each transcript.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "        log_filename = f\"{contact_id}_{timestamp}.log\"\n",
    "        log_filepath = os.path.join(temp_log_folder, log_filename)\n",
    "\n",
    "        thread_logger = logging.getLogger(log_filename)\n",
    "        thread_logger.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "        # Remove handlers to prevent duplication\n",
    "        if thread_logger.hasHandlers():\n",
    "            thread_logger.handlers.clear()\n",
    "\n",
    "        file_handler = logging.FileHandler(log_filepath)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        thread_logger.addHandler(file_handler)\n",
    "\n",
    "        return thread_logger, log_filepath\n",
    "\n",
    "\n",
    "    def merge_logs(log_files, master_log_file, master_logger):\n",
    "        \"\"\"Merge all thread logs into the master log file, sorting by timestamp.\"\"\"\n",
    "        if not log_files:\n",
    "            master_logger.warning(\"No log files found to merge.\")\n",
    "            return\n",
    "\n",
    "        # Filter valid log files and handle NoneType values\n",
    "        valid_logs = [\n",
    "            log_file for log_file in log_files\n",
    "            if isinstance(log_file, str) and log_file.endswith(\".log\") and os.path.exists(log_file)\n",
    "        ]\n",
    "\n",
    "        if not valid_logs:\n",
    "            master_logger.warning(\"No valid log files to merge.\")\n",
    "            return\n",
    "\n",
    "        # Sort logs based on timestamps in filenames (assuming format: transcript_YYYYMMDD_HHMMSS.log)\n",
    "        sorted_logs = sorted(valid_logs, key=lambda x: os.path.basename(x).split(\"_\")[-1].replace(\".log\", \"\"))\n",
    "\n",
    "        with open(master_log_file, \"a\") as master_log:  # Open in append mode\n",
    "            for log_file in sorted_logs:\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as thread_log:\n",
    "                        master_log.write(thread_log.read() + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    master_logger.error(f\"Error reading {log_file}: {e}\")\n",
    "\n",
    "        master_logger.info(f\"All thread logs merged into: {master_log_file}\")\n",
    "\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message,\n",
    "        logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {str(e)}\")\n",
    "\n",
    "    def fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        master_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        List all files in a GCS bucket, handling pagination.\n",
    "\n",
    "        :param bucket_name: Name of the GCS bucket\n",
    "        :param prefix: (Optional) Folder path to filter files\n",
    "        :return: List of file paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            master_logger.info(f\"Fetching Transcripts from GCS: {gcs_transcripts_folder}\")\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blobs_iterator = bucket.list_blobs(prefix=gcs_transcripts_folder)  # GCS handles pagination internally\n",
    "\n",
    "            transcripts_list = []\n",
    "            for page in blobs_iterator.pages:  # Handling pagination\n",
    "                for blob in page:\n",
    "                    if not blob.name.endswith(\"/\"):\n",
    "                        transcripts_list.append(blob.name)\n",
    "                        # transcripts_list.append(os.path.basename(blob.name))\n",
    "            master_logger.info(f\"Completed: Fetching Transcripts from GCS #: {len(transcripts_list)}\")\n",
    "            return transcripts_list\n",
    "\n",
    "        except Exception as e:            \n",
    "            master_logger.info(f\"Exception in: fetch_transcripts_from_gcs. {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view,\n",
    "        master_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetch Category-Subcategory mapping from Snowflake using a private key stored in GCP Secret Manager.\n",
    "\n",
    "        :param snf_secret_project_id: GCP project where the secret is stored.\n",
    "        :param secret_name: Name of the secret containing the Snowflake private key.\n",
    "        :param snowflake_params: Dictionary containing Snowflake connection parameters.\n",
    "\n",
    "        :return: Pandas DataFrame with category mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None  # Default backend\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            # Step 3: Connect to Snowflake\n",
    "            catsubcat_conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_catsubcat_databse,\n",
    "                'schema': snf_catsubcat_schema\n",
    "            }\n",
    "\n",
    "            # Connect to Snowflake\n",
    "            conn = sc.connect(**catsubcat_conn_params)\n",
    "\n",
    "            # Fetch data from Snowflake\n",
    "            query = f\"SELECT CATEGORY, SUBCATEGORY FROM {snf_catsubcat_view}\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "            conn.close()\n",
    "            master_logger.info(\"Completed: Fetching Category, Sub-Category Mapping.\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            master_logger.info(f\"Exception in: fetch_category_mapping_from_snowflake. {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Intra Call \n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def mask_cvv_contextually(df):\n",
    "        \"\"\"Masks CVVs with contextual awareness.\"\"\"\n",
    "        cvv_context = False\n",
    "        indices_to_mask = []\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 3  # Number of lines to wait before resetting context\n",
    "        context_counter = 0\n",
    "\n",
    "        cvv_patterns = [\n",
    "            r'\\b\\d{3}\\b',\n",
    "            r'\\b\\d{4}\\b'\n",
    "        ]\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            if re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card|the 3 digit code)\\b', caption, re.IGNORECASE):\n",
    "                cvv_context = True\n",
    "                indices_to_mask = []\n",
    "                context_counter = 0\n",
    "            elif cvv_context:\n",
    "                clean_caption = re.sub(r'[^0-9]', '', caption)  # Extract numbers\n",
    "                if clean_caption:\n",
    "                    indices_to_mask.append(i)\n",
    "                    context_counter += 1\n",
    "\n",
    "                    for pattern in cvv_patterns:\n",
    "                        if re.search(pattern, clean_caption):\n",
    "                            for idx in indices_to_mask:\n",
    "                                masked_captions[idx] = \"[CVV_REDACTED]\"\n",
    "                            cvv_context = False\n",
    "                            indices_to_mask = []\n",
    "                            break #break the for loop, as we have found the matching pattern\n",
    "                    else:\n",
    "                        if context_counter > context_timeout:\n",
    "                            cvv_context = False\n",
    "                            indices_to_mask = []\n",
    "                else:\n",
    "                    context_counter += 1\n",
    "                    if context_counter > context_timeout:\n",
    "                        cvv_context = False\n",
    "                        indices_to_mask = []\n",
    "\n",
    "            else:\n",
    "                cvv_context = False\n",
    "                indices_to_mask = []\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "    def mask_expiration_date_contextually(df):\n",
    "        \"\"\"Masks expiration dates with contextual awareness, redacting only the pattern.\"\"\"\n",
    "        exp_date_context = False\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 4  # Number of lines to wait before resetting context\n",
    "        context_counter = 0\n",
    "\n",
    "        exp_patterns = [\n",
    "            r'\\b(0[1-9]|1[0-2])\\s*/\\s*(\\d{2}|\\d{4})\\b',  # MM/YY, MM/YYYY\n",
    "            r'\\b([1-9])\\s*/\\s*(\\d{2}|\\d{4})\\b',  # M/YY, M/YYYY\n",
    "            r'\\b(0[1-9]|1[0-2])(\\d{2}|\\d{4})\\b',  # MMYY, MMYYYY\n",
    "            r'\\b([1-9])(\\d{2}|\\d{4})\\b',  # MYY, MYYY\n",
    "            r'\\b(0[1-9]|1[0-2])\\s+(\\d{2}|\\d{4})\\b',  # MM DD, MM YYYY\n",
    "            r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*(?:\\d{2,4})?\\b', # Month name followed by day number and optional year\n",
    "            r'\\b(0[1-9]|1[0-2]):(\\d{2}|\\d{4})\\b', #MM:YY, MM:YYYY\n",
    "            r'\\b(0[1-9]|1[0-2])[-/](\\d{2}|\\d{4})\\b|\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*\\d{2,4}\\b' #DLP regex\n",
    "        ]\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            if re.search(r'\\b(?:expiration date|exp date|expiry date|Expiration is)\\b', caption, re.IGNORECASE):\n",
    "                exp_date_context = True\n",
    "                context_counter = 0\n",
    "                for pattern in exp_patterns:\n",
    "                    match = re.search(pattern, caption)\n",
    "                    if match:\n",
    "                        masked_captions[i] = re.sub(re.escape(match.group(0)), \"[EXPIRY_DATE_REDACTED]\", masked_captions[i])\n",
    "                        exp_date_context = False\n",
    "                        context_counter = 0\n",
    "                        break\n",
    "\n",
    "            elif exp_date_context:\n",
    "                if context_counter <= context_timeout:\n",
    "                    for pattern in exp_patterns:\n",
    "                        match = re.search(pattern, caption)\n",
    "                        if match:\n",
    "                            masked_captions[i] = re.sub(re.escape(match.group(0)), \"[EXPIRY_DATE_REDACTED]\", masked_captions[i])\n",
    "                            exp_date_context = False\n",
    "                            context_counter = 0\n",
    "                            break\n",
    "                    else:\n",
    "                        context_counter += 1\n",
    "                else:\n",
    "                    exp_date_context = False\n",
    "                    context_counter = 0\n",
    "            else:\n",
    "                context_counter = 0\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "    def mask_card_numbers_contextually(df):\n",
    "        \"\"\"Masks card numbers across multiple lines, redacting only the numbers.\"\"\"\n",
    "        card_number_context = False\n",
    "        concatenated_number = \"\"\n",
    "        indices_to_mask = []\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 5  # Number of lines to wait before resetting context\n",
    "        context_counter = 0\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            cleaned_caption = re.sub(r'[^0-9]', '', caption)  # Extract only digits\n",
    "\n",
    "            # Detect context where card number is mentioned\n",
    "            if re.search(r'\\b(?:card number|credit card|new card number|card details|that is)\\b', caption, re.IGNORECASE):\n",
    "                card_number_context = True\n",
    "                concatenated_number = \"\"\n",
    "                indices_to_mask = [i]\n",
    "                context_counter = 0\n",
    "\n",
    "            elif card_number_context:\n",
    "                if cleaned_caption:  # If line contains numbers, capture them\n",
    "                    concatenated_number += cleaned_caption\n",
    "                    indices_to_mask.append(i)\n",
    "\n",
    "                context_counter += 1\n",
    "\n",
    "            # If total digits collected suggest a credit card number, redacting them\n",
    "            if len(concatenated_number) >= 13 and len(concatenated_number) <= 19:\n",
    "                for idx in indices_to_mask:\n",
    "                    numbers = re.findall(r'\\d+', df['caption'][idx])\n",
    "                    for num in numbers:\n",
    "                        masked_captions[idx] = re.sub(re.escape(num), \"[CARD_NUMBER_REDACTED]\", masked_captions[idx])\n",
    "                card_number_context = False  # Reset context\n",
    "                concatenated_number = \"\"\n",
    "                indices_to_mask = []\n",
    "            elif context_counter > context_timeout:\n",
    "                card_number_context = False\n",
    "                concatenated_number = \"\"\n",
    "                indices_to_mask = []\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mask_emails_contextually(df):\n",
    "        \"\"\"Masks email addresses spoken across multiple lines with contextual awareness.\"\"\"\n",
    "        email_context = False\n",
    "        email_parts = []\n",
    "        indices_to_mask = []\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 5  \n",
    "        context_counter = 0\n",
    "\n",
    "        # Improved regex to match emails correctly (including optional spaces around '@' and '.')\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@\\s*[A-Za-z0-9.-]+\\s*\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            # Remove spaces around '@' and '.' to catch incorrectly spaced emails\n",
    "            cleaned_caption = re.sub(r'\\s*@\\s*', '@', caption)\n",
    "            cleaned_caption = re.sub(r'\\s*\\.\\s*', '.', cleaned_caption)\n",
    "\n",
    "            # Immediate masking if full email is found in one line\n",
    "            if re.search(email_pattern, cleaned_caption):\n",
    "                masked_captions[i] = re.sub(email_pattern, \"[EMAIL_REDACTED]\", cleaned_caption)\n",
    "                email_context = False  \n",
    "                continue  \n",
    "\n",
    "            # Detect email context\n",
    "            if re.search(r'\\b(?:email|email address|send to|mail to)\\b', cleaned_caption, re.IGNORECASE):\n",
    "                email_context = True\n",
    "                email_parts = []\n",
    "                indices_to_mask = []\n",
    "                context_counter = 0\n",
    "\n",
    "            elif email_context:\n",
    "                # Capture words containing '@' or adjacent to it\n",
    "                potential_parts = re.findall(r'\\b[A-Za-z0-9._%+-]+(?:@|(?:@[A-Za-z0-9.-]+))?\\b', cleaned_caption)\n",
    "                valid_parts = [part for part in potential_parts if '@' in part or len(email_parts) > 0]\n",
    "\n",
    "                if valid_parts:\n",
    "                    indices_to_mask.append(i)\n",
    "                    context_counter += 1\n",
    "                    email_parts.extend(valid_parts)\n",
    "\n",
    "                    if re.search(email_pattern, \"\".join(email_parts)):\n",
    "                        for idx in indices_to_mask:\n",
    "                            masked_captions[idx] = \"[EMAIL_REDACTED]\"\n",
    "                        email_context = False\n",
    "                        email_parts = []\n",
    "                        indices_to_mask = []\n",
    "                        break  \n",
    "                else:\n",
    "                    context_counter += 1\n",
    "                    if context_counter > context_timeout:\n",
    "                        email_context = False\n",
    "                        email_parts = []\n",
    "                        indices_to_mask = []\n",
    "\n",
    "            else:\n",
    "                email_context = False\n",
    "                email_parts = []\n",
    "                indices_to_mask = []\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mask_check_number(df):\n",
    "        \"\"\"Masks check numbers with contextual awareness.\"\"\"\n",
    "        check_context = False\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 3  # Adjust as needed\n",
    "        context_counter = 0\n",
    "\n",
    "        # Regex to capture check numbers (4-8 digits is common)\n",
    "        check_number_regex = r'\\b\\d{4,8}\\b'\n",
    "\n",
    "        for i, caption in enumerate(masked_captions):\n",
    "            if re.search(r'\\b(?:check number|cheque number|check #|cheque #)\\b', caption, re.IGNORECASE):\n",
    "                check_context = True\n",
    "                context_counter = 0\n",
    "                numbers = re.findall(check_number_regex, caption)\n",
    "                if numbers:\n",
    "                    for num in numbers:\n",
    "                        masked_captions[i] = re.sub(re.escape(num), \"[CHECK_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            elif check_context:\n",
    "                numbers = re.findall(check_number_regex, caption)\n",
    "                if numbers:\n",
    "                    for num in numbers:\n",
    "                        masked_captions[i] = re.sub(re.escape(num), \"[CHECK_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "                context_counter += 1\n",
    "                if context_counter > context_timeout:\n",
    "                    check_context = False\n",
    "\n",
    "            else:\n",
    "                check_context = False\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mask_routing_number(df):\n",
    "        \"\"\"Masks routing numbers with contextual awareness.\"\"\"\n",
    "        routing_context = False\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 3\n",
    "        context_counter = 0\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            if re.search(r'\\b(?:routing number|ABA number|bank routing|bank details)\\b', caption, re.IGNORECASE):\n",
    "                routing_context = True\n",
    "                context_counter = 0\n",
    "                numbers = re.findall(r'\\b\\d{9}\\b', caption)\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[ROUTING_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            elif routing_context:\n",
    "                numbers = re.findall(r'\\b\\d{9}\\b', caption)  # Extract 9-digit numbers\n",
    "                if numbers:\n",
    "                    for num in numbers:\n",
    "                        masked_captions[i] = re.sub(re.escape(num), \"[ROUTING_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "                context_counter += 1\n",
    "                if context_counter > context_timeout:\n",
    "                    routing_context = False\n",
    "\n",
    "            else:\n",
    "                routing_context = False\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "    def mask_account_number(df):\n",
    "        \"\"\"Masks account numbers with contextual awareness.\"\"\"\n",
    "        account_context = False\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        context_timeout = 5\n",
    "        context_counter = 0\n",
    "\n",
    "        for i, caption in enumerate(df['caption']):\n",
    "            if re.search(r'\\b(?:account number|bank account|checking account|savings account|bank details)\\b', caption, re.IGNORECASE):\n",
    "                account_context = True\n",
    "                context_counter = 0\n",
    "                numbers = re.findall(r'\\b\\d{6,18}\\b', caption)\n",
    "                for num in numbers:\n",
    "                    masked_captions[i] = re.sub(re.escape(num), \"[ACCOUNT_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            elif account_context:\n",
    "                numbers = re.findall(r'\\b\\d{6,18}\\b', caption)  # Extract 6-18 digit numbers\n",
    "                if numbers:\n",
    "                    for num in numbers:\n",
    "                        masked_captions[i] = re.sub(re.escape(num), \"[ACCOUNT_NUMBER_REDACTED]\", masked_captions[i])\n",
    "\n",
    "                context_counter += 1\n",
    "                if context_counter > context_timeout:\n",
    "                    account_context = False\n",
    "\n",
    "            else:\n",
    "                account_context = False\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mask_card_ending(df):\n",
    "        \"\"\"Masks card endings with proximity-aware date exclusion.\"\"\"\n",
    "        masked_captions = df['caption'].astype(str).tolist()\n",
    "        address_keywords = [\"billing address\", \"address\", \"zip code\", \"postal code\", \"street\", \"city\", \"state\"]\n",
    "        date_keywords = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "        proximity_window = 50  # Adjust as needed\n",
    "\n",
    "        for i, caption in enumerate(masked_captions):\n",
    "            caption_lower = caption.lower()\n",
    "\n",
    "            # Check for \"card ending in\" context\n",
    "            if \"card ending in\" in caption_lower:\n",
    "                match = re.search(r'card ending in\\s*(\\d{4,6})\\b', caption_lower)\n",
    "                if match:\n",
    "                    ending = match.group(1)\n",
    "                    ending_index = caption_lower.find(match.group(0))\n",
    "\n",
    "                    # Check for date keywords within the proximity window\n",
    "                    date_found_nearby = False\n",
    "                    for date_keyword in date_keywords:\n",
    "                        if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                            date_found_nearby = True\n",
    "                            break\n",
    "\n",
    "                    if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                        masked_captions[i] = re.sub(re.escape(ending), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                    continue\n",
    "\n",
    "            # Check for \"on the card\" context\n",
    "            if \"on the card\" in caption_lower:\n",
    "                match = re.search(r'on the card\\s*(\\d{4,6})\\b', caption_lower)\n",
    "                if match:\n",
    "                    ending = match.group(1)\n",
    "                    ending_index = caption_lower.find(match.group(0))\n",
    "\n",
    "                    # Check for date keywords within the proximity window\n",
    "                    date_found_nearby = False\n",
    "                    for date_keyword in date_keywords:\n",
    "                        if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                            date_found_nearby = True\n",
    "                            break\n",
    "\n",
    "                    if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                        masked_captions[i] = re.sub(re.escape(ending), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                    continue\n",
    "\n",
    "            # Check for other card ending keywords\n",
    "            card_ending_keywords = [\"ending in\", \"ending with\", \"ends in\"]\n",
    "            for keyword in card_ending_keywords:\n",
    "                if keyword in caption_lower:\n",
    "                    numbers = re.findall(r'\\b\\d{4,6}\\b', caption)\n",
    "                    if numbers:\n",
    "                        for num in numbers:\n",
    "                            ending_index = caption_lower.find(keyword)\n",
    "                            date_found_nearby = False\n",
    "                            for date_keyword in date_keywords:\n",
    "                                if date_keyword in caption_lower[max(0, ending_index - proximity_window):min(len(caption_lower), ending_index + proximity_window)]:\n",
    "                                    date_found_nearby = True\n",
    "                                    break\n",
    "\n",
    "                            if not any(address_keyword in caption_lower for address_keyword in address_keywords) and not date_found_nearby:\n",
    "                                masked_captions[i] = re.sub(re.escape(num), \"[CARD_ENDING_REDACTED]\", masked_captions[i])\n",
    "                    break\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def mask_account_ending(df):\n",
    "        \"\"\"Masks account endings with context and non-digit character check.\"\"\"\n",
    "        masked_captions = df['caption'].astype(str).tolist()\n",
    "\n",
    "        for i, caption in enumerate(masked_captions):\n",
    "            try:\n",
    "                match = re.search(r'(?i)(account ending in|account ending with|account ends in)\\s(\\d{4,6})\\b', caption)\n",
    "                if match:\n",
    "                    ending = match.group(2)\n",
    "                    ending_index = match.start(2)\n",
    "\n",
    "                    # Check for non-digit characters before or after the ending digits\n",
    "                    before = caption[:ending_index].strip()\n",
    "                    after = caption[ending_index + len(ending):].strip()\n",
    "\n",
    "                    if not before or not after:\n",
    "                        # If there's nothing before or after, it's likely just the ending digits.\n",
    "                        masked_captions[i] = re.sub(re.escape(ending), \"[ACCOUNT_ENDING_REDACTED]\", masked_captions[i])\n",
    "                    else:\n",
    "                        # check if the surrounding characters are part of the context.\n",
    "                        if re.search(r'(?i)(account ending in|account ending with|account ends in)', before) or re.search(r'(?i)(account ending in|account ending with|account ends in)', after):\n",
    "                            masked_captions[i] = re.sub(re.escape(ending), \"[ACCOUNT_ENDING_REDACTED]\", masked_captions[i])\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing line {i}: {e}\")\n",
    "\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    \"\"\"=================Adress Masking===========\"\"\"\n",
    "\n",
    "    def mask_address(df):\n",
    "        \"\"\"Redacts addresses from a DataFrame's 'caption' column.\"\"\"\n",
    "        def clean_text(text):\n",
    "            \"\"\"Remove extra spaces and fix uppercase letter spacing.\"\"\"\n",
    "            text = re.sub(r'\\b([A-Z])(?:\\s([A-Z]))+\\b', lambda m: ''.join(m.group().split()), text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "\n",
    "        STATE_PATTERN = r'\\b(?:Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|' \\\n",
    "                        r'Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|' \\\n",
    "                        r'Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|' \\\n",
    "                        r'North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|' \\\n",
    "                        r'South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b'\n",
    "\n",
    "        CITY_STATE_ZIP_PATTERN = r'\\b[A-Za-z]+(?:\\s[A-Za-z]+)*,?\\s(?:' + \\\n",
    "                                  r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|' + \\\n",
    "                                  r'Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|' + \\\n",
    "                                  r'Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|' + \\\n",
    "                                  r'Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|' + \\\n",
    "                                  r'Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\s*\\d{5}(?:-\\d{4})?\\b'\n",
    "\n",
    "        ZIPCODE_PATTERN = r'(?<!\\d{3}-\\d{3}-)\\b\\d{5}(?:-\\d{4})?\\b(?!-\\d{3})'\n",
    "\n",
    "        HOUSE_NUMBER_PATTERN = r'\\b\\d{1,5}(?=\\s+[A-Za-z]+\\s+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct)\\b)'\n",
    "\n",
    "        STREET_SUFFIX_PATTERN = r'\\b\\d{1,5}\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct)\\b'\n",
    "        SANDWICHED_PATTERN = r'\\[ADDRESS_REDACTED\\](?:,)?\\s+(.*?)\\s+\\[ADDRESS_REDACTED\\]|\\[ADDRESS_REDACTED\\]\\s+in\\s+([A-Za-z\\s]+)\\b'\n",
    "\n",
    "        masked_captions = df['caption'].tolist()\n",
    "        for i, caption in enumerate(masked_captions):\n",
    "            caption = clean_text(caption)\n",
    "            caption = re.sub(CITY_STATE_ZIP_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            caption = re.sub(ZIPCODE_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            caption = re.sub(STATE_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            caption = re.sub(STREET_SUFFIX_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            caption = re.sub(HOUSE_NUMBER_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            caption = re.sub(SANDWICHED_PATTERN, '[ADDRESS_REDACTED]', caption)\n",
    "            masked_captions[i] = caption\n",
    "        df['caption'] = masked_captions\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mask_pii_in_captions(\n",
    "        contact_id,\n",
    "        df,\n",
    "        project_id,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Masks PII data in the 'caption' column of a pandas DataFrame using Google Cloud DLP API.\n",
    "\n",
    "        Args:\n",
    "            contact_id: Identifier for logging purposes\n",
    "            df (pandas.DataFrame): DataFrame with a 'caption' column to process\n",
    "            project_id (str): Your Google Cloud project ID\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with masked PII in the 'caption' column\n",
    "        \"\"\"\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Masking PII Data\")\n",
    "\n",
    "            masked_df = df.copy()\n",
    "            masked_df['original_index'] = masked_df.index\n",
    "            masked_df['previous_caption'] = masked_df['caption'].shift(1)\n",
    "\n",
    "            # Apply contextual card number, expiration date, and CVV redaction FIRST\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_card_numbers_contextually\")\n",
    "            masked_df = mask_card_numbers_contextually(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_cvv_contextually\")\n",
    "            masked_df = mask_cvv_contextually(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_expiration_date_contextually\")\n",
    "            masked_df = mask_expiration_date_contextually(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_emails_contextually\")\n",
    "            masked_df = mask_emails_contextually(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_routing_number\")\n",
    "            masked_df = mask_routing_number(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_account_number\")\n",
    "            masked_df = mask_account_number(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_card_ending\")\n",
    "            masked_df = mask_card_ending(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_account_ending\")\n",
    "            masked_df = mask_account_ending(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_address\")\n",
    "            masked_df = mask_address(masked_df)\n",
    "            thread_logger.info(f\"{contact_id}: Masking mask_check_number\")\n",
    "            masked_df = mask_check_number(masked_df)\n",
    "\n",
    "            cvv_requested = False\n",
    "            exp_requested = False\n",
    "\n",
    "            def preprocess_text(row):\n",
    "                nonlocal cvv_requested, exp_requested\n",
    "                result = row['caption']\n",
    "                if not re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card)\\b', row['caption'], re.IGNORECASE):\n",
    "                    cvv_requested = False\n",
    "                if not re.search(r'\\b(?:exp|expires|expiration|expiry)\\b', row['caption'], re.IGNORECASE):\n",
    "                    exp_requested = False\n",
    "\n",
    "                if re.search(r'\\b(?:cvv|security code|digits on the back|card verification|3 digits at the back of the card)\\b', row['caption'], re.IGNORECASE):\n",
    "                    cvv_requested = True\n",
    "                if re.search(r'\\b(?:exp|expires|expiration|expiry)\\b', row['caption'], re.IGNORECASE):\n",
    "                    exp_requested = True\n",
    "                return result\n",
    "\n",
    "            masked_df['caption'] = masked_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "\n",
    "            masked_df['marked_caption'] = masked_df.index.astype(str) + \"|||SEPARATOR|||\" + masked_df['caption'].astype(str)\n",
    "            all_captions = \"\\n===RECORD_BOUNDARY===\\n\".join(masked_df['marked_caption'])\n",
    "\n",
    "\n",
    "            dlp_client = dlp_v2.DlpServiceClient()\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "\n",
    "            inspect_config = {\n",
    "                \"info_types\": [\n",
    "                    {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                    {\"name\": \"STREET_ADDRESS\"},\n",
    "\n",
    "                    {\"name\": \"IP_ADDRESS\"},\n",
    "                    {\"name\": \"DATE_OF_BIRTH\"},\n",
    "                    {\"name\": \"PHONE_NUMBER\"}, # add phone number to inspect\n",
    "                    {\"name\": \"EMAIL_ADDRESS\"} # added email address\n",
    "                ],\n",
    "                \"custom_info_types\": [\n",
    "                    {\n",
    "                        \"info_type\": {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                        \"regex\": {\"pattern\": r'\\b(0[1-9]|1[0-2])[-/](\\d{2}|\\d{4})\\b|\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s*\\d{2,4}\\b'},\n",
    "                        \"likelihood\": dlp_v2.Likelihood.POSSIBLE\n",
    "                    }\n",
    "                ],\n",
    "                \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE\n",
    "            }\n",
    "\n",
    "            deidentify_config = {\n",
    "                \"info_type_transformations\": {\n",
    "                    \"transformations\": [\n",
    "                        {\n",
    "                            \"info_types\": [\n",
    "                                {\"name\": \"CREDIT_CARD_NUMBER\"},\n",
    "                                {\"name\": \"CREDIT_CARD_EXPIRATION_DATE\"},\n",
    "                                {\"name\": \"STREET_ADDRESS\"},\n",
    "\n",
    "                                {\"name\": \"IP_ADDRESS\"},\n",
    "                                {\"name\": \"DATE_OF_BIRTH\"},\n",
    "                                {\"name\": \"EMAIL_ADDRESS\"} # added email address\n",
    "                            ],\n",
    "                            \"primitive_transformation\": {\n",
    "                                \"replace_config\": {\"new_value\": {\"string_value\": \"[REDACTED]\"}}\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = dlp_client.deidentify_content(\n",
    "                    request={\n",
    "                        \"parent\": parent,\n",
    "                        \"deidentify_config\": deidentify_config,\n",
    "                        \"inspect_config\": inspect_config,\n",
    "                        \"item\": {\"value\": all_captions}\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                thread_logger.error(f\"{contact_id}: mask_pii_in_captions() failed Error in DLP API call: {e}\")\n",
    "                return df\n",
    "\n",
    "            processed_content = response.item.value\n",
    "            processed_records = processed_content.split(\"\\n===RECORD_BOUNDARY===\\n\")\n",
    "\n",
    "            processed_dict = {\n",
    "                int(parts[0]): parts[1]\n",
    "                for record in processed_records\n",
    "                if (parts := record.split(\"|||SEPARATOR|||\", 1)) and len(parts) == 2\n",
    "            }\n",
    "\n",
    "            masked_df['caption'] = masked_df.apply(\n",
    "                lambda row: processed_dict.get(row['original_index'], row['caption']),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            masked_df.drop(['original_index', 'marked_caption', 'previous_caption'], axis=1, inplace=True)\n",
    "            thread_logger.info(f\"{contact_id}: Completed Masking PII Data\")\n",
    "            return masked_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"mask_pii_in_captions() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def get_sentiment_label(row):\n",
    "        try:\n",
    "            # Check conditions in order of priority (Positive > Negative > Neutral)\n",
    "            if row['positive'] > row['negative'] and row['positive'] > row['neutral']:\n",
    "                return 'Positive'\n",
    "            elif row['negative'] > row['positive'] and row['negative'] > row['neutral']:\n",
    "                return 'Negative'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_label() failed: {str(e)}\")\n",
    "\n",
    "    def get_different_times(\n",
    "        intra_call,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            # Apply formatting to both time columns\n",
    "            intra_call['start_time_second'] = (intra_call['Begin_Offset'] / 1000).astype(int)\n",
    "            intra_call['end_time_second'] = (intra_call['End_Offset'] / 1000).astype(int)\n",
    "            intra_call['time_spoken_second'] = intra_call['end_time_second'] - intra_call['start_time_second']\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].where(intra_call['time_spoken_second'] >= 0, 0)\n",
    "            intra_call['time_spoken_second'] = intra_call['time_spoken_second'].fillna(0).astype(int)\n",
    "            intra_call['time_silence_second'] = intra_call['start_time_second'].shift(-1) - intra_call['end_time_second']\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].where(intra_call['time_silence_second'] >= 0, 0)\n",
    "            intra_call['time_silence_second'] = intra_call['time_silence_second'].fillna(0).astype(int)\n",
    "            intra_call['load_date'] = datetime.now()\n",
    "\n",
    "            # Dropping time formatted columns\n",
    "            intra_call = intra_call.drop(['Begin_Offset', 'End_Offset'], axis=1)\n",
    "\n",
    "            return intra_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_different_times() failed: {str(e)}\")\n",
    "\n",
    "    def get_sentiment_scores(\n",
    "        contact_id,\n",
    "        text_list,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Calculating Caption Sentiments.\")\n",
    "            dict_sentiments = []\n",
    "            for text in text_list:\n",
    "                encoded_input = tokenizer(text, return_tensors='pt')\n",
    "                output = model_sentiment(**encoded_input)\n",
    "                scores = output[0][0].detach().numpy()\n",
    "                scores = np.round(np.multiply(softmax(scores), 100), 2)\n",
    "                merged_dict = dict(zip(list(config.id2label.values()), list(scores)))\n",
    "                dict_sentiments.append(merged_dict)\n",
    "\n",
    "            df_dict_sentiments = pd.DataFrame(dict_sentiments)\n",
    "            df_dict_sentiments['sentiment_lable'] = df_dict_sentiments[['positive','negative','neutral']].apply(get_sentiment_label, axis=1)\n",
    "            thread_logger.info(f\"{contact_id}: Completed calculating Caption Sentiments.\")\n",
    "\n",
    "            return df_dict_sentiments\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"get_sentiment_scores() failed: {str(e)}\")\n",
    "\n",
    "    def process_transcript(\n",
    "        contact_id,\n",
    "        transcript_data,\n",
    "        tokenizer,\n",
    "        thread_logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Pre-process the transcript loaded from S3 Buckets:\n",
    "        1. Load the transcript as Pandas Dataframe.\n",
    "        2. Select only the necessary columns ['BeginOffsetMillis', 'EndOffsetMillis', 'ParticipantId', 'Content', 'Sentiment', 'LoudnessScore'].\n",
    "        3. Format the time in minutes and seconds.\n",
    "        4. Rename the columns for better understanding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Loading the Transcript as Pandas Dataframe.\")\n",
    "            transcript_df = pd.json_normalize(transcript_data['Transcript'])\n",
    "\n",
    "            # Select the relevant Columns\n",
    "            columns_to_select = [\n",
    "                'BeginOffsetMillis',\n",
    "                'EndOffsetMillis',\n",
    "                'ParticipantId',\n",
    "                'Content'\n",
    "            ]\n",
    "            formatted_df = transcript_df[columns_to_select].copy()\n",
    "\n",
    "            # Optionally rename columns to reflect their new format\n",
    "            formatted_df = formatted_df.rename(columns={\n",
    "                'BeginOffsetMillis': 'Begin_Offset',\n",
    "                'EndOffsetMillis': 'End_Offset',\n",
    "                'Content': 'caption',\n",
    "                'Sentiment': 'sentiment_label',\n",
    "                'ParticipantId': 'speaker_tag'\n",
    "            })\n",
    "\n",
    "            # Inserting the Call ID:\n",
    "            formatted_df.insert(loc=0, column='contact_id', value=contact_id)\n",
    "            formatted_df['call_language'] = transcript_data['LanguageCode']\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Returning formated DataFrame.\")\n",
    "            return formatted_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"process_transcript() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def create_intra_call_df(\n",
    "        contact_id,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        tokenizer,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Creating df_intra_call \")\n",
    "            intra_call = process_transcript(\n",
    "                contact_id,\n",
    "                transcript_data,\n",
    "                tokenizer,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            df_sentiment_scores = get_sentiment_scores(\n",
    "                contact_id,\n",
    "                intra_call.caption.to_list(),\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            intra_call = pd.concat([intra_call, df_sentiment_scores], axis=1)    \n",
    "            intra_call = get_different_times(\n",
    "                intra_call,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            intra_call = mask_pii_in_captions(\n",
    "                contact_id,\n",
    "                intra_call,\n",
    "                gcp_project_id,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: Successfully created df_intra_call \")\n",
    "\n",
    "            return intra_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_intra_call_df() failed: {str(e)}\")\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe: Inter Call \n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def dict_to_newline_string(data):\n",
    "        \"\"\"Converts a dictionary into a new-line formatted string.\"\"\"\n",
    "        try:\n",
    "            formatted_str = \"\"\n",
    "            for key, value in data.items():\n",
    "                formatted_str += f\"{key}:\\n\"\n",
    "                for item in value:\n",
    "                    formatted_str += f\"  - {item}\\n\"\n",
    "            return formatted_str.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"dict_to_newline_string() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    class CategoryValidator:\n",
    "        def __init__(self, df_cat_subcat_mapping):\n",
    "            \"\"\"\n",
    "            Initialize with category mapping from a Pandas DataFrame.\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame containing 'CATEGORY' and 'SUBCATEGORY' columns.\n",
    "            \"\"\"\n",
    "            self.df_cat_subcat_mapping = df_cat_subcat_mapping  # Ensure only the correct DataFrame is used\n",
    "            self.valid_categories = set(df_cat_subcat_mapping['CATEGORY'].dropna().unique())\n",
    "            self.category_subcategory_map = self._create_category_mapping()\n",
    "\n",
    "        def _create_category_mapping(self):\n",
    "            \"\"\"Create category to subcategory mapping.\"\"\"\n",
    "            try:\n",
    "                mapping = {}\n",
    "                for _, row in self.df_cat_subcat_mapping.dropna().iterrows():\n",
    "                    category = row['CATEGORY']\n",
    "                    subcategory = row['SUBCATEGORY']\n",
    "\n",
    "                    if category not in mapping:\n",
    "                        mapping[category] = set()\n",
    "\n",
    "                    if subcategory:  # Only add non-null subcategories\n",
    "                        mapping[category].add(subcategory)\n",
    "\n",
    "                return mapping\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"_create_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "        def validate_category(self, category: str) -> bool:\n",
    "            \"\"\"Check if category is valid.\"\"\"\n",
    "            try:\n",
    "                return category in self.valid_categories\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category() failed: {str(e)}\")\n",
    "\n",
    "        def validate_subcategory(self, category: str, subcategory: str) -> bool:\n",
    "            \"\"\"Check if subcategory is valid for the given category.\"\"\"\n",
    "            try:\n",
    "                return category in self.category_subcategory_map and subcategory in self.category_subcategory_map[category]\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_subcategory() failed: {str(e)}\")\n",
    "\n",
    "        def get_valid_subcategories(self, category: str) -> set:\n",
    "            \"\"\"Get valid subcategories for a category.\"\"\"\n",
    "            try:\n",
    "                return self.category_subcategory_map.get(category, set())\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_valid_subcategories() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    class CallSummary(BaseModel):\n",
    "        summary: str = Field(..., max_length=500)\n",
    "\n",
    "    class CallTopic(BaseModel):\n",
    "        primary_topic: str = Field(..., max_length=100)\n",
    "        category: str = Field(..., max_length=100)\n",
    "        sub_category: str = Field(..., max_length=100)\n",
    "\n",
    "        def validate_category_mapping(\n",
    "            self,\n",
    "            category_validator: CategoryValidator,\n",
    "            thread_logger\n",
    "        ):\n",
    "            \"\"\"Validate category and subcategory against mapping. Replace with 'Unspecified' if invalid.\"\"\"\n",
    "            try:\n",
    "                if not category_validator.validate_category(self.category):\n",
    "                    thread_logger.warning(f\"Invalid category: {self.category}. Replacing with 'Unspecified'.\")\n",
    "                    self.category = \"Unspecified\"\n",
    "                    self.sub_category = \"Unspecified\"\n",
    "                elif not category_validator.validate_subcategory(self.category, self.sub_category):\n",
    "                    thread_logger.warning(f\"Invalid subcategory '{self.sub_category}' for category '{self.category}'. Replacing subcategory with 'Unspecified'.\")\n",
    "                    self.sub_category = \"Unspecified\"\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_category_mapping() failed: {str(e)}\")\n",
    "\n",
    "    class AgentCoaching(BaseModel):\n",
    "        strengths: List[str] = Field(..., max_items=3)\n",
    "        improvement_areas: List[str] = Field(..., max_items=3)\n",
    "        specific_recommendations: List[str] = Field(..., max_items=4)\n",
    "        skill_development_focus: List[str] = Field(..., max_items=3)\n",
    "\n",
    "    class TranscriptAnalysis(BaseModel):\n",
    "        call_summary: CallSummary\n",
    "        call_topic: CallTopic\n",
    "        agent_coaching: AgentCoaching\n",
    "\n",
    "    class KPIExtractor:\n",
    "        def __init__(\n",
    "            self,\n",
    "            project_id: str,\n",
    "            location: str,\n",
    "            df_cat_subcat_mapping,\n",
    "            thread_logger\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize the KPIExtractor with Vertex AI model and category validator.\n",
    "            :param project_id: GCP Project ID\n",
    "            :param location: GCP Region\n",
    "            :param df_cat_subcat_mapping: Pandas DataFrame with 'CATEGORY' and 'SUBCATEGORY'\n",
    "            \"\"\"\n",
    "            vertexai.init(project=project_id, location=location)\n",
    "            self.model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "            self.category_validator = CategoryValidator(df_cat_subcat_mapping)\n",
    "\n",
    "            self.generation_config = {\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_output_tokens\": 1024,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "\n",
    "            self.safety_settings = {\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "            }\n",
    "\n",
    "\n",
    "        def get_categories_prompt(self) -> str:\n",
    "            \"\"\"Create prompt section for valid categories and subcategories, handling null values\"\"\"\n",
    "            try:\n",
    "                categories_prompt = []\n",
    "\n",
    "                for category, subcategories in self.category_validator.category_subcategory_map.items():\n",
    "                    if category is None:  # Skip if category is None\n",
    "                        continue\n",
    "\n",
    "                    # Ensure subcategories are valid (remove None values)\n",
    "                    valid_subcategories = [subcat for subcat in subcategories if subcat is not None]\n",
    "\n",
    "                    if valid_subcategories:\n",
    "                        subcats = ', '.join(sorted(valid_subcategories))\n",
    "                    else:\n",
    "                        subcats = \"No defined subcategories\"\n",
    "\n",
    "                    categories_prompt.append(f\"Category '{category}' can have subcategories: {subcats}\")\n",
    "\n",
    "                return '\\n'.join(categories_prompt)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"get_categories_prompt() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "        def create_prompt(self, transcript):\n",
    "            \"\"\"Create structured prompt with category guidance\"\"\"\n",
    "            categories_guidance = self.get_categories_prompt()\n",
    "\n",
    "            return f\"\"\"\n",
    "            Analyze this call transcript and provide a structured analysis in the exact JSON format specified below.\n",
    "            Keep responses concise, specific, and actionable.\n",
    "\n",
    "            Guidelines:\n",
    "            - Call summary should be factual and highlight key interactions\n",
    "            - Topics and categories MUST match the following valid mappings:\n",
    "            {categories_guidance}\n",
    "            - If the calls are received, forwarded or reached to a voicemail then always map:\n",
    "              Category=\"Unsuccessful Contact\" and Sub-Category with appropriate value from mapping\n",
    "            - Coaching points should be specific and actionable\n",
    "            - All responses must follow the exact structure specified\n",
    "            - Ensure all lists have the specified maximum number of items\n",
    "            - All text fields must be clear, professional, and free of fluff\n",
    "\n",
    "            Transcript:\n",
    "            {transcript}\n",
    "\n",
    "            Required Output Structure:\n",
    "            {{\n",
    "                \"call_summary\": {{\n",
    "                    \"summary\": \"3-4 line overview of the call\"\n",
    "                }},\n",
    "                \"call_topic\": {{\n",
    "                    \"primary_topic\": \"Main topic of discussion\",\n",
    "                    \"category\": \"MUST BE ONE OF THE VALID CATEGORIES LISTED ABOVE\",\n",
    "                    \"sub_category\": \"MUST BE A VALID SUB-CATEGORY FOR THE CHOSEN CATEGORY\"\n",
    "                }},\n",
    "                \"agent_coaching\": {{\n",
    "                    \"strengths\": [\"Strength 1\", \"Strength 2\", \"Strength 3\"],\n",
    "                    \"improvement_areas\": [\"Area 1\", \"Area 2\", \"Area 3\"],\n",
    "                    \"specific_recommendations\": [\"Rec 1\", \"Rec 2\", \"Rec 3\", \"Rec 4\"],\n",
    "                    \"skill_development_focus\": [\"Skill 1\", \"Skill 2\", \"Skill 3\"]\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            1. Maintain exact JSON structure\n",
    "            2. No additional fields or comments\n",
    "            3. No markdown formatting\n",
    "            4. Ensure all arrays have the exact number of items specified\n",
    "            5. Keep all text concise and professional\n",
    "            6. Do not mention any PII information such as Customer Name etc.\n",
    "            7. STRICTLY use only the categories and subcategories from the provided mapping\n",
    "            \"\"\"\n",
    "\n",
    "        def extract_json(self, response):\n",
    "            \"\"\"Extract valid JSON from response\"\"\"\n",
    "            try:\n",
    "                match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "                if match:\n",
    "                    json_str = match.group(1)\n",
    "                else:\n",
    "                    json_str = response.strip()\n",
    "                return json.loads(json_str)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"extract_json() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "        def validate_response(\n",
    "            self,\n",
    "            response_json,\n",
    "            thread_logger,\n",
    "            contact_id = None        \n",
    "        ):\n",
    "            \"\"\"Validate response using Pydantic models and category mapping\"\"\"\n",
    "            try:\n",
    "                # First validate basic structure with Pydantic\n",
    "                analysis = TranscriptAnalysis(**response_json)\n",
    "\n",
    "                # Then validate category mapping\n",
    "                analysis.call_topic.validate_category_mapping(self.category_validator, thread_logger)\n",
    "\n",
    "                return analysis\n",
    "\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"validate_response() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "#         def extract_genai_kpis(self, transcript, contact_id = None):\n",
    "#             \"\"\"Extract KPIs from transcript with validation\"\"\"\n",
    "#             try:\n",
    "#                 # Generate prompt\n",
    "#                 prompt = self.create_prompt(transcript)\n",
    "\n",
    "#                 # Get response from Gemini\n",
    "#                 response = self.model.generate_content(\n",
    "#                     prompt,\n",
    "#                     generation_config=self.generation_config,\n",
    "#                     safety_settings=self.safety_settings\n",
    "#                 )\n",
    "\n",
    "#                 # Parse JSON response\n",
    "#                 response_json = self.extract_json(response.text)\n",
    "\n",
    "#                 # Validate response structure and categories\n",
    "#                 validated_response = self.validate_response(response_json, contact_id)\n",
    "\n",
    "#                 return validated_response.model_dump()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 raise RuntimeError(f\"extract_genai_kpis() failed: {str(e)}\")\n",
    "                \n",
    "        def extract_genai_kpis(\n",
    "           self,\n",
    "           transcript: str,\n",
    "           contact_id: str = None\n",
    "        ):\n",
    "            \"\"\"Extract KPIs from transcript with validation and retries\"\"\"\n",
    "            max_retries = 3\n",
    "            attempt = 0\n",
    "\n",
    "            while attempt < max_retries:\n",
    "                try:\n",
    "                    # Generate prompt\n",
    "                    prompt = self.create_prompt(transcript)\n",
    "\n",
    "                    # Get response from Gemini\n",
    "                    response = self.model.generate_content(prompt)\n",
    "\n",
    "                    # Parse JSON response\n",
    "                    response_json = self.extract_json(response.text)\n",
    "\n",
    "                    # If response is empty, retry\n",
    "                    if not response_json or \"NA\" in response_json.values():\n",
    "                        logger.warning(f\"Attempt {attempt + 1}: Gemini returned NA or empty response. Retrying...\")\n",
    "                        attempt += 1\n",
    "                        time.sleep(2)  # Wait before retrying\n",
    "                        continue\n",
    "\n",
    "                    # Validate response\n",
    "                    validated_response = self.validate_response(response_json, contact_id)\n",
    "\n",
    "                    if validated_response:\n",
    "                        return validated_response.model_dump()\n",
    "\n",
    "                    logger.warning(f\"Attempt {attempt + 1}: Invalid response structure. Retrying...\")\n",
    "                    attempt += 1\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Attempt {attempt + 1}: Error extracting KPIs: {str(e)}\")\n",
    "                    attempt += 1\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "\n",
    "            logger.error(f\"Failed to extract valid KPIs after {max_retries} attempts.\")\n",
    "            return {\"error\": \"Failed to extract KPIs after multiple attempts\"}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Create Dataframe Inter Call\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def create_inter_call_df(\n",
    "        contact_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        pipeline_run_name,\n",
    "        transcript_data,\n",
    "        ac_last_modified_date,\n",
    "        df_intra_call,\n",
    "        gcp_project_id,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping,\n",
    "        thread_logger\n",
    "    ):\n",
    "        try:\n",
    "            thread_logger.info(f\"{contact_id}: Creating df_inter_call \")\n",
    "            thread_logger.info(f\"{contact_id}: Extracting KPIs from Gemini\")      \n",
    "            extractor = KPIExtractor(\n",
    "                gcp_project_id,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping,\n",
    "                thread_logger\n",
    "            )\n",
    "            transcript = \" \".join(df_intra_call.caption)\n",
    "            call_gen_kpis = extractor.extract_genai_kpis(transcript)\n",
    "            thread_logger.info(f\"{contact_id}: Completed Extracting KPIs from Gemini\") \n",
    "\n",
    "            inter_call_dict = {}\n",
    "            inter_call_dict['contact_id'] = str(df_intra_call['contact_id'][0])\n",
    "            inter_call_dict['call_text'] = \" \".join(df_intra_call.caption)\n",
    "            inter_call_dict['call_summary'] = call_gen_kpis['call_summary']['summary']\n",
    "            inter_call_dict['topic'] = call_gen_kpis['call_topic']['primary_topic']\n",
    "            inter_call_dict['category'] = call_gen_kpis['call_topic']['category']\n",
    "            inter_call_dict['sub_category'] = call_gen_kpis['call_topic']['sub_category']\n",
    "            inter_call_dict['agent_coaching'] = dict_to_newline_string(call_gen_kpis['agent_coaching'])\n",
    "            df_inter_call = pd.DataFrame(pd.Series(inter_call_dict)).T\n",
    "            \n",
    "            # Replace values where Categories are not in allowed list\n",
    "            allowed_categories = df_cat_subcat_mapping['CATEGORY'].drop_duplicates().to_list()\n",
    "            df_inter_call.loc[\n",
    "                ~df_inter_call['category'].isin(allowed_categories) | df_inter_call['category'].isna(),\n",
    "                ['category', 'sub_category']\n",
    "            ] = 'Unspecified'\n",
    "\n",
    "            df_inter_call['agent_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['AGENT']['AverageWordsPerMinute']\n",
    "            df_inter_call['customer_speech_speed'] = transcript_data['ConversationCharacteristics']['TalkSpeed']['DetailsByParticipant']['CUSTOMER']['AverageWordsPerMinute']\n",
    "            df_inter_call['total_talktime_agent_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['AGENT']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_customer_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['DetailsByParticipant']['CUSTOMER']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_talktime_call_second'] = int(transcript_data['ConversationCharacteristics']['TalkTime']['TotalTimeMillis']/1000)\n",
    "            df_inter_call['total_duration_call_second'] = int(transcript_data['ConversationCharacteristics']['TotalConversationDurationMillis']/1000)\n",
    "            df_inter_call['total_dead_air_call_second'] = df_inter_call['total_duration_call_second'] - df_inter_call['total_talktime_call_second']\n",
    "            # df_inter_call['customer_instance_id'] = transcript_data['CustomerMetadata']['InstanceId']\n",
    "            # df_inter_call['call_job_status'] = transcript_data['JobStatus']\n",
    "            df_inter_call['call_language'] = transcript_data['LanguageCode']\n",
    "            df_inter_call['call_s3_uri'] = transcript_data['CustomerMetadata']['InputS3Uri']\n",
    "            df_inter_call['ac_last_modified_date'] = ac_last_modified_date\n",
    "            thread_logger.info(f\"{contact_id}: Successfully created df_inter_call \")\n",
    "\n",
    "            return df_inter_call\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"create_inter_call_df() failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Function: Process Single Transcript\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    def process_single_transcript(\n",
    "        pipeline_run_name,\n",
    "        gcp_project_id,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_logs_folder,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        transcript_path,\n",
    "        tokenizer,\n",
    "        gcp_project_location,\n",
    "        df_cat_subcat_mapping\n",
    "    ):\n",
    "        contact_id = transcript_path.split('/')[-1].split('analysis')[0].strip('_')\n",
    "        ac_last_modified_date = datetime.strptime(\n",
    "                transcript_path.split('/')[-1].split('analysis_')[-1].split('.')[0].replace('_', ':'),\n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            )\n",
    "\n",
    "        thread_logger, log_filepath = setup_thread_logger(contact_id)\n",
    "\n",
    "        try:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(transcript_path)\n",
    "            transcript_data = json.loads(blob.download_as_text())\n",
    "\n",
    "            thread_logger.info(f\"{contact_id}: started processing\")\n",
    "\n",
    "            df_intra_call = create_intra_call_df(\n",
    "                contact_id,\n",
    "                gcp_project_id,\n",
    "                vai_gcs_bucket,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                tokenizer,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            df_inter_call = create_inter_call_df(\n",
    "                contact_id,\n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                pipeline_run_name,\n",
    "                transcript_data,\n",
    "                ac_last_modified_date,\n",
    "                df_intra_call,\n",
    "                gcp_project_id,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping,\n",
    "                thread_logger\n",
    "            )\n",
    "\n",
    "            if not df_intra_call.empty and not df_inter_call.empty:\n",
    "                csv_path_df_intra_call = f\"gs://{vai_gcs_bucket}/{gcs_intra_call_dfs_folder}/{contact_id}_df_intra_call.csv\"\n",
    "                df_intra_call.to_csv(csv_path_df_intra_call, index=False)\n",
    "                thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_intra_call.csv\")\n",
    "\n",
    "                csv_path_df_inter_call = f\"gs://{vai_gcs_bucket}/{gcs_inter_call_dfs_folder}/{contact_id}_df_inter_call.csv\"\n",
    "                df_inter_call.to_csv(csv_path_df_inter_call, index=False)\n",
    "                thread_logger.info(f\"{contact_id}: Persisted: {contact_id}_df_inter_call.csv\")\n",
    "\n",
    "                thread_logger.info(f\"{contact_id}: Processing Complete\")\n",
    "                thread_logger.info(\"\")\n",
    "                thread_logger.info(\"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            handle_exception(contact_id, vai_gcs_bucket, pipeline_run_name, gcs_errored_folder, str(e), thread_logger)\n",
    "            return None # Continue processing other files\n",
    "\n",
    "        return log_filepath\n",
    "\n",
    "    def merge_and_save_transcripts(\n",
    "        bucket_name,\n",
    "        input_folder,\n",
    "        output_folder,\n",
    "        output_file,\n",
    "        master_logger\n",
    "    ):\n",
    "        try:\n",
    "            \"\"\"Reads, merges all files in a GCS folder, and saves the master DataFrame as CSV.\"\"\"\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(bucket_name)\n",
    "\n",
    "            dfs = [\n",
    "                pd.read_parquet(bucket.blob(blob.name).open(\"rb\")) if blob.name.endswith(\".parquet\") \n",
    "                else pd.read_csv(bucket.blob(blob.name).open(\"r\")) \n",
    "                for blob in bucket.list_blobs(prefix=input_folder) \n",
    "                if blob.name.endswith(('.csv', '.parquet'))\n",
    "            ]\n",
    "\n",
    "            if dfs:\n",
    "                master_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Convert DataFrame to CSV in-memory\n",
    "                csv_buffer = io.StringIO()\n",
    "                master_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "                # Upload CSV to GCS\n",
    "                bucket.blob(f\"{output_folder}/{output_file}\").upload_from_string(\n",
    "                    csv_buffer.getvalue(), content_type=\"text/csv\"\n",
    "                )\n",
    "                master_logger.info(f\"Completed: merging and writing {output_file} to {output_folder}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            master_logger.error(f\"Error processing {input_folder}: {str(e)}\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ========================================================\n",
    "    Variables\n",
    "    ========================================================\n",
    "    \"\"\"\n",
    "    configs = fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    )\n",
    "    \n",
    "    # GCP Configuration\n",
    "    gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "    gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "    vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "    # Pipeline Configuration\n",
    "    gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "    gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "    gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "    gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "    gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "    gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "    # Snowflake Configuration\n",
    "    snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "    snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "    snf_private_key = configs.get(\"private_key\")\n",
    "    snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "    snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "    snf_catsubcat_databse = configs.get(\"VAI_SNF_CATSUBCAT_DATABASE\")\n",
    "    snf_catsubcat_schema = configs.get(\"VAI_SNF_CATSUBCAT_SCHEMA\")\n",
    "    snf_catsubcat_view = configs.get(\"VAI_SNF_CATSUBCAT_VIEW\")\n",
    "\n",
    "    # Step 2: Download Master Log File from GCS\n",
    "    master_log_file = f\"{pipeline_run_name}.logs\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(vai_gcs_bucket)\n",
    "    blob = bucket.blob(f\"{gcs_logs_folder}/{master_log_file}\")\n",
    "    # Download master log file\n",
    "    blob.download_to_filename(master_log_file)\n",
    "\n",
    "    master_logger = setup_logger(master_log_file)\n",
    "    master_logger.info(\"\")\n",
    "    master_logger.info(\"\")\n",
    "    master_logger.info(\"============================================================================\")\n",
    "    master_logger.info(\"COMPONENT: Process Transcripts.\")\n",
    "    master_logger.info(\"============================================================================\")\n",
    "    master_logger.info(\"Fetched Master Log File from GCS bucket.\")\n",
    "\n",
    "    temp_log_folder = \"temp_logs\"\n",
    "    os.makedirs(temp_log_folder, exist_ok=True)\n",
    "\n",
    "    df_cat_subcat_mapping = fetch_category_mapping_from_snowflake(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_catsubcat_databse,\n",
    "        snf_catsubcat_schema,\n",
    "        snf_catsubcat_view,\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    transcripts_list = fetch_transcripts_from_gcs(\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        gcs_transcripts_folder,\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    master_logger.info(\"Starting the Multi-threading.\")\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    threads_log_files = []  # Store generated log files\n",
    "\n",
    "    # Max parallelism for multi-threading\n",
    "    max_parallelism = 50\n",
    "    # Multi-threaded execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallelism) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_transcript,\n",
    "                pipeline_run_name,\n",
    "                gcp_project_id, \n",
    "                vai_gcs_bucket,\n",
    "                gcs_stagging_folder,\n",
    "                gcs_errored_folder,\n",
    "                gcs_logs_folder,\n",
    "                gcs_intra_call_dfs_folder,\n",
    "                gcs_inter_call_dfs_folder,\n",
    "                transcript_path,\n",
    "                tokenizer,\n",
    "                gcp_project_location,\n",
    "                df_cat_subcat_mapping\n",
    "            ) for transcript_path in transcripts_list\n",
    "        ]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            threads_log_files.append(future.result())\n",
    "\n",
    "    # Merge all threaded transcripts\n",
    "    threads_log_files = [file for file in threads_log_files if isinstance(file, str) and file.endswith(\".log\")]\n",
    "    merge_logs(\n",
    "        threads_log_files,\n",
    "        master_log_file,\n",
    "        master_logger\n",
    "    )\n",
    "    master_logger.info(\"===================================================================\")\n",
    "    master_logger.info(\"Completed the Multi-threading.\")\n",
    "    master_logger.info(\"===================================================================\")\n",
    "\n",
    "    master_logger = setup_logger(master_log_file)\n",
    "\n",
    "    # Step 3: Merge all outputs into master files after processing\n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_intra_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_intra_call_df.csv\",\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    merge_and_save_transcripts(\n",
    "        vai_gcs_bucket,\n",
    "        gcs_inter_call_dfs_folder,\n",
    "        gcs_stagging_folder,\n",
    "        \"master_inter_call_df.csv\",\n",
    "        master_logger\n",
    "    )\n",
    "\n",
    "    # Upload the master log file back into GCS Bucket\n",
    "    gcs_bucket = storage.Client().bucket(vai_gcs_bucket)\n",
    "    blob = gcs_bucket.blob(f\"{gcs_logs_folder}/{master_log_file}\")\n",
    "    blob.upload_from_filename(master_log_file, checksum=None)\n",
    "    master_logger.info(\"Uploaded Master Log File back to GCS bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d365f-c880-4ace-970d-a5618d503ad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Component: Write data to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92da9211-4634-4f47-9226-0bf6e6d7a165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=f\"us-central1-docker.pkg.dev/dev-posigen/dev-cx-voiceai/dev-cx-voiceai-docker-image:dev-1\"\n",
    ")\n",
    "def write_data_to_snowflake(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    import io, logging, json\n",
    "    import pytz\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from google.cloud import secretmanager\n",
    "    from google.cloud import storage\n",
    "    import snowflake.connector as sc\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "    def fetch_secrets(\n",
    "        project_id,\n",
    "        secret_id,\n",
    "        version_id\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Access a secret from Google Secret Manager\n",
    "\n",
    "        Args:\n",
    "            project_id: Your Google Cloud project ID\n",
    "            secret_id: The ID of the secret to access\n",
    "            version_id: The version of the secret (default: \"latest\")\n",
    "\n",
    "        Returns:\n",
    "            The secret payload as a string\n",
    "        \"\"\"\n",
    "        # Create the Secret Manager client\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Decode and parse the JSON payload\n",
    "        secret_payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(secret_payload)  # Convert string to JSON\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"The secret payload is not a valid JSON\")\n",
    "\n",
    "\n",
    "    def setup_logger(\n",
    "        log_file\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up a logger that writes to a log file, console, and optionally Google Cloud Logging.\n",
    "\n",
    "        Args:\n",
    "            log_file (str): Path of the log file.\n",
    "\n",
    "        Returns:\n",
    "            logger: Configured logger instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger(log_file)\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logger.propagate = False  # Prevent duplicate logs\n",
    "\n",
    "            # Remove existing handlers if any (clean start)\n",
    "            if logger.hasHandlers():\n",
    "                logger.handlers.clear()\n",
    "\n",
    "            # Formatter for log messages\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S'\n",
    "            )\n",
    "\n",
    "            # File Handler (append mode)\n",
    "            file_handler = logging.FileHandler(log_file, mode='a')\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "            # Console Handler\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(logging.INFO)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            logger.addHandler(console_handler)\n",
    "\n",
    "            return logger\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize logger: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def handle_exception(\n",
    "        file_id,\n",
    "        vai_gcs_bucket,\n",
    "        run_folder,\n",
    "        error_folder,\n",
    "        error_message,\n",
    "        logger\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs the error, appends the file_id to error tracking CSV, and triggers a notification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            error_df_path = f\"{error_folder}/{run_folder}_errors.csv\"\n",
    "\n",
    "            logger.error(f\"Error processing file {file_id}: {error_message}\")\n",
    "\n",
    "            gcs_client = storage.Client()\n",
    "            bucket = gcs_client.bucket(vai_gcs_bucket)\n",
    "            blob = bucket.blob(error_df_path)\n",
    "\n",
    "            if blob.exists():\n",
    "                error_df = pd.read_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\")\n",
    "            else:\n",
    "                error_df = pd.DataFrame(columns=[\"File_ID\", \"Error_Message\"])\n",
    "\n",
    "            error_df = pd.concat([error_df, pd.DataFrame([{\"File_ID\": file_id, \"Error_Message\": error_message}])], ignore_index=True)\n",
    "            error_df.to_csv(f\"gs://{vai_gcs_bucket}/{error_df_path}\", index=False)\n",
    "            logger.info(f\"Logged error for file {file_id} in {error_df_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to error tracking file: {e}\")\n",
    "\n",
    "\n",
    "    def insert_new_records(\n",
    "        logger,\n",
    "        pipeline_run_name,\n",
    "        vai_gcs_bucket,\n",
    "        gcs_stagging_folder,\n",
    "        gcs_errored_folder,\n",
    "        snf_account,\n",
    "        snf_user,\n",
    "        snf_private_key,\n",
    "        snf_private_key_pwd,\n",
    "        snf_warehouse,\n",
    "        snf_databse,\n",
    "        snf_schema,\n",
    "        table_name,\n",
    "        df\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inserts only new records (based on ID) into Snowflake table with UTC load timestamp.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from cryptography.hazmat.primitives import serialization\n",
    "            from datetime import datetime\n",
    "            import pytz\n",
    "            import snowflake.connector as sc\n",
    "            from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "            # Step 1: Load & Decrypt the Private Key\n",
    "            snf_private_key = serialization.load_pem_private_key(\n",
    "                snf_private_key.encode(),\n",
    "                password=snf_private_key_pwd.encode(),\n",
    "                backend=None\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to Snowflake Compatible Format\n",
    "            pkey_bytes = snf_private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.DER,\n",
    "                format=serialization.PrivateFormat.PKCS8,\n",
    "                encryption_algorithm=serialization.NoEncryption(),\n",
    "            )\n",
    "\n",
    "            conn_params = {\n",
    "                'account': snf_account,\n",
    "                'user': snf_user,\n",
    "                'private_key': snf_private_key,\n",
    "                'warehouse': snf_warehouse,\n",
    "                'database': snf_databse,\n",
    "                'schema': snf_schema\n",
    "            }\n",
    "\n",
    "            conn = sc.connect(**conn_params)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Step 3: Fetch existing CONTACT_IDs\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT DISTINCT(CONTACT_ID) \n",
    "                FROM {table_name} \n",
    "                WHERE LOAD_DATE >= DATEADD(DAY, -1, CURRENT_DATE)\n",
    "            \"\"\")\n",
    "            existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "            # Step 4: Filter to only new records\n",
    "            new_records_df = df[~df['CONTACT_ID'].isin(existing_ids)].reset_index(drop=True)\n",
    "\n",
    "            if new_records_df.empty:\n",
    "                logger.info(\"No new records to insert.\")\n",
    "            else:\n",
    "                try:\n",
    "                    utc_now = datetime.now(pytz.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    new_records_df[\"LOAD_DATE\"] = utc_now\n",
    "\n",
    "                    logger.debug(f\"Prepared DataFrame with shape: {new_records_df.shape}\")\n",
    "                    logger.debug(f\"Sample rows: \\n{new_records_df.head()}\")\n",
    "\n",
    "                    success, nchunks, nrows, _ = write_pandas(\n",
    "                        conn=conn,\n",
    "                        df=new_records_df,\n",
    "                        table_name=table_name,                        \n",
    "                        quote_identifiers=False,\n",
    "                        on_error=\"continue\"\n",
    "                    )\n",
    "\n",
    "                    if not success or nrows == 0:\n",
    "                        error_msg = f\"write_pandas failed or inserted 0 rows for table {table_name}\"\n",
    "                        logger.error(error_msg)\n",
    "                        handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", error_msg, logger)\n",
    "                        raise RuntimeError(error_msg)\n",
    "                    else:\n",
    "                        logger.info(f\"Inserted {nrows} new records into {table_name} with UTC load date {utc_now}\")\n",
    "                        logger.info(f\"Skipped {len(df) - len(new_records_df)} existing records (deduped)\")\n",
    "\n",
    "                except Exception as insert_err:\n",
    "                    err_msg = f\"Error during Snowflake insert: {insert_err}\"\n",
    "                    handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(insert_err), logger)\n",
    "                    logger.exception(err_msg)\n",
    "                    raise  # re-raise the insert error\n",
    "\n",
    "        except Exception as outer_err:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(outer_err), logger)\n",
    "            logger.exception(f\"Outer exception during insert_new_records: {outer_err}\")\n",
    "            raise  # re-raise to propagate critical failure\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            except Exception as cleanup_err:\n",
    "                logger.warning(f\"Error closing Snowflake connection: {cleanup_err}\")\n",
    "\n",
    "    # Function to read CSV from GCS\n",
    "    def read_gcs_csv(file_path):\n",
    "        blob = bucket.blob(file_path)\n",
    "        csv_data = blob.download_as_text()\n",
    "        return pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "    try:\n",
    "        configs = fetch_secrets(\n",
    "            project_id,\n",
    "            secret_id,\n",
    "            version_id\n",
    "        )\n",
    "\n",
    "        # GCP Configuration\n",
    "        gcp_project_id = configs.get(\"VAI_GCP_PROJECT_ID\")\n",
    "        gcp_project_location = configs.get(\"GCP_PROJECT_LOCATION\")\n",
    "        vai_gcs_bucket = configs.get(\"VAI_GCP_PIPELINE_BUCKET\")\n",
    "\n",
    "        # Pipeline Configuration\n",
    "        gcs_stagging_folder = f\"{pipeline_run_name}/Stagging\"\n",
    "        gcs_errored_folder = f\"{pipeline_run_name}/Errored\"\n",
    "        gcs_logs_folder = f\"{pipeline_run_name}/Logs\"\n",
    "        gcs_transcripts_folder = f\"{pipeline_run_name}/Transcripts\"\n",
    "        gcs_intra_call_dfs_folder = f\"{pipeline_run_name}/Stagging/IntraCallDFs\"\n",
    "        gcs_inter_call_dfs_folder = f\"{pipeline_run_name}/Stagging/InterCallDFs\"\n",
    "\n",
    "        # Snowflake Configuration\n",
    "        snf_account = configs.get(\"VAI_SNF_ACCOUNT\")\n",
    "        snf_user = configs.get(\"VAI_SNF_USER\")\n",
    "        snf_private_key = configs.get(\"private_key\")\n",
    "        snf_private_key_pwd = configs.get(\"VAI_SNF_PRIVATE_KEY_PWD\")\n",
    "        snf_warehouse = configs.get(\"VAI_SNF_WAREHOUSE\")\n",
    "        snf_database = configs.get(\"VAI_SNF_DATABASE\")\n",
    "        snf_schema = configs.get(\"VAI_SNF_SCHEMA\")\n",
    "\n",
    "\n",
    "        # Step 2: Download Master Log File from GCS\n",
    "        log_file = f\"{pipeline_run_name}.logs\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(vai_gcs_bucket)\n",
    "        blob = bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "        # Download master log file\n",
    "        blob.download_to_filename(log_file)\n",
    "\n",
    "        logger = setup_logger(log_file)\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"============================================================================\")\n",
    "        logger.info(\"COMPONENT: Write Data to Snowflake.\")\n",
    "        logger.info(\"============================================================================\")\n",
    "        logger.info(\"Fetched Master Log File from GCS bucket.\")\n",
    "\n",
    "        # Read Inter & Intra Call DataFrames\n",
    "        inter_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_inter_call_df.csv\")\n",
    "        inter_call_df.columns = inter_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "        intra_call_df = read_gcs_csv(f\"{gcs_stagging_folder}/master_intra_call_df.csv\")\n",
    "        intra_call_df.columns = intra_call_df.columns.str.upper() # For snowflake Schema matching\n",
    "\n",
    "\n",
    "        logger.info(f\"VAI_SNF_WAREHOUSE: {snf_warehouse}\")\n",
    "        logger.info(f\"VAI_SNF_DATABASE: {snf_database}\")\n",
    "        logger.info(f\"snf_schema: {snf_schema}\")\n",
    "\n",
    "        logger.info(f\"Started: writing data to snowflake.\")\n",
    "        table_name ='SRC_GCP_INTER_CALLS'    \n",
    "        logger.info(f\"Writing data to table: {snf_warehouse}.{snf_database}.{table_name}\")\n",
    "        insert_new_records(\n",
    "            logger,\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            inter_call_df\n",
    "        )\n",
    "\n",
    "\n",
    "        logger.info(f\"Writing data to table: {snf_warehouse}.{snf_database}.{snf_schema}.{table_name}\")\n",
    "        table_name ='SRC_GCP_INTRA_CALLS'\n",
    "        insert_new_records(\n",
    "            logger,\n",
    "            pipeline_run_name,\n",
    "            vai_gcs_bucket,\n",
    "            gcs_stagging_folder,\n",
    "            gcs_errored_folder,\n",
    "            snf_account,\n",
    "            snf_user,\n",
    "            snf_private_key,\n",
    "            snf_private_key_pwd,\n",
    "            snf_warehouse,\n",
    "            snf_database,\n",
    "            snf_schema,\n",
    "            table_name,\n",
    "            intra_call_df\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Completed: writing data to snowflake.\")\n",
    "    \n",
    "        # Upload the updated log file back to GCS\n",
    "        logger.info(\"Uploading updated log file back to GCS.\")\n",
    "        blob = bucket.blob(f\"{gcs_logs_folder}/{log_file}\")\n",
    "        blob.upload_from_filename(log_file)\n",
    "        logger.info(\"Uploaded updated log file to GCS.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "            handle_exception(\"N/A\", vai_gcs_bucket, pipeline_run_name, f\"{pipeline_run_name}/Errored\", str(e), logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8767d-24ba-4713-a6d4-00be95aa4c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b8fae6e-1f9c-4ab9-8330-ddd074a3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"VAI Audio to KPI Pipeline\",\n",
    "    description=\"Process Amazon Audio Transcripts into KPIs\"\n",
    ")\n",
    "def vai_audio_to_kpi_pipeline(\n",
    "    pipeline_run_name: str,\n",
    "    project_id: str,\n",
    "    secret_id: str,\n",
    "    version_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline to:\n",
    "    1. List calls from S3 and download them to GCS.\n",
    "    2. Process each transcript in parallel using Kubeflow Pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: List and Download Calls from S3 to GCS\n",
    "    get_calls_to_process = list_download_calls_s3_to_gcs(\n",
    "        pipeline_run_name=pipeline_run_name,\n",
    "        project_id=project_id,\n",
    "        secret_id=secret_id,\n",
    "        version_id=version_id\n",
    "    )\n",
    "\n",
    "    # Step 2 and 3 should run **only if** there are calls to process\n",
    "    with dsl.If(get_calls_to_process.output > 0):\n",
    "        # Step 2: Process Transcripts\n",
    "        process_calls = process_transcripts(\n",
    "            pipeline_run_name=pipeline_run_name,\n",
    "            project_id=project_id,\n",
    "            secret_id=secret_id,\n",
    "            version_id=version_id\n",
    "        )\n",
    "\n",
    "        # Step 3: Write Processed Data to Snowflake\n",
    "        persist_to_snowflake = write_data_to_snowflake(\n",
    "            pipeline_run_name=pipeline_run_name,\n",
    "            project_id=project_id,\n",
    "            secret_id=secret_id,\n",
    "            version_id=version_id\n",
    "        )\n",
    "\n",
    "        # Ensure Step 3 runs **after** Step 2\n",
    "        persist_to_snowflake.after(process_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9f8e-1b71-4632-b429-b38c7f642abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25c4a84f-936d-43b2-8bea-9c42f298b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(vai_audio_to_kpi_pipeline, 'cx-voiceai-process-calls.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce403f4b-1c6e-4d70-a9c3-4b55e99a88ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "posigen",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "posigen (Local)",
   "language": "python",
   "name": "posigen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
